\documentclass{howto}
\usepackage{distutils}

% TODO:
%   Fill in XXX comments

\title{Pygr: Docs Overview}


\input{boilerplate}

\author{Chris Lee}
\authoraddress{
\strong{UCLA, Department of Chemistry and Biochemistry}\\
	Email: \email{leec@chem.ucla.edu}
}

\makeindex

\begin{document}

\maketitle

\begin{abstract}
  \noindent
   A roadmap to the Pygr documentation.
\end{abstract}

%\begin{abstract}
%\noindent
%Abstract this!
%\end{abstract}


% The ugly "%begin{latexonly}" pseudo-environment supresses the table
% of contents for HTML generation.
%
%begin{latexonly}
\tableofcontents
%end{latexonly}


\section{Introductory Tutorial}
\label{intro}

Pygr is open source software designed to make it easy to do powerful sequence and
comparative genomics analyses, even with extremely large multi-genome alignments.


\subsection{Sequence / Alignment Tutorial}
\label{seq-align}

Sequences and alignments also can be modeled as graph structures in Pygr, providing the same consistent and simple framework for queries.

\subsubsection{Sequence Objects}
Pygr tries to provide a very "Pythonic" model for sequences.  This python interpreter session illustrates some simple features:

\begin{verbatim}
>>> from pygr.seqdb import *
>>> s=NamedSequence('attatatgccactat','bobo') #create a sequence named bobo
>>> s # interpreter will print repr(s)
bobo[0:15]
>>> t=s[-8:] #python slice gives last 8 nt of s
>>> t # interpreter will print repr(t)
bobo[7:15]
>>> str(t) #string conversion just yields the sequence as a string 
'gccactat'
>>> rc= -s #get the reverse complement
>>> str(rc[:5]) #its first five letters
'atagt'
\end{verbatim}

Several points:
\begin{itemize}

\item
   Slices of a sequence object (e.g. s[1:10] or s[-8:]) are themselves sequence objects.

\item    
The string value of a sequence object (e.g. str(s)) is just the sequence itself (as a string).

\end{itemize}

\subsubsection{Comparative Genomics Query of Multigenome Alignments}

Many groups (e.g. David Haussler's group at UC Santa Cruz) have constructed alignments of multiple genomes.  These alignments are extremely useful and interesting, but so large that it is cumbersome to work with the dataset using conventional methods.  For example, for the 17-genome alignment you have to work simultaneously with the individual genome datasets for human, chimp, mouse, rat, dog, chicken, fugu and zebrafish etc., as well as the huge alignment itself.  Pygr makes this quite easy.  Here we illustrate an example of mapping a set of human exons, which has two splice sites
(\code{ss1} and \code{ss2}) bracketing a single exon (\code{exon}).
We use the alignment database to map each of these splice sites onto all the aligned
genomes, and to print the percent-identity and percent-aligned for each genome,
as well as the two nucleotides consituting the splice site itself.
It also prints the conservation of the two exonic region (between \code{ss1}
and \code{ss2}: 

\begin{verbatim}
import pygr.Data # FINDS DATA WHEREVER IT'S REGISTERED
msa=pygr.Data.Bio.Seq.MSA.ucsc17() # SANTA CRUZ 17-GENOME ALIGNMENT
exons=pygr.Data.Leelab.ASAP2.hg17.exons() # ASAP2 HUMAN EXONS
idDict= ~(msa.seqDict) # INVERSE: MAPS SEQ --> STRING IDENTIFIER
def printConservation(id,label,site):
    for src,dest,edge in msa[site].edges(mergeMost=True):
        print '%d\t%s\t%s\t%s\t%s\t%s\t%2.1f\t%2.1f' \
              %(id,label,repr(src),src,idDict[dest],dest,
                100*edge.pIdentity(),100*edge.pAligned())
for id,exon in exons.iteritems():
    ss1=exon.before()[-2:] # GET THE 2 NT SPLICE SITES
    ss2=exon.after()[:2]
    cacheHint=msa[ss1+ss2] #CACHE THE COVERING INTERVALS FROM ss1 TO ss2
    printConservation(id,'ss1',ss1)
    printConservation(id,'ss2',ss2)
    printConservation(id,'exon',exon)
\end{verbatim}

A few notes:

\begin{itemize}

\item
Querying a large multi-genome alignment requires special interval indexing
algorithms (R-Tree or nested-list used in Pygr).  Pygr provides a high-performance
C implementation of a disk-based nested-list database that provides both
very fast interval overlap query times (sub-millisecond per query, compared with
10-30 seconds per query using MySQL multi-column indexing, and much faster
than Postgres R-Tree indexing), and a very small memory footprint
(e.g. 2.5 MB RSS in-memory, 8 MB VSZ virtual size,
for working with the UCSC 17 vertebrate
genome alignment and sequence databases).  For more information on the
nested-list algorithm and performance comparisons, see the published paper,
Alekseyenko and Lee, Bioinformatics 2007.

\item
The alignment database query is in the first line of \code{printConservation()}.
\code{msa} is the database; \code{site} is the interval query; and the
\method{edges} methods iterates over the results, returning a tuple for
each, consisting of a {\em source sequence} interval (i.e. an interval of
\code{site}), a {\em destination sequence} interval (i.e. an interval in
an aligned genome), and an {\em edge object} describing that alignment.
We are taking advantage of Pygr's group-by operator \code{mergeMost},
which will cause multiple intervals in a given sequence to be merged
into a single interval that constitutes their ``union''.  Thus,
for each aligned genome, the \code{edges} iterator will return a single
aligned interval.  The alignment edge object provides some useful 
conveniences, such as calculating the percent-identity between \code{src}
and \code{dest} automatically for you.  \method{pIdentity()} computes
the fraction of identical residues; \method{pAligned} computes the 
fraction of {\em aligned} residues (allowing you to see if there are 
big gaps or insertions in the alignment of this interval).  If we 
had wanted to inspect the detailed alignment letter by letter, we
would just iterate over the \member{letters} attribute instead of
the \method{edges} method. (See the \class{NLMSASlice} documentation for 
further information).

\item
Pygr provides convenient query options for specifying precisely how regions
of alignment should be ``grouped'' together (e.g. treat alignment intervals
separated by indels up to a certain size as being a {\em single} alignment
region) or filtered (e.g. require a certain level of conservation over some
minimum size of alignment region).  Here's an example:
\begin{verbatim}
results=msa[site].edges(maxgap=1,maxinsert=1,
                        minAlignSize=14,pIdentityMin=0.9)
\end{verbatim}
This example groups together any number of alignment intervals separated by indels
of at most one in length, and then filters these alignment regions to
just those (sub)regions that have at least 90\% sequence identity over
a region of at least 14 residues in length.

\item
\code{src} and \code{dest} print the first two nucleotides
of the site in human and in the aligned genome.

\item
it's worth noting that the actual sequence string comparisons are being
done using a completely different database mechanism (either Pygr's
simple \code{pureseq} text format, or (before release 0.5) NCBI's \code{fastacmd}),
not the \code{cnestedlist} database.  Basically, each genome is being queried
as a separate sequence database, represented in Pygr by the
\class{BlastDB} class.  Pygr makes this complex set of multi-database
operations more or less transparent to the user.
For further information, see the \class{BlastDB} documentation.  

\item
Note: \code{exon} must itself be a slice of a sequence in our alignment, 
or the alignment query \code{msa[site]} will raise an KeyError informing
the user that the sequence \code{site} is not in the alignment.

\item
One interesting operation here is the use of interval
addition to obtain the ``union'' of two intervals, e.g. \code{ss1+ss2}.
This obtains a single interval that covers both of the input intervals.

\item
When the print  statement requests str() representations of these sequence objects, Pygr uses fseek() to extract just the right piece of the corresponding chromosomes from the 17 BLAST databases representing all the different genomes.

\item
Given the high speed of the NLMSA alignment query, it turns out that the
operation of reading sequence strings from the sequence databases (in this
case, for printing them in printConservation() and calculating the percent identity
in pIdentity()) is the rate-limiting step for this analysis.  I.e. this analysis
spends far more time waiting for disk I/O to read a particular piece of sequence 
than it does running the NLMSA alignment queries.  To solve this problem, Pygr
provides a mechanism for intelligent caching of sequence data.  Whenever you
perform a query (e.g. \code{msa[site]}), it infers that you are likely to look
at the sequence intervals that are contained within this slice of the alignment
(i.e. within the region aligned to \code{site}).  It sets ``caching hints'' on the
associated sequence databases, recording for each aligned sequence
the covering interval coordinates (i.e. the smallest interval that fully contains
all portions of the sequence that are aligned to \code{site}).  These caching hints
do not themselves trigger reading of sequence string data from the databases.  Only
when user code actually requests sequence strings that fall within these covering
intervals, the sequence database object will load not the requested interval, but
the entire covering interval, which is then cached.  Thereafter, all sequence
string requests that fall within the covering interval are simply immediately sliced
from the cached sequence string, completely avoiding any need to read from disk.
This greatly accelerates sequence analysis with very large multigenome alignments
and sequence databases.  

In this case, to enforce the most efficient caching possible, we simply performed
a query that contains all three sites of interest (ss1, ss2, and exon).  By performing
this query first, and holding onto the query result, we ensure that Pygr will
use the same cache for all three subsequent queries contained in it.  As soon
as we release the reference to this query result (i.e. in the example above,
whenever the variable \code{cacheHint} is deleted or over-written with a new value,
freeing Python to garbage-collect the original query result), the associated 
cache hint information will also be cleared.

\end{itemize}

(Actually, because of Pygr's caching / optimizations, considerably more is going on than indicated in this simplified sketch.  But you get the idea: Pygr makes it relatively effortless to work with a variety of disparate (and large) resources in an integrated way.)

Here is some example output:
\begin{verbatim}
NEED TO UPDATE THESE RESULTS
1       Mm.99996        ss1     hg17    50.0    100.0   AG      GG
1       Mm.99996        ss1     canFam1 50.0    100.0   AG      GG
1       Mm.99996        ss1     panTro1 50.0    100.0   AG      GG
1       Mm.99996        ss1     rn3     100.0   100.0   AG      AG
1       Mm.99996        ss2     hg17    100.0   100.0   AG      AG
1       Mm.99996        ss2     canFam1 100.0   100.0   AG      AG
1       Mm.99996        ss2     panTro1 100.0   100.0   AG      AG
1       Mm.99996        ss2     rn3     100.0   100.0   AG      AG
1       Mm.99996        ss3     hg17    100.0   100.0   GT      GT
1       Mm.99996        ss3     canFam1 100.0   100.0   GT      GT
1       Mm.99996        ss3     panTro1 100.0   100.0   GT      GT
1       Mm.99996        ss3     rn3     100.0   100.0   GT      GT
1       Mm.99996        e1      hg17    78.9    100.0   AG      GG
1       Mm.99996        e1      canFam1 84.2    100.0   AG      GG
1       Mm.99996        e1      panTro1 77.6    100.0   AG      GG
1       Mm.99996        e1      rn3     97.4    98.7    AG      AG
1       Mm.99996        e2      hg17    91.6    99.1    CC      CC
1       Mm.99996        e2      canFam1 88.8    99.1    CC      CC
1       Mm.99996        e2      panTro1 91.6    99.1    CC      CC
1       Mm.99996        e2      rn3     97.2    100.0   CC      CC
\end{verbatim}

\subsubsection{Working with Sequences from Databases}

Pygr provides a variety of "back-end" implementations of sequence objects, ranging from sequences stored in a relational database table, or a BLAST database, to sequences created by the user in Python (as above).  All of these provide the same consistent interface, and in general try to be efficient.  For example, Pygr sequence objects are just "placeholders" that record what sequence interval you're working with, but if the back-end is an external database, the sequence object itself does not store the sequence, and creating new sequence objects (e.g. taking slices of the object as above) will not require anything to be done on the actual sequence itself (such as copying a portion of it).  Pygr only obtains sequence information when you actually ask for it (e.g. by taking the string value str(s) of a sequence object), and normally only obtains just the portion that you ask for (i.e. str(s[1000000:1000100]) only obtains 100nt of sequence, even if s is a 100 megabase sequence.  By contrast str(s)[1000000:1000100] would force it to obtain the whole sequence from the database, then slice out just the 100 nt you selected). 

Here's an example of working with sequences from a BLAST database:

\begin{verbatim}
NEED TO UPDATE THESE RESULTS
>>> from pygr.seqdb import *
>>> db=BlastDB('sp') # open BLAST database associated with FASTA file 'sp'
>>> s=db['CYGB_HUMAN'][90:150] # get a sequence by ID, and take a slice
>>> str(s)
'TVVENLHDPDKVSSVLALVGKAHALKHKVEPVYFKILSGVILEVVAEEFASDFPPETQRA'
>>> m=db.blast(s) # get alignment to all BLAST hits in db
>>> for src,dest,edge in m.edges(): # print out the alignment edges
...     print src,repr(src),'\n',dest,repr(dest),e.pIdentity(),'\n'
... 
TVVENLHDPDKVSSVLALVGKAHALKHKVEPVYFKILSGVILEVVAEEFASDFPP CYGB_HUMAN[90:145] 
TLVENLRDADKLNTIFNQMGKSHALRHKVDPVYFKILAGVILEVLVEAFPQCFSP CYGB_BRARE[87:142] 72

TVVENLHDPDKVSSVLALVGKAHALKHKVEPVYFKILSGVILEVVAEEFASDFPPETQRA CYGB_HUMAN[90:150] 
TVVENLHDPDKVSSVLALVGKAHALKHKVEPVYFKILSGVILEVVAEEFASDFPPETQRA CYGB_HUMAN[90:150]
 120

TVVENLHDPDKVSSVLALVGKAHALKHKVEPVYFKILSGVILEVVAEEFASDFPPETQRA CYGB_HUMAN[90:150] 
TVVENLHDPDKVSSVLALVGKAHALKHKVEPMYFKILSGVILEVIAEEFANDFPVETQKA CYGB_MOUSE[90:150]
112 
...
\end{verbatim}

This example introduces the use of a Pygr alignment object to store the mapping of s onto homologous sequences in db, obtained from BLAST.  Here's what Pygr actually does:

\begin{itemize}

\item
We can construct a BlastDB object from any FASTA formatted sequence file.  A BlastDB object is just a placeholder that acts as a convenient interface to the BLAST database.  For example, it acts as a Python dictionary mapping sequence IDs to the associated sequence objects (i.e. if 'CYGB_HUMAN' is a sequence ID in sp, then db['CYGB_HUMAN'] is the sequence object for that sequence.  This object is instantiated from class BlastSequence, which provides an interface to the BLAST database.

\item 
When you work with such sequence objects, slicing etc. happens in the usual way, creating new sequence objects.

\item  
Only when you ask for actual sequence (by taking str(s)) does it obtain a sequence string from the database.  This is done using fseek() system call to obtain just the selected slice.  So you can efficiently obtain a substring of a sequence, even if that sequence is an entire chromosome.

\item
When you first create the BlastDB object, it looks for existing BLAST database files associated with the FASTA file 'sp'.  If present, it uses them.  If not, it will create them automatically if the user actually tries to run a BLAST query.  Pygr builds BLAST database files using the NCBI program formatdb (Pygr figures out whether the sequences are nucleotide or protein, and gives formatdb the appropriate command line options).  

\item 
When you invoke the db.blast() method on a sequence object, it obtains the actual string of the object, and uses it to run a BLAST search.  It determines the type (nucleotide or protein) of the sequence object, and uses the appropriate search method (in this case blastp).  You can pass optional arguments for controlling BLAST.  It then reads the results into a Pygr multiple sequence alignment object, which stores the alignments as sets of matched intervals.  Specifically, it is a graph, whose nodes are sequence intervals (i.e. sequence objects that typically represent only part of a sequence), and whose edges represent an alignment between a pair of intervals.  To illustrate this, we ran a for-loop over all the "edge relations" in this graph, and printed them out.  This is a tuple of 3 values: \code{src} and \code{dest} are the two aligned sequence intervals, and \code{edge} provides a convenient interface to information about their relationship (e.g. \%identity, etc.).  

\item
Note: print converts its arguments to strings (i.e. calls str() on them), so we used \code{repr(src)} to get a "string representation" of each sequence interval.  When print calls str() on individualBlastSequence interval objects returned by the BLAST search, they invoke fseek() to obtain the specific sequence slice representing that interval.
\end{itemize}

\subsection{Simplifying the Challenges of Working with Complex Datasets}
\subsubsection{pygr.Data: a Namespace for Transparently Importing Data}
One challenge in bioinformatics is the complexity of managing many diverse
data resources.  For example, running a large job on a heterogeneous cluster
of computers is complicated by the fact that individual computers often can't
access a given data resource in the same way (i.e. the file path may be different),
and some machines may not have direct access at all to certain resources.

Pygr provides a systematic solution to this problem: creating a consistent
namespace for data.  A given resource is given a unique name that then becomes
its universal handle for accessing it, no matter where you are (just as Python's
\code{import} command provides a consistent name for accessing a given code
resource, regardless of where you are).  For example, say we want to add the
hg17 (release 17 of the human genome sequence) as ``Bio.Seq.Genome.hg17''
(the choice of name is arbitrary, but it's best to choose a good convention and follow
it consistently):

\begin{verbatim}
from pygr import seqdb
import pygr.Data # MODULE PROVIDES ACCESS TO OUR DATA NAMESPACE
pygr.Data.Bio.Seq.Genome.hg17=seqdb.BlastDB('hg17') # SAVE AS THIS NAME
\end{verbatim}

In any subsequent Python session, we can now access it directly by its
pygr.Data name:
\begin{verbatim}
import pygr.Data # MODULE PROVIDES ACCESS TO OUR DATA NAMESPACE
hg17=pygr.Data.Bio.Seq.Genome.hg17() # FIND THE RESOURCE
\end{verbatim}
The call syntax (\code{hg17()}) emphasizes that this acts like a Python
constructor: it constructs a Python object for us (in this case, the
desired seqdb.BlastDB object representing this genome database).
Note that we did {\em not} even have to know how to construct the hg17 
object, e.g. what Python class to use (seqdb.BlastDB), or even to import
the necessary modules for constructing it.  \code{pygr.Data} uses the
power of Python pickling to figure out automatically what to import.
pygr.Data looks at the environment variable PYGRDATAPATH to get a list 
of local and remote resource databases in which to look up any resource name
that you try to load.  For example, in the shell you might set:
\begin{verbatim}
setenv PYGRDATAPATH ~,.,/usr/local/pygr,mysql:PYGRDATA.index,http://leelab.mbi.ucla.edu:5000
\end{verbatim}
This is a comma-separated string (since colon ':' appears inside URLs).
In this case it tells pygr.Data to look for resource databases (in order):
\code{\$HOME/.pygr_data}; \code{./.pygr_data}; \code{/usr/local/pygr/.pygr_data};
the MySQL table PYGRDATA.index (using your
MySQL .my.cnf file to determine the MySQL host and authentication);
and the XMLRPC server running on leelab.mbi.ucla.edu on port 5000.

pygr.Data is smart about figuring out data resource dependencies.
For example, you could just save a 17-genome alignment in a single step
as follows:
\begin{verbatim}
from pygr import cnestedlist
import pygr.Data # MODULE PROVIDES ACCESS TO OUR DATA NAMESPACE
pygr.Data.Bio.Seq.MSA.ucsc17=cnestedlist.NLMSA('/loaner/ucsc17')
\end{verbatim}
This works, even though using this 17-genome alignment (behind the
scenes) involves accessing 17 BlastDB sequence databases (one for each
of the genomes in the alignment).  Because the alignment object (NLMSA)
references the 17 BlastDB databases, pygr.Data automatically saves information
about how to access them too.

However, it would be a lot smarter to give those databases pygr.Data resource
names too.  Let's do that:
\begin{verbatim}
from pygr import cnestedlist
import pygr.Data # MODULE PROVIDES ACCESS TO OUR DATA NAMESPACE
nlmsa=cnestedlist.NLMSA('/loaner/ucsc17')
for id,genome in nlmsa.seqDict.prefixDict.items(): # 1st SAVE THE GENOMES
    pygr.Data.getResource.addResource('Bio.Seq.Genome.'+id,genome)
pygr.Data.Bio.Seq.MSA.ucsc17=nlmsa # NOW SAVE THE ALIGNMENT
\end{verbatim}

This has several advantages.  First, we can now access other genome databases
using pygr.Data too:
\begin{verbatim}
import pygr.Data # MODULE PROVIDES ACCESS TO OUR DATA NAMESPACE
mm7=pygr.Data.Bio.Seq.Genome.mm7() # GET THE MOUSE GENOME
\end{verbatim}
But more importantly, when we try to load the ucsc17 alignment on
another machine, if the genome databases are not in the same directory
as on our original machine, the first method above would fail, whereas in
the second approach pygr.Data now will automatically scan all its resource databases to
figure out how to load each of the genomes on that machine.

\subsubsection{pygr.Data.schema: a Simple Framework For Managing Database Schemas}
{\em Schema} refers to any relationship between two or more collections of
data.  It captures the structure of relationships that define these particular
kinds of data.  For example ``a genome has genes, and genes have exons'', or 
``an exon is connected to another exon by a splice''.  In pygr.Data we can
store such schema information as easily as:
\begin{verbatim}
pygr.Data.Bio.Genomics.ASAP2.hg17.splicegraph=splicegraph # ADD A NEW RESOURCE
pygr.Data.schema.Bio.Genomics.ASAP2.hg17.splicegraph= \
  pygr.Data.ManyToManyRelation(exons,exons,splices, # ADD ITS SCHEMA RELATIONS
                               bindAttrs=('next','previous','exons'))
\end{verbatim}
This example assumes that 
\begin{itemize}
\item \code{splicegraph} is a graph whose nodes are exons, and whose
edges are splices connecting a pair of exons.  Specifically,
\code{splicegraph[exon1][exon2]=splice1} means \code{splice1} is a 
splice object (from the container \code{splices}) that connects
\code{exon1} and \code{exon2} (both from the container \code{exons}).

\item An exon can have one or more ``outgoing'' splices connecting it
to subsequent exons, as well as one or more ``incoming'' splices from
previous exons.  Thus this relation of exon to exon is a Many-to-Many
mapping (e.g. as distinguished from a One-to-One mapping, where each
exon must have exactly one such relationship with another exon).

\item Because pygr.Data now knows the schema for splicegraph, it
will automatically reconstruct these relationships for any user who
accesses these data from pygr.Data.  Specifically, if a user
retrieves \code{pygr.Data.Bio.Genomics.ASAP2.hg17.splicegraph},
the \code{sourceDB}, \code{targetDB}, \code{edgeDB} attributes on
the returned object will automatically be set to point to the 
corresponding pygr.Data resources representing \code{exons} and \code{splices}
respectively.  \code{splicegraph} does not need to do anything to 
remember these relationships; pygr.Data.schema remembers and applies
this information for you automatically.  Note that when you access
\code{splicegraph}, neither \code{exons} nor \code{splices} will be 
actually loaded unless you do something that specifically tries to
read these data (e.g. \code{for exon in splicegraph} will read
\code{exons} but not \code{splices}).

\item The easiest way for users to work with a schema is to translate
it into object-oriented behavior.  I.e. instead of remembering that
when we have \code{exons} we can use \code{splicegraph} to find its
\code{splices} via code like 
\begin{verbatim}
for exon,splice in splicegraph[exon0].items():
   do something...
\end{verbatim}
most people would find it easier to remember that every \code{exon}
has a \code{next} attribute that gives its splices to subsequent exons
via code like
\begin{verbatim}
for exon,splice in exon0.next.items():
   do something...
\end{verbatim}
Based on the schema statement we gave it,
pygr.Data.schema will automatically create the attributes \code{next},
\code{previous} on any exon item from the container \code{exons},
according to the schema.  I.e. \code{exon.next} will be equivalent to
\code{splicegraph[exon]}.  Note that as long as the object \code{exon0}
came from the pygr.Data resource, the user {\em would not have to do anything}
to be able to use the \code{next} attribute.  On the basis of the saved
schema information, pygr.Data will construct this attribute automatically,
and will automatically load the resources \code{splicegraph} and \code{splices}
if the user tries to actually use the \code{next} attribute.
\end{itemize}

\subsubsection{pygr.Data Sharing Over a Network via XMLRPC}
Sometimes individual compute nodes may not have sufficient disk space to
store all the data resources (for example, just the single the ucsc17 17-genome alignment and 
associated genome databases takes about 200 GB).  Yet it would be useful
to run compute-intensive analyses on those machines accessing such data.
pygr.Data makes that easy.  We first create an XMLRPC server on a machine that
has access to the data:

\begin{verbatim}
import pygr.Data
nlmsa=pygr.Data.Bio.Seq.MSA.ucsc17() # GET OUR NLMSA AND SEQ DBs
server=pygr.Data.getResource.newServer('nlmsa_server') # SERVE ALL LOADED DATA
server.register() # TELL PYGRDATA INDEX SERVER WHAT RESOURCES WE'RE SERVING
server.serve_forever() # START THE SERVICE...
\end{verbatim}

This example code looks for a pygr.Data XMLRPC server in your PYGRDATAPATH,
and registers our resources to that index.  Now any machine that can access
your servers can access the alignment as easily as:
\begin{verbatim}
import pygr.Data
nlmsa=pygr.Data.Bio.Seq.MSA.ucsc17() # GET THE NLMSA AND SEQ DBs
\end{verbatim}
Alignment queries and sequence strings will be obtained via XMLRPC 
queries over the network.  Note that if any of the sequence databases
{\em are} available locally (on this machine), Pygr will automatically use that 
in preference to obtaining it over the network (based on your PYGRDATAPATH
settings).  However, if a particular resource is not available locally,
Pygr will transparently get access to it from the server we created,
using XMLRPC.

\subsubsection{pygr.Data Layers}
Based on your PYGRDATAPATH, pygr.Data provides a number of named {\em layers}
that give abstract names for where you want to read or store your pygr.Data info.
For example, if you wanted to store a resource specifically in the resource
database in your current directory, you could type:
\begin{verbatim}
pygr.Data.here.Bio.Seq.MSA.ucsc17=nlmsa # SAVE THE NLMSA AND SEQ DBs
\end{verbatim}
\begin{itemize}
\item The abstract pygr.Data layer \code{here} refers to the first entry in your
PYGRDATAPATH that starts with ``.'' (dot).  For other layer names, see
the reference documentation.  This might be useful for prototyping or
testing a new resource, without yet adding it to your long-term resource
database.

\item Similarly, the pygr.Data layer
\code{my} is the first entry that begins with your home directory
(i.e. ~ (tilde), ``/home/yourname'' or whatever your home directory is).

\item Every pygr.Data resource database server (XMLRPC or MySQL) has
a ``layer name'' that will be automatically loaded to your pygr.Data module
when you import it.  For example, to delete this particular resource rule
from our lab's central resource database (called ``leelab'', because it is
not accessible outside our lab):
\begin{verbatim}
del pygr.Data.leelab.Bio.Seq.MSA.ucsc17 # DELETE THIS RESOURCE RULE
\end{verbatim}
\end{itemize}


\subsection{Storing Alignments}
\subsubsection{Alignment Basics}

Pygr multiple alignment objects can be treated as mappings of sequence intervals onto sequence intervals.  Here is a very simple example, showing basic operations for constructing an alignment:

\begin{verbatim}
>>> from pygr import cnestedlist
>>> m2=cnestedlist.NLMSA('myo',mode='memory') # an empty alignment, in-memory
>>> m2+=s # add sequence s to the alignment
>>> ival=s[100:160] # AN INTERVAL OF s
>>> m2[ival]+=db['MYG_CHICK'][83:143] # add an edge mapping interval s -> an interval of MYG_CHICK
>>> m2[ival]+=db['MYG_CANFA'][45:105] # add an additional edge
>>> m2.build() # done constructing the alignment.  Initialize for query.
>>> for s2 in m2[ival[:10]]: # get aligned seqs for the first 10 letters of ival...
...     print repr(s2)
...
MYG_CHICK[83:93]
MYG_CANFA[45:55]
\end{verbatim}
In this case we used in-memory storage of the alignment.  

\subsubsection{Storing an All-vs-All BLAST Alignment}
However, for really large
alignments (e.g. an all vs. all BLAST analysis) we may prefer to store the alignment
on-disk.  In pygr, all we have to do is change the mode flag to 'w' (implying {\em write}
a file):
\begin{verbatim}
from pygr import cnestedlist,seqdb
msa=cnestedlist.NLMSA('all_vs_all',mode='w',bidirectional=False) # ON-DISK
sp=seqdb.BlastDB('sp') # OPEN SWISSPROT DATABASE
for id,s in sp.iteritems(): # FOR EVERY SEQUENCE IN SWISSPROT
    sp.blast(s,msa,expmax=1e-10) # GET STRONG HOMOLOGS, SAVE ALIGNMENT IN msa
msa.build() # DONE CONSTRUCTING THE ALIGNMENT, SO BUILD THE ALIGNMENT DB INDEXES
# msa READY TO QUERY NOW...
\end{verbatim}
Again you can see how pygr makes it quite simple to do a large analysis
and create a powerful resource (an all-vs-all alignment database).
A couple of points deserve comment:

\begin{itemize}

\item
The in-memory and on-disk NLMSA alignment storages have exactly the same
interface.  You can work with alignments from small to large to gimungous
using the same consistent set of tools.  Moreover the performance will be
fast across the whole range of scales, because the NLMSA storage and query
algorithms scale very well (O(logN)).

\item
Because of the \code{mode='w'} flag, NLMSA will create a set of alignment
index files called 'all_vs_all'.

\item 
\code{bidirectional=False}: whenever you store an alignment relationship
$S \rightarrow T$, this can either be {\em unidirectional} or {\em bidirectional}.
Unidirectional means only $S \rightarrow T$ is stored; bidirectional means
both $S \rightarrow T$ and $T \rightarrow S$ are stored (i.e. you can both query
with S (and get T), and query with T (and get S).  In general, you want 
a unidirectional alignment storage when {\em directionality matters}.  For
example, in a BLAST all vs. all search the alignment of S and T that you get
when you blast S against the database (finding T, among others) may well be
different from the alignment of S and T that you get when you blast T against
the database (finding S, among others).  If you stored the all-vs-all alignment
using bidirectional storage, querying \code{msa} with S would get TWO alignments 
to T: one from the $S \rightarrow T$ BLAST search results, and one from the
$T \rightarrow S$ BLAST search results.  This simply reflects the fact that 
the all vs all BLAST stored two alignments of S and T into \code{msa}.
What this highlights is that BLAST is not a true multiple sequence alignment
algorithm (among other things, it is not symmetric: you can get different
mappings in one direction vs. the other).  

In general, bidirectional storage
mainly makes sense for true multiple sequence alignments (which are guaranteed
to be symmetric).

\item
Supplying the BlastDB.blast() method with an alignment object makes it store
its results into that alignment, rather than creating its own alignment holder
for us.  In this way we can make it store many different BLAST searches into
a single alignment database.

\item
To make the NLMSA algorithm scalable, pygr defers construction of the alignment
indexes until the alignment is complete.  We trigger this by calling its build()
method.  At this point we now have an alignment database stored on disk, which
we can open at any time later and query with the high-speed nested list algorithm
as illustrated in the examples in previous sections.

\end{itemize}


\subsubsection{Building an Alignment Database from MAF files}
It may be helpful to see how a large multi-genome alignment database
is created in Pygr.  It is quite straightforward.
UCSC has defined a new file format for large multigenome alignment,
called MAF.  Pygr provides high-performance utilities for reading
MAF alignment files and building a disk-based NLMSA alignment database.
(These utilities are written in C for performance).  Here's an
example of building an alignment database from scratch using a
set of MAF files stored in a directory called \code{maf/}:

\begin{verbatim}
import os
from pygr import cnestedlist,seqdb

genomes={'hg17':'hg17','mm5':'mm5', 'rn3':'rn3', 'canFam1':'cf1', 
         'danRer1':'dr1', 'fr1':'fr1','galGal2':'gg2', 'panTro1':'pt1'}
for k,v in genomes.items(): # PREFIX DICTIONARY FOR UNION OF GENOMES
    genomes[k]=seqdb.BlastDB(v) # USE v AS FILENAME FOR FASTA FILE
genomeUnion=seqdb.PrefixUnionDict(genomes) # CREATE UNION OF THESE DBs
# CREATE NLMSA DATABASE ucsc8 ON DISK, FROM MAF FILES IN maf/
msa=cnestedlist.NLMSA('ucsc8','w',genomeUnion,os.listdir('maf'))
\end{verbatim}

The only real work here is due to the fact that UCSC's MAF files
use a {\em prefix.suffix} notation for identifying specific sequences,
where {\em prefix} gives the name of the genome, and {\em suffix}
gives the identifier of the sequence in that genome database.
Here we use Pygr's \class{PrefixUnionDict} class to wrap the 
set of genome databases in a dict-like interface that accepts
string keys of the form {\em prefix.suffix} and returns the
right sequence object from the right genome database.  As an
added twist, the genome names in the MAF files match the
filenames of the associated genome databases in most cases, but
not all, so we have to create an initial dictionary giving the
correct mapping.  Actually building the NLMSA requires just one
line, but actually a number of steps are happening behind the
scenes:
\begin{itemize}
\item If you have never opened \class{BlastDB} objects for these genome
databases before, \class{BlastDB} will initialize each one.  This means
two things.  First, it builds an index of all the sequences and their 
lengths.  This is essential for combining the
large numbers of sequences in these databases into 
``unified'' coordinate systems in the NLMSA (otherwise there would
have to be a separate database file for each individual sequence).
Second, it saves the sequences to a simple indexed file format that
allows Pygr to retrieve individual sequence fragments quickly and
efficiently.  We got tired of NCBI \code{fastacmd}'s horrible
memory requirements and slow speed, so we implemented fast sequence
indexing.

\item \class{NLMSA} reads each MAF file and divides the interval
alignment data into one or more coordinate systems created 
on-the-fly (for efficient memory usage, NLMSA uses \class{int}
coordinates (32-bit), which has a maximum size of approximately
2 billion.  This is too small even for a single genome like human;
\class{NLMSA} automatically splits the database into as many
coordinate systems are needed to represent the alignment.
Each coordinate system has its own database file on disk.

\item After it has finished reading the MAF data, \class{NLMSA}
begins to build the database indexes for each coordinate 
system.  Computationally, this operation is equivalent to
a {\em sort} (N log N complexity).  Once the indexes are built, the database is
ready for use.
\end{itemize}
	
\subsubsection{Example: Mapping an entire gene set onto a new genome version}
To illustrate how Pygr can perform a big task with a little code, here is an example that maps a set of gene sequences onto a new version of the genome, using megablast to do the mapping, and a relational database to store the results.  Moreover, since mapping 80,000 gene clusters takes a fair amount of time, the calculation is parallelized to run over a large number of compute nodes simultaneously:

\begin{verbatim}
import pygr.Data
from pygr.apps.leelabdb import * # this accesses our databases
from pygr import coordinator     # this provides parallelization support

def map_clusters(server,dbname='HUMAN_SPLICE_03',
                 result_table='GENOME_ALIGNMENT.hg17_cluster_JUN03_all',
                 rmOpts='',**kwargs):
    "CLIENT FUNCTION: map clusters one by one"
    # CONSTRUCT RESOURCE FOR US IF NEEDED
    genome=pygr.Data.Bio.Seq.Genome.hg17()
    # LOAD DB SCHEMA
    (clusters,exons,splices,genomic_seq,spliceGraph,alt5Graph,alt3Graph,mrna, 
    protein, clusterExons,clusterSplices)=getSpliceGraphFromDB(spliceCalcs[dbname])
    # NOW MAP CLUSTER SEQUENCES ONE BY ONE TO OUR NEW genome
    for cluster_id in server:
        g=genomic_seq[cluster_id] # GET THE OLD GENOMIC SEQUENCE FOR THIS CLUSTER
        m=genome.megablast(g,maxseq=1,minIdentity=98,rmOpts=rmOpts) # MASK, BLAST, READ INTO m
        # SAVE ALIGNMENT m TO DATABASE TABLE result_table USING cursor
        createTableFromRepr(m.repr_dict(),result_table,clusters.cursor,
                            {'src_id':'varchar(12)','dest_id':'varchar(12)'})
        yield cluster_id # WE MUST FUNCTION AS GENERATOR TO KEEP ERROR TRAPPING 
		         # HAPPY

def serve_clusters(dbname='HUMAN_SPLICE_03',
                   source_table='HUMAN_SPLICE_03.genomic_cluster_JUN03',**kwargs):
    "SERVER FUNCTION: serve up cluster_id one by one to as many clients as you want"
    cursor=getUserCursor(dbname)
    t=SQLTable(source_table,cursor)
    for id in t:
        yield id # HAND OUT ONE CLUSTER ID TO A CLIENT

if __name__=='__main__': # AUTOMATICALLY RUN EITHER THE CLIENT OR SERVER FUNCTION
    coordinator.start_client_or_server(map_clusters,serve_clusters,[],__file__)
\end{verbatim}

First, let's just focus on the map_clusters() function, which illustrates how the mapping of each gene is generated and saved.  Let's examine the data piece by piece:
\begin{itemize}

\item
genome: a BLAST database storing our hg17 genome sequence

\item
genomic_seq: another sequence database (which in this case happens to be stored in a relational database), mapping each cluster ID to a piece of the old genomic sequence version containing that specific gene.

\item   
cluster_id: a cluster ID for us to process.

\item
g: the actual sequence object associated with this cluster_id

\item
m: the mapping of g onto genome, as generated by megablast after first running RepeatMasker on g, using the RepeatMasker options passed as rmOpts.  Note that only the top hit will be saved (maximum number of hits to save maxseq=1), and only if it has at least 98\% identity.  This alignment is then saved to a relational database table using createTableFromRepr().

\end{itemize}
This code will run in parallel over as many compute nodes as you have free, using Pygr's coordinator module.  The parallelization model for this particular task is simple: a single iterator (server) dispensing task IDs to many clients. 

\begin{itemize}

\item
server: the serve_clusters() function is trivial: all it does is connect to a specific database table (source_table) and iterate over all its primary keys, yielding them one by one.

\item    
client: the map_clusters() function expects an iterator as its first argument, which must give it a sequence of task IDs (cluster_id in this script).  This iterator is actually using an XMLRPC request to the server to get the next task ID, but that is done transparently by the coordinator.Processor() class.  The map_clusters() function is modeled as a generator: that is, it first does some initial setup (loading the database schema for example), then it runs its actual task loop, yielding each completed task ID. This enables coordinator.Processor to run map_clusters() within an error-trapping try: except: clause that catches and reports all errors to the central coordinator.Coordinator instance, and also to implement some intelligent error handling policies (like robustly preventing rare individual errors from causing an entire Processor() to crash, but detecting when consistent patterns of errors occur on a particular Processor, and automatically shutting down that Processor.

\item 
start_client_or_server(): this line automatically starts up the correct function (depending on whether this process is running as client or server).  To make a long story short, all you have to do is run the script once (as a server), and it will automatically start clients for you on free compute nodes (using ssh-agent), with reasonable load-balancing and queuing policies.  For details, see the coordinator module docs.
\end{itemize}


\subsection{Sequence Annotation Databases}
\subsubsection{What is a pygr sequence annotation?}
{\em Annotation} -- information bound to specific intervals of a genome
or sequence -- is an essential concept in bioinformatics.  We would like to
be able to store and query annotations naturally as part of working with
multi-genome alignments, as a standard operation in comparative genomics.
Pygr makes this easy:
\begin{verbatim}
for alignedRegion in msa[myRegion]: # FIND ALIGNMENT IN OTHER GENOMES
  for exon in alignedRegion.exons: # SEE IF THIS CONTAINS ANY ANNOTATED EXONS
    print 'exon\tID:%d\tSEQ:%s' % (exon.id,str(exon)) # PRINT ITS SEQUENCE
    for exon2,splice in exon.next.items(): # LOOK AT ALTERNATIVE SPLICING OF THIS EXON
      do something...
\end{verbatim}
\begin{itemize}
\item In the above code, we assumed that there exists a mapping of any genomic
sequence region (\code{alignedRegion}) to exon annotations.  This mapping
is bound by pygr.Data.schema to the sequence object's \code{exons} attribute.
In a moment we will see how to construct such a mapping.

\item An annotation is {\em both} a sequence interval (just like any other
sequence interval in pygr, e.g. you can get its sequence string via \code{str}),
and also an annotation object.  What makes it an annotation object?  

\item Pygr marks
it with one attribute (\code{annot}) that points to its parent annotation
object.  For example, in the case above, \code{alignedRegion} might not contain
the whole exon, in which case \code{exon} would be just the part of the exon
contained in \code{alignedRegion}, and \code{exon.annot} would be the complete
exon.

\item In addition, pygr also marks it with 
two attributes that identify it as belonging to its annotation database:
\code{id} gives its unique identifier (primary key) in that database,
and \code{db} points to the annotation database object itself.

\item Because pygr can see that \code{exon} is part of the exons annotation database,
it can apply schema information automatically to it.  In this particular case,
it applies the splicegraph schema to it (see example from pygr.Data.schema
tutorial above), so we can find out what exons it splices to via its \code{next}
attribute.
\end{itemize}


\subsubsection{Constructing an Annotation Database}
Suppose you had a set of annotations \code{sliceDB} each consisting of a sequence ID,
start, and stop coordinates.  We can easily construct an annotation database
from this:
\begin{verbatim}
from pygr import seqdb,cnestedlist
annoDB=seqdb.AnnotationDB(sliceDB,genome) # CREATE THE ANNOTATION DB
nlmsa=cnestedlist.NLMSA('exonAnnot','w', # STORE SEQ->ANNOT MAPPING AS AN ALIGNMENT
                        use_virtual_lpo=True,bidirectional=False)
for a in annoDB.itervalues(): # SAVE ALL ANNOTATION INTERVALS
  nlmsa.addAnnotation(a) # ADD ALIGNMENT BETWEEN ival AND ann INTERVALS
nlmsa.build() # WRITE INDEXES FOR THE ALIGNMENT
pygr.Data.Bio.Genomics.ASAP2.exons=annoDB # ADD AS A PYGR.DATA RESOURCE
pygr.Data.Bio.Genomics.ASAP2.exonmap=nlmsa # NOW SAVE MAPPING AND SCHEMA
pygr.schema.Data.Bio.Genomics.ASAP2.exonmap= \
      pygr.Data.ManyToManyRelation(genome,annoDB,bindAttrs=('exons'))
\end{verbatim}
\begin{itemize}
\item NLMSA provides an efficient, high-performance way to store and
query huge annotation databases.  The mapping is stored on disk but is
accessed with high-speed indexing.

\item More importantly, however, a pygr.Data user need never even be
aware that an NLMSA is being used to provide this mapping.  As far as
users are concerned, all they need to know is that any sequence object from \code{genome}
has an \code{exons} attribute that automatically gives a list of exon
annotations contained within that sequence.  I.e. as in the previous 
example, you would simply access it via:
\begin{verbatim}
for exon in someRegion.exons:
  do something...
\end{verbatim}

\item The \code{ManyToManyRelation} indicates that \code{nlmsa} should
be interpreted as being a many-to-many relation from items of \code{genome}
to items of \code{annoDB}.  It also creates an \code{exons} attribute on
items of \code{genome} that translates to \code{g.exons=nlmsa[g]}.

\item In the above example, we assumed that \code{genome} was obtained
from pygr.Data (and thus a pygr.Data resource ID).  If not, we would first
have to add it, just as we did for \code{annoDB}.

\item Note that we only bound an attribute (\code{exons}, to the 
\code{genome} items) for the forward mapping (from \code{genome} to \code{annoDB}).
We did not even store the reverse mapping in \code{nlmsa}, because
it is completely trivial.  (i.e. an annotation from \code{annoDB} is itself
the interval of \code{genome} that it maps to).  This was set by
the \code{bidirectional=False} option to the \code{NLMSA}.

\item The \code{use_virtual_lpo} option indicates that this is a pairwise
(sequence to sequence) alignment, not a true multiple sequence alignment
(which requires its own coordinate system, called an LPO; see the reference
docs on NLMSA for more information).  This option could have been omitted;
pygr would have figured it out automatically from the fact that we saved 
direct alignments of sequence interval pairs to \code{nlmsa}.  NLMSA does not
permit mixing pairwise and true MSA alignment formats in a single NLMSA.
\end{itemize}

\subsubsection{Constructing an Annotation Mapping using Megablast}
What if someone provided you with a set of ``exon annotations'' in the form
of short sequences representing the exons, rather than actual genomic
coordinates?  Again, pygr makes this mapping extremely easy to save:
\begin{verbatim}
from pygr import seqdb,cnestedlist,sequence
annoDB=seqdb.AnnotationDB({},genome) # CREATE THE ANNOTATION DB
nlmsa=cnestedlist.NLMSA('exonAnnot','w', # STORE SEQ->ANNOT MAPPING AS AN ALIGNMENT
                        use_virtual_lpo=True,bidirectional=False)
for id,s in exonSeqs.items(): # SAVE ALL ANNOTATION INTERVALS
  ann=sequence.Sequence(s,id) # CREATE A SEQUENCE OBJECT
  ann.db=annoDB # FORCE IT TO BE PART OF THE ANNOTATION DB
  genome.megablast(ann,nlmsa,maxseq=1,minIdentity=98) # SAVE ITS BEST HIT IN nlmsa
nlmsa.build() # WRITE INDEXES FOR THE ALIGNMENT
pygr.Data.Bio.Genomics.ASAP2.exons=annoDB # ADD AS A PYGR.DATA RESOURCE
pygr.Data.Bio.Genomics.ASAP2.exonmap=nlmsa # NOW SAVE MAPPING AND SCHEMA
pygr.schema.Data.Bio.Genomics.ASAP2.exonmap= \
      pygr.Data.ManyToManyRelation(genome,annoDB,bindAttrs=('exons'))
\end{verbatim}

\begin{itemize}
\item This example assumes \code{exonSeqs} is a dictionary of exon IDs and sequence
strings.

\item We construct a sequence object \code{ann} that looks like an annotation
object (i.e. an item of the initially empty container \code{annoDB}) 
to pygr because it has an \code{id} and \code{db} attributes.

\item We then find the best match using megablast (using proper repeat masking
for alignment seeding, but propagating alignment through repeats).  Since we provide
our own alignment object \code{nlmsa}, it will use that to save the best hit.

\item We save resource and schema information as before.

\item
\end{itemize}

\subsection{Using Pygr as a Graph Database}
The real power of Pygr is that it provides a simple model for viewing
all data as {\em graph databases}-- in which all data are represented
as nodes and connections between nodes (edges), and queries are formulated
as a specific pattern of connections to find --
in a very Pythonic style.  To illustrate the simplicity and power
of the graph database approach, Pygr has a strong emphasis
on bioinformatics applications ranging from genome-wide analysis of
alternative splicing patterns, to comparative genomics queries of
multi-genome alignment data.

The following introductory examples show how to use Pygr for graph queries, sequence searching and alignment queries, annotation queries, and multigenome alignment queries.


\subsubsection{Example: Simple graph query}
Why would you want to use Pygr?  Interesting data often consists of specific graph structures, and these relationships are much easier to describe as graphs than they are in SQL.  For example, the simplest and most common form of alternative splicing is exon-skipping, where an exon is either skipped or included (see slide 15 of the ISMB slides for a picture).  This can be defined immediately as a graph in which three nodes (exons 1, 2, 3) are joined by edges either as 1-2-3 or 1-3.  Unfortunately, writing an SQL query for this simple pattern requires a 6-way JOIN (argh).

\begin{verbatim}
SELECT * FROM exons t1, exons t2, exons t3, splices t4, splices t5, splices t6 
WHERE t1.cluster_id=t4.cluster_id AND t1.gen_end=t4.gen_start 
  AND t4.cluster_id=t2.cluster_id AND t4.gen_end=t2.gen_start 
  AND t2.cluster_id=t5.cluster_id AND t2.gen_end=t5.gen_start 
  AND t5.cluster_id=t3.cluster_id AND t5.gen_end=t3.gen_start 
  AND t1.cluster_id=t6.cluster_id AND t1.gen_end=t6.gen_start 
  AND t6.cluster_id=t3.cluster_id AND t6.gen_end=t3.gen_start;
\end{verbatim}

Such a six-way JOIN is painfully slow in a relational database; in general such queries just aren't practical.  More fundamentally, the relational schema is forced to represent the graph relation with combinations of foreign keys and other data that the user really should not have to remember.  All the user should know is that there is a specific relation, e.g. from this exon, the "next" exon is X, and the relation joining them is splice Y.

In Pygr, writing the query is just a matter of writing down the graph (edges from 1 to 2, 1 to 3, and 2 to 3, but no special "edge information"):

\begin{verbatim}
queryGraph={1:{2:None,3:None},2:{3:None},3:{}}
\end{verbatim}

We can now execute the query using the GraphQuery class:

\begin{verbatim}
results=[dict(m) for m in GraphQuery(spliceGraph,queryGraph)]
\end{verbatim}

This is more or less equivalent to writing a bunch of for-loops for iterating over the possible closures:

\begin{verbatim}
results=[]
for e1 in spliceGraph: # FIND ALL EXONS
    for e2 in spliceGraph[e1]: # NEXT EXON
        for e3 in spliceGraph[e2]: # NEXT EXON
            if e3 in spliceGraph[e1]: # MAKE SURE SPLICE FROM e1 -> e3
                results.append({1:e1,2:e2,3:e3}) # OK, SAVE MATCH
\end{verbatim}

It is often convenient to bind an object attribute to a graph, so that you can use either the graph syntax or a traditional object attribute and mean exactly the same thing.  In the splice graph example, we bind the exon.next attribute to the spliceGraph, so the above for-loops can also be written:

\begin{verbatim}
results=[]
for e1 in spliceGraph: # FIND ALL EXONS
    for e2 in e1.next: # NEXT EXON
        for e3 in e2.next: # NEXT EXON
            if e3 in e1.next: # MAKE SURE SPLICE FROM e1 -> e3
                results.append({1:e1,2:e2,3:e3}) # OK, SAVE MATCH
\end{verbatim}

Another interesting query in the alternative splicing field is the so-called U12-adapter exon query (see slide 21 of the ISMB slides):

\begin{verbatim}
queryGraph={0:{1:dict(dataGraph=alt5Graph),
               2:dict(filter=lambda edge,**kwargs:edge.type=='U11/U12')},
            1:{3:None},
            2:{3:None},
            3:{}}
\end{verbatim}

Here we use edge information in the query graph to add a few constraints:

\begin{itemize}
\item
the dataGraph argument tells the query to search for exon 1 from exon 0 using a different graph (alt5Graph).

\item    
the filter argument provides a function that returns True only if the edge between exon 0 and exon 2 is of type U11/U12.  Therefore the query will only match sp
lice graphs that have a U12 splice between this pair of exons.

\end{itemize}

Note that the query graph "nodes" (in this example, the integers 0, 1, 2, 3) are
quite arbitrary.  We could have used strings, or other kinds of objects instead.

Now if we want to see the results right away, we use the mapping returned by GraphQuery to look at individual nodes and edges of the dataGraph that matched our query:

\begin{verbatim}
for m in GraphQuery(spliceGraph,queryGraph):
    print m[1].id,m[0,2].id # PRINT EXON ID FOR EXON 1,
                            # SPLICE ID FOR SPLICE 0 -> 2
\end{verbatim}

The match is returned by GraphQuery as a mapping from nodes and edges of the query graph to nodes and edges of the data graph.  Edges are specified simply as tuples of the nodes you want to get the edge for (in this example 0,2).
Constructing a Graph
How was the spliceGraph created in the first place?  Let's say we have an initial list of tuples giving connections between exon objects and splice objects, where each tuple consists of a pair of exons connected by a splice.

\begin{verbatim}
for exon1,exon2,splice in spliceConnections: 
    spliceGraph+=exon1 # add exon1 as a node in the graph
    spliceGraph+=exon2 # if already a node in the graph, does nothing...
    exon1.next[exon2]=splice # add an edge, with splice as the edgeinfo
\end{verbatim}

The last operation makes use of the binding of exon.next to spliceGraph, and is equivalent to

\begin{verbatim}
spliceGraph[exon1][exon2]=splice
\end{verbatim}

If we didn't want to save the edge information, we could use the simpler syntax

\begin{verbatim}
spliceGraph[exon1]+=exon2 # equivalent to exon1.next+=exon2
\end{verbatim}

This "short" form is equivalent to saving None as the edge information.


\subsubsection{Alignment Query as a Graph Database Query}

Pygr can do much more sophisticated analyses than this fairly easily.  
Just to give a taste of how to use these capabilities, we will illustrate
one example of a standard Pygr model: querying Pygr data by drawing a ``query
graph'' showing the connections we want to find, and running its GraphQuery()
engine.  Since Pygr alignments follow the same interface as any Pygr graph, we can query them using the standard GraphQuery class.  Let's say we have a Python script load_alignments.py that loads two alignments:

\begin{itemize}

\item
mRNA_swiss: an alignment of mRNA sequences to homologous SwissProt sequences;

\item
swiss_features: an alignment of SwissProt sequences onto annotation objects. 

\end{itemize}
To find out how the known SwissProt annotations map on to our mRNA sequences requires a join, which can be formulated as a simple Pygr graph, consisting of a mapping of an mRNA sequence interval (node 1), onto a SwissProt sequence interval (node 2), onto a feature annotation (node 3):

\begin{verbatim}
>>> from load_alignments import * # load the alignments
>>> from pygr.graphquery import *      # import the graph query code
# draw a graph using a dict.  Note: edge 2->3 must come from swiss_features
>>> queryGraph={1:{2:None},2:{3:dict(dataGraph=swiss_features)},3:{}} 
# run the query and save the mappings
>>> l=[dict(d) for d in GraphQuery(mRNA_swiss,queryGraph)] 
>>> len(l) # how many annotations mapped onto our mRNA sequences?
4703
\end{verbatim}

We assumed that mRNA_swiss would be passed as the default dataGraph, and specified directly that edge 2->3 should be looked up in swiss_features.  We then captured all the results from the GraphQuery iterator using a Python list comprehension.  Note that since the iterator returns each result in the same container (mapping object), if we want to save all the individual results we have to copy each one to a new mapping (dict) object, as illustrated in this example.
Storing Alignments in a Relational Database


\subsubsection{Example: a MySQL Database OBSOLETE}
Here's an example of working with sequences from a relational database:

\begin{verbatim}
>>> import MySQLdb # standard module for accessing MySQL, now get a cursor...
>>> rdb=MySQLdb.connect(db='HUMAN_SPLICE_03',read_default_file=os.environ['HOME'
]+'/.my.cnf')
>>> t=SQLTable('genomic_cluster_JUN03',rdb.cursor()) #interface to a table of
 sequences
>>> from pygr.seqdb import *   # pygr module for working with sequences from databases
>>> t.objclass(DNASQLSequence) #use this class as "row objects"
>>> s2=t['Hs.1162'] # get a specific sequence object by ID
>>> str(s2[1000:1050]) # this will only get 50 nt of the genomic sequence from 
MySQL
'acctgggtgatgaaataaatttttacgccaaatcccgatgacacacaatt'
\end{verbatim}

(Note: in this example we used MySQLdb.connect()'s ability to read database 
server and user authentication information directly from the standard ~/.my.cnf file normally used by the MySQL client).

\subsubsection{More examples}
\label{more-exam}

Additional examples of how to use pygr can be found in the tests/ directory within the pygr distribution package.




\section{Module Documentation}
\label{module-doc}

The following subsections provide details about how to use specific
modules of Pygr functionality. 

\subsection{Installation}
\label{install}
Installing pygr is quite simple:
\begin{verbatim}
tar -xzvf pygr-0.3.tar.gz 
cd pygr
python setup.py install 
\end{verbatim}

Once the test framework has completed successfully, the setup script
will install pygr into python's respective site-packages directory. 

If you don't want to install pygr into your system-wide site-packages,
replace the "python setup.py install" command with
\begin{verbatim}
python setup.py build
\end{verbatim}
This will build pygr but not install it in site-packages.

Pygr contains several modules imported as follows:
\begin{verbatim}
from pygr import seqdb # IMPORT SEQUENCE DATABASE MODULE
\end{verbatim}

If you did not install pygr in your system-wide site-packages, you 
must set your PYTHONPATH to the location of your pygr build.
For example, if your top-level pygr source directory is PYGRDIR then
you'd type something like:
\begin{verbatim}
setenv PYTHONPATH PYGRDIR/build/lib.linux-i686-2.3
\end{verbatim}
where the last directory name depends on your specific architecture.


\subsection{sequence Module}
\label{sequence}

{\em Base classes for representing sequences and sequence intervals.}


\subsubsection{Overview}
Pygr provides one base class representing both sequences and sequence intervals (SeqPath),
from which all sequence classes are derived (Sequence, SQLSequence, BlastSequence etc.).
In this section we document both the features of the base class, and ways to extend or
customize it by creating your own subclasses derived from SeqPath.  The IntervalTransform
class represents a coordinate system mapping from one interval of a sequence, onto 
another interval of the same or a different sequence.

\subsubsection{SeqPath}
This class provides the basic capabilities of a sliceable sequence or sequence interval,
widely used in Pygr.  It tries to provide core operations on sequences in a highly
Pythonic way:

\begin{itemize}

\item    
{\em Python Sequence}: of course, SeqPath behaves like a Python sequence. i.e.
the length of a \class{SeqPath} \var{s} is just \code{len(s)}, 
and you iterate over the ``letters'' in it using \code{for l in s:}  
(Note, the individual letters produced by this iterator
will themselves be \class{SeqPath} objects (by default, of length 1)).  
And all the slicing
operations defined for Python Sequences also apply to 
\class{SeqPath} (see below).

\item    
{\em Slicing}: \class{SeqPath} is designed to represent a slice 
(subinterval) of a sequence.
Like the Python builtin \class{slice} class, it has \member{start}, 
\member{stop}, and \member{step} attributes that indicate 
the interval beginning, end, and ``stride''.
Moreover, it is itself sliceable in the usual pythonic way, 
i.e. \code{s[start:stop]},
where \var{start} and \var{stop} are in the local coordinate system of \var{s} 
(i.e. \code{s[0]} is the first letter of the interval represented by 
\var{s}). Note that \class{SeqPath}
follows the Python slicing coordinate conventions of positive integers as
forward coordinates (i.e. counting from the interval start) and negative integers
as reverse coordinates (i.e. counting from the interval end).

\item    
{\em String value}: to obtain the actual sequence string representation
of a \class{SeqPath}, just use the Python builtin \code{str(s)}.  
Note that in most cases
a SeqPath object does not itself store the sequence string associated with it
but obtains it from somewhere else when the user requests it.

\item
{\em comparison and containment}: \class{SeqPath}
implements the interval-ordering
and interval-containment relations using the standard Python order operators
and containment operators. i.e. s<t iff s.start<t.start, and s in t iff
t.start<=s.start and s.stop<=t.stop.

\item
{\em orientation}: SeqPath carefully represents relationships between intervals
on opposite strands of a double-stranded nucleotide sequence.  A SeqPath object
knows whether it is an interval on the forward or reverse strand.  Pygr provides
a number of operations for manipulating and comparing intervals of different
orientations.  For example, \code{-s} yields the interval of the opposite strand that
is base-paired to interval s (i.e. this is not just the reverse-complement of \var{s}
in the string 'atgc' $\rightarrow$ 'gcat' sense, but is specifically the SeqPath
object representing the coordinate
interval on the opposite strand that is base-paired to \var{s}).

\item
{\em schema}: a SeqPath object knows ``what sequence'' it is an interval of;
it is not just a (start,stop) coordinate pair, but is actually bound to a specific
parent sequence object.  Specifically, s.path is the parent sequence object of
which s is a subinterval; s.path will itself be an instance of SeqPath, and its path
attribute will simply be itself.  All SeqPath objects are descended from such ``top-level''
SeqPath objects.  Note that when you have sequence intervals from both forward
and reverse strands of a sequence, all of the forward strand intervals will share
the same path attribute (your original top-level sequence object representing
the whole sequence in forward orientation), while all the reverse strand intervals
will reference another top-level SeqPath created automatically to represent the
reverse strand.

\item
{\em graph structure}: a SeqPath object itself acts as a graph, whose nodes are
the individual letters of the sequence, and whose edges represent the link 
from each letter to the next (if any).  Thus standard graph query works on
SeqPath objects, through the usual interfaces:

\begin{verbatim}
for l in s: # GET EACH LETTER OF THE SEQUENCE
    c=str(l)

edge=s[l1][l2] # GET EDGE INFORMATION FOR l1 --> l2

for l1,l2,edge in s.edges(): # GET ALL l1 --> l2 EDGES
    do_something(l1,l2,e)

# DUMB GRAPH QUERY TO FIND 'AG' SUBSTRINGS IN SEQUENCE s
for d in GraphQuery(s,{0:{1:dict(filter=lambda fromNode,toNode:
                                 str(fromNode)=='A' and str(toNode)=='G')},
                       1:{}})
    l1,l2,edge=d[0],d[1],d[0,1]
\end{verbatim}  

For more information about edges, see the LetterEdge class.

\item
{\em Mutable Sequences}: Just as the Python builtin list class implements
``mutable sequence'' objects that can be resized, SeqPath objects can be
resized and changed, without breaking existing subinterval objects that
are ``part of'' the resized SeqPath object.  In particular, just as a list
can be resized by extending its ``stop'' coordinate to a higher value, a SeqPath can
be resized by extending its stop coordinate to a higher value.  Indeed,
you can even create a SeqPath for a particular sequence without knowing that
sequence's length (computing the length of a genome sequence might take a long
time, if all you want to do is create a sequence object to represent that
sequence).  You can do this by passing {\em None} as the stop (or start)
coordinate.  In that case, SeqPath will automatically determine its own
length at a later time iff a specific user operation makes it absolutely 
necessary to know this length.

\item
{\em intersection, union, difference}: SeqPath uses the Python *, + and - 
operators to implement interval intersection, union, and difference
operations respectively.

\end{itemize}

\begin{funcdesc}{before}{} 
  This method returns the entire sequence interval preceding this interval.
  For example, if \code{exon} is an interval of genomic sequence, then
  \code{exon.before()[-2:]} is its acceptor splice site (i.e. the 2 nt immediately
  before \code{exon}).
\end{funcdesc}

\begin{funcdesc}{after}{} 
  This method returns the entire sequence interval following this interval.
  For example, if \code{exon} is an interval of genomic sequence, then
  \code{exon.after()[:2]} is its donor splice site, (i.e. the 2 nt immediately
  after \code{exon}).
\end{funcdesc}


\subsubsection{Sequence class}

\begin{funcdesc}{sequence.Sequence}{s, id}
  The Sequence class provides a SeqPath flavor that stores a sequence string
  {\em s} and identifier {\em id} for this sequence.

\begin{verbatim}
from pygr import sequence
seq=sequence.Sequence('GPTPCDLMETQ','FOOG_HUMAN')
\end{verbatim}
\end{funcdesc}


\begin{funcdesc}{update}{s}
  You can change the actual string sequence to a new string {\em s}
  using the {\em update} method:

\begin{verbatim}
seq.update('TKRRPLEDKMNEPS')
\end{verbatim}
\end{funcdesc}

\begin{funcdesc}{seqtype}{}
  returns DNA_SEQTYPE for DNA sequences, 
  RNA_SEQTYPE for RNA, and PROTEIN_SEQTYPE for protein.
\end{funcdesc}


\begin{funcdesc}{reverse_complement}{s}
  returns the reverse complement of the sequence string s.
\end{funcdesc}


\begin{itemize}
\item
\member{id}: the \member{id} attribute stores the sequence's identifier.

\end{itemize}

\subsubsection{Coordinate System}
SeqPath follows Python slicing conventions (i.e. 0-based indexing, positive indexes
count forward from start, negative indexes count backwards from the sequence
end, and always {\em s.start<s.stop}).

Each SeqPath object has a number of attributes giving information about its
``location'':

\begin{itemize}

\item    
\member{orientation}: +1 if on the forward strand, or -1 if on the reverse strand.

\item
\member{path}: the top-level sequence object that this interval is part of, or self
if this object is its top-level (i.e. not a slice of a larger sequence).  Note that
all forward intervals share the same path attribute, but reverse strand intervals
all have a path attribute that represents the entire reverse strand.

\item
\member{pathForward}: same as {\em path}, but always the forward strand sequence.

\item
\member{start}: start coordinate of the interval.  NB: SeqPath stores coordinates
relative to the start of the {\em forward} strand.  This is necessary for allowing
resizing of the top-level SeqPath; if coordinates were relative to the end of the
sequence, they would have to be recomputed every time the length of the sequence 
changed.  The main consequence of this is that coordinates for forward intervals
are always positive, whereas coordinates for reverse intervals are always 
negative (i.e. following the Python convention
that negative coordinates count backwards
from the end, and the fact that the end of the reverse strand corresponds to 
the start of the forward strand). NB2: if the SeqPath was originally created with
{\em start=None}, requesting its start attribute will force it to compute its start
coordinate, typically requiring a computation of the sequence length.  In this
case, the start attribute will computed automatically by SeqPath.__getattr__().

\item
\member{stop}: end coordinate of the interval.  The above comments for {\em start}
apply to {\em stop}.  Note that for reverse intervals, a {\em stop} value of 0
means the end of the reverse strand (i.e. -1 is the last nucleotide of the 
reverse strand, and 0 is one beyond the last nucleotide of the reverse strand).

\item
\member{_abs_interval}: a tuple giving the ({\em start,stop}) coordinates of the 
interval on the forward strand corresponding to this interval (i.e. for a 
forward interval, itself, or for a reverse interval, the interval that base-pairs
to it).

\end{itemize}


\subsubsection{Extending and Customizing}
There are several methods and attributes you can override to extend or customize
the behavior of your own SeqPath-derived classes.  Typically you will derive
either from the Sequence class, or in some cases from the SeqPath class.

\begin{funcdesc}{strslice}{start, stop, useCache=True} 
  called to get the string
  sequence of the interval ({\em start, stop}).  You can provide your own strslice()
  method to customize how sequence is stored and accessed.  For example,
  \method{SQLSequence.strslice()} gets the sequence via a SQL query, and 
  \method{BlastSequence.strslice()} obtains it using the 
  \code{fastacmd -L start,stop} 
  UNIX shell command from the NCBI toolkit.
  The optional \var{useCache} argument controls whether your \method{strslice} method
  should attempt to get the sequence slice from its database cache (if any),
  or, if false, only directly from its back-end storage (in the usual way
  described above).
\end{funcdesc}

\begin{funcdesc}{__len__}{}
  called to compute the length of the sequence.  You can
  customize this to provide an efficient length method for your particular
  sequence storage.  e.g. \class{SQLSequence} obtains it via a SQL query; 
  \class{BlastSequence} obtains it from a precomputed length index.
  The default \method{Sequence.__len__()} method computes it from 
  \code{len(self.seq)}, assuming that the sequence can be accessed
  from the \member{seq} attribute.
\end{funcdesc}

\begin{funcdesc}{__getitem__}{slice_obj}
  if you want to monitor or intercept slicing
  requests on your sequence, you can do so by providing your own getitem method.
  See \class{seqdb.BlastSequenceCache} class for an example.
  If the sequence object has a \code{db} attribute, and that database object
  it points to has an \code{itemSliceClass} attribute, \code{SeqPath.__getitem__}
  will use that class to construct the subinterval object.  Similarly,
  if the sequence object has an \code{annot} attribute, and that annotation
  object has a \code{db} attribute, again the \code{itemSliceClass} attribute
  of that database will be used as the class to construct the subinterval
  object.  Otherwise it will
  use \code{SeqPath} itself as the class for constructing the subinterval object.

  Note: this \code{itemSliceClass} behavior applies not only to 
  sequence slices obtained via \method{__getitem__}, but also from all other
  methods that return sequence slices, such as the following list:
  \method{before}, \method{after}, \method{__mul__}, \method{__neg__}.
  \method{__add__}, \method{__iadd__}.
\end{funcdesc}

\begin{funcdesc}{__mul__}{other}
  get the sequence interval intersection of \var{self} and \var{other}.
\end{funcdesc}

\begin{funcdesc}{__neg__}{}
  get the sequence interval representing the opposite strand of \var{self} 
  i.e. the slice whose string value is the reverse complement of the string
  value of \var{self}.
\end{funcdesc}

\begin{funcdesc}{__add__}{other}
  get the sequence interval union of \var{self} and \var{other}, i.e.
  the smallest sequence interval that contains both of them.
\end{funcdesc}


\begin{funcdesc}{__getattr__}{attr}
  if you subclass a \class{SeqPath}-derived class and supply a \method{__getattr__}
  method for your subclass, it {\em must} call the parent class's 
  \method{__getattr__}.  This is essential for ``delayed evaluation'' of
  \member{start} and \member{stop} attributes, which are generated automatically
  by \class{SeqPath}'s \method{__getattr__}.  If your subclass inherits from
  more than one parent class, check whether {\em both} parents supply a 
  \method{__getattr__}, in which case your subclass must supply a
  \method{__getattr__} that explicitly calls both of them.  Failing to do so
  will lead to strange bugs.
\end{funcdesc}

\begin{itemize}
\item
\member{seq}: the \method{Sequence.strslice()} method assumes that 
the actual sequence is stored
on the \member{seq} attribute.  You could customize this behavior by 
making the \member{seq} attribute a property that is computed on the fly
by some method of your own.

\end{itemize}


\subsubsection{IntervalTransform}
This class provides a mapping transform between the coordinate
systems of a pair of intervals.

\begin{verbatim}
xform=IntervalTransform(srcPath,destPath)
d2=xform(s2) # MAPS s2 FROM srcPath coords to destPath coord system
d3=xform[s2] # CLIPS s2 TO NOT EXTEND OUTSIDE srcPath, THEN XFORMS
s3=xform.reverse(d3) # MAP BACK TO srcPath COORD SYSTEM
\end{verbatim}

\subsubsection{Seq2SeqEdge}
This class represents a segment of alignment between two sequences.
It is a temporary object created in association with a MSASlice
object (see Alignment Module below).

\begin{funcdesc}{__init__}{msaSlice, targetPath, sourcePath=None}
  Create a Seq2SeqEdge for the targetPath, on the specified alignment
  slice.  If sourcePath is None, it will be calculated automatically
  by calling the slice's methods.
\end{funcdesc}

\begin{funcdesc}{__iter__}{sourceOnly=True, **kwargs}
  iterate over source intervals within this segment of alignment.
  \var{kwargs} will be passed on to the \var{msaSlice}'s 
  \method{groupByIntervals} and \method{groupBySequences} methods.
\end{funcdesc}

\begin{funcdesc}{items}{**kwargs}
  same as \method{__iter__}, but gets tuples of (source_interval,target_interval).
\end{funcdesc}

\begin{funcdesc}{pIdentity}{mode=max,trapOverflow=True}
  Compute the percent identity between the source and target sequence
  intervals in this segment of the alignment.  \var{mode} controls
  the method used for determining the denominator based on the lengths of
  the two aligned sequence intervals.  \var{trapOverflow} controls
  whether overflow (due to multiple mappings of the query sequence to 
  {\em different} regions of the alignment) is trapped as an error.
  To turn off such error trapping, set \var{trapOverflow=False}.
\end{funcdesc}

\begin{funcdesc}{pAligned}{mode=max,trapOverflow=True}
  Compute the percent alignment between the source and target sequence
  intervals in this segment of the alignment, i.e. the fraction of
  residues that are actually aligned as opposed to gaps / insertions,
  in the two intervals.
\end{funcdesc}

\begin{funcdesc}{conservedSegment}{pIdentityMin=.9,minAlignSize=1,mode=max}
  Return the longest alignment interval (possibly including gaps) with
  a \%identity fraction higher than \var{pIdentityMin}.  If there is no
  such interval, or the longest such interval
  is shorter than \var{minAlignSize}, it returns \var{None}.  The interval
  is returned as a tuple of integers \code{(srcStart,srcEnd,destStart,destEnd)}.
\end{funcdesc}

{\em Warning}: if your query sequence has multiple mappings in the alignment
(i.e. it is aligned to two or more different regions in the alignment),
\method{pIdentity}() and \method{pAligned}() may return fractions larger
than 1.0.  This is because the query sequence may align to a given
target sequence via {\em more} than one region in the alignment.  If you
encounter this problem, you can iterate through the individual mappings
yourself (by calling the \method{iter}(), \method{items}() or
\method{edges}() iterator methods for your alignment slice object), 
and calculating the percentage identity or alignment (via your own algorithm)
individually for each specific mapping.  For more
background on this problem, see ``Multiple Mappings'', below.

Note that the presence of multiple mappings is {\em not} a Pygr bug,
but simply reflects the alignment data loaded into Pygr.  \class{Seq2SeqEdge}
should be able to avoid this problem mostly, beginning with release 0.6.
(It tries to screen out hits not consistent with the specific region-region
mapping stored with this edge).

\subsubsection{SeqFilterDict}
This dict-like class provides a simple way for masking a set of sequences
to specific intervals.  It stores a specific interval for each
sequence.  Subsequent look-up using a sequence interval as a key will
return the intersection between that interval and the stored interval
for that sequence in the dictionary.  If there is no overlap, it
raises \code{KeyError}.

\begin{verbatim}
d=SeqFilterDict(seqIntervalList)
overlap=d[ival] # RETURNS INTERSECTION OF ival AND STORED IVAL, OR KeyError
\end{verbatim}

You can pass a list of intervals to store to the class constructor (as 
shown above).  You can also add a single interval using the syntax
\code{d[saveInterval]=saveInterval}.  (This syntax reflects the actual
mapping that the dictionary will perform if later called with the
same interval).

\subsubsection{LetterEdge}
This class represents an edge from origin -> target node.

\begin{funcdesc}{__iter__}{}
  iterate over seqpos for sequences that traverse this edge.
\end{funcdesc}

\begin{funcdesc}{iteritems}{}
  generate origin, target seqpos for sequences that traverse this edge.
\end{funcdesc}

\begin{funcdesc}{__getitem__}{seq}
  return origin,target seqpos for sequence \var{seq};
  raise \code{KeyError} if not in this edge
\end{funcdesc}

\begin{itemize}
\item
\member{seqs}: returns its sequences that traverse this edge
\end{itemize}


\subsubsection{Functions}
The sequence module also provides convenience functions:

\begin{funcdesc}{guess_seqtype}{s}
  based on the letter composition of
  the string {\em s}, returns DNA_SEQTYPE for DNA sequences, 
  RNA_SEQTYPE for RNA, and PROTEIN_SEQTYPE for protein.
\end{funcdesc}

\begin{funcdesc}{absoluteSlice}{seq, start, stop}
  returns the sequence interval of top-level sequence object associated
  with \var{seq}, interpreting \var{start} and \var{stop} according to
  the Pygr convention: a pair of positive values represents an interval
  on the forward strand; a pair of negative values represents an
  interval on the reverse strand (see Coordinate System, above).
  Note: if {\em seq} is itself a subinterval, then the {\em start,stop}
  coordinates are interpreted relative to its parent sequence, i.e.
  \code{seq.pathForward[start:stop]}.
\end{funcdesc}


\begin{funcdesc}{relativeSlice}{seq, start, stop}
  returns the sequence interval of \var{seq}, interpreting
  \var{start} and \var{stop} according to
  the Pygr convention: a pair of positive values represents an interval
  on the forward strand; a pair of negative values represents an
  interval on the reverse strand (see Coordinate System, above).
  Note: if {\em seq} is itself a subinterval, then the {\em start,stop}
  coordinates are interpreted relative to {\em seq} itself, i.e.
  \code{seq[start:stop]}.
\end{funcdesc}




\subsection{Alignment Module}
\label{seqdb}

{\em Pygr interface to sequence alignment, and scalable storage for multigenome alignments}


\subsubsection{Overview}

Pygr provides a general model for interfacing with any kind of sequence alignment,
and also a uniquely scalable storage system for working with huge multiple sequence
alignments such as multigenome alignments.  Specifically, it lets you work with
an alignment both in the traditional Row-Column model (each row is a sequence, each
column is a set of individual letters from different sequences, that are aligned;
we will refer to this as the RC-MSA model), and also
as a graph structure (known as a Partial Order Alignment, which we will refer to as
the PO-MSA model).  This supports ``traditional'' alignment analysis, as well
as graph-algorithms, and even graph query of alignments.

This model has a few basic classes:
\begin{itemize}
\item    
\class{MSA}: this class represents an entire alignment.  It acts as a graph whose
nodes are sequences (or sequence intervals) that are aligned, and whose edges 
represent specific alignment relationships between specific pairs of sequences 
(or intervals).  Specifically, it acts as a dictionary whose keys are SeqPath
objects, and whose values are MSASlice objects (representing an alignment segment
associated with a specific SeqPath, see below for details).  For example, to find
out what's aligned to some sequence interval s1:
\begin{verbatim}
for s2 in msa[s1]: # GET ALL INTERVALS s2 ALIGNED TO s1 IN msa
    do_something(s1,s2)
\end{verbatim}

In addition, its {\em letters} attribute acts as a graph interface
to the Partial Order alignment (PO-MSA) representation of the alignment.  I.e.
it is a graph whose nodes each represent a set of individual letters from 
different sequences, that are aligned to each other, and whose edges connect
pairs of nodes that are ``adjacent'' to each other in at least one sequence.
Specifically, it acts as a dictionary whose keys are MSANode objects (see below),
and whose edges are LetterEdge objects (see previous section).
\begin{verbatim}
for node in msa.letters: # GET ALL ALIGNMENT ``COLUMNS'' IN msa
    for l in node: # GET ALL INDIVIDUAL SEQ LETTERS ALIGNED HERE
        say_something(node,l)
\end{verbatim}


\item    
\class{MSASlice}: this class represents a segment of alignment associated with
a specific sequence interval (s1).  It acts as dictionary whose keys are sequence
intervals s2 aligned to s1, and whose values are MSASeqEdge objects
that represent the alignment relationship between s1 $\rightarrow$ s2.  It also 
has a {\em letters} attribute, that represents the subgraph of nodes
associated specifically with s1, and the edges that interconnect them.

\begin{verbatim}
myslice=msa[s1]: # GET SLICE ALIGNED TO s1 IN msa
    for node in myslice.letters:  # GET ALL ALIGNMENT ``COLUMNS'' FOR s1
        for l1,l2,e in node.edges(): # GET INDIVIDUAL LETTERS ALIGNED TO l1 OF s1
	    whatever(l1,l2,e)
\end{verbatim}

This class also has a {\em regions} method that generates all the alignment
interval relationships in this slice according to ``grouping'' criteria such
as maximum permissible gap length, etc.  (i.e. any region of alignment containing
no gaps larger than a specified size would be returned as a single region, 
whereas any gap larger than the specified size would split it into two separate
regions).  This provides a general interface for group-by operations in alignment
query.

\item    
\class{MSASeqEdge}: this class represents a relationship between a pair of 
sequence intervals s1 and s2 (SeqPath objects).  It provides a mapping between
subintervals of s1 $\rightarrow$ s2.  I.e. it acts as a dictionary 
that accepts subintervals of s1 as keys, and maps them to aligned
subintervals of s2.  It also 
has a {\em letters} attribute, that represents the subgraph of nodes
associated specifically with them, and the edges that interconnect the nodes.

\item    
\class{MSANode}: this class represents a specific ``column'' in the alignment
that aligns a set of individual letters from different sequences.  This
corresponds to a node in the PO-MSA representation of the alignment.
It acts as a dictionary whose keys are sequence intervals (typically only
one letter long) aligned in this column, and whose values are MSASeqEdges
representing the alignment of that letter to the column (see above).

\end{itemize}

\subsubsection{NestedList Storage}
Pygr provides a highly scalable storage mechanism for working with
multi-genome alignments.  One fundamental challenge in working with
very large alignments is the {\em interval overlap query} problem: 
to obtain a portion of an alignment (defined by some interval of
interest) requires finding all interval elements in the ``alignment
database'' that overlap the query interval.  Since the intervals
can be indexed by start (or end) position, one can typically find the
first overlapping element in $O(\log N)$ time, where $N$ is the total
number of intervals in the database.  The problem is that since
standard index structures cannot index both {\em start} and {\em end}, 
to obtain {\em all} intervals that overlap the query interval, one must scan
forwards (or backwards) from that point.  Furthermore, one cannot stop
at the first non-overlapping interval; there might be an extremely long 
interval at the very beginning of the index, that extends to overlap 
the query interval.  In this case, one would have to scan the entire
database ($O(N)$ time) to guarantee that all overlapping intervals are
found.

The {\em nested list} data structure solves this problem, by moving
any interval in the database that is {\em contained} in another interval
out of the top-level interval list, into the {\em sublist} of the
parent interval.  Based on this, one can prove that one can stop
the scanning operation at the first non-overlapping interval (i.e.
the overlapping intervals in any list form a single contiguous block).
Overall, this reduces the query time to $O(\log N + n)$, where $n$ is
the number of intervals in the database that actually overlap the 
query (i.e. results to return).  Moreover, the nested list data structure
can be implemented very well both in computer memory (RAM) or as indexed
disk files.  Pygr's disk-based cnestedlist database can complete
a typical interval query of the 26GB UCSC 8 genome alignment in
about 60 microseconds, compared with 10-30 seconds per query using
MySQL.

\subsubsection{Multiple Mappings: a Warning}
Multi-genome alignments take traditional models of alignment to an
entirely different scale, and inevitably many of the assumptions of
standard row-column multiple sequence alignment are broken (e.g. 
no inversions; no cycles; etc.).  One major issue that users should be
aware of in UCSC multi-genome alignments is the possibility of 
{\em multiple mappings}, in which a given query sequence interval is
mapped to two or more different regions of the alignment (and thus potentially
to two or more different locations in a given target genome).  Currently,
UCSC multi-genome alignment are typically based on a single 
{\em reference genome}, to which all other genomes are aligned.  While
a given region of the reference genome might be guaranteed to have
a unique mapping in the UCSC multi-genome alignment, {\em other} genomes
do not appear to have any such guarantee: a region in any of those genome
can have multiple mappings.  This is problematic for several reasons:
\begin{itemize}

\item It introduces ambiguity in the alignment: you don't know which of the
multiple hits is considered to be the ``right'' alignment; the UCSC alignment
file does not tell you.

\item There is no scoring information to resolve this ambiguity.  In a way,
this situation is even worse than the common situation we previously faced
in search for alignment mappings using BLAST, because (unlike BLAST) the 
MAF alignment does not give a score that indicates which mapping is best.
(We haven't seen such scoring information; if it can be recovered for these
alignment files, we'd be love to know about that...).

\item It can cause ``buggy'' results in calculations based on the alignment.
For example, Pygr's \method{pIdentity}() and \method{pAligned}() computations
can give values larger than one when a query region has multiple hits.  This
is not, strictly speaking, a Pygr bug: the query region is mapped by the MAF
file to the same target region {\em multiple} times, resulting in multiple
overlaps.

\end{itemize}

If you encounter multiple mappings, you can always iterate over them one
by one, and perform your own computations for each one.  However, to avoid them
altogether, you can restrict your queries to the reference genome for this specific
alignment (UCSC offers different versions of each alignment set, each based on 
a different reference genome).

\subsubsection{NLMSA}
Top-level object representing an entire multiple sequence alignment, 
stored using a set of disk-based nested list interval databases.
The alignment is stored as an interval representation of a 
{\em linearized partial order} (LPO), using {\em nested list}
databases.  This has several elements:

\begin{itemize}

\item
{\em PO-MSA}: Conceptually, the alignment is represented as a partial order alignment
(PO-MSA), in which aligned sequence intervals are fused together as a single
``node'' in the alignment graph; two nodes are connected by an edge if and only
if they are adjacent in at least one of the sequences aligned to them
(i.e. if residue {\em i} of that sequence is in the first node, and 
residue {\em i+1} is in the second node, then there is a directed edge
from the first PO-MSA node to the second node).

\item
{\em LPO}: This alignment graph is {\em partially ordered}.  Let's define an
ordering relation {\em ``i<j''} to mean ``there exists a path
of directed edges from {\em i} to {\em j}''.  For two 
letters {\em i} and {\em j} in a sequence, {\em i<j XOR j<i} (i.e. all
nodes have an ordering relationship).  By contrast, if two nodes in the LPO
represent insertions in different sequences, then NOT {\em i<j} AND NOT {\em j<i}.
Thus there can be some nodes in the LPO that have no ordering relationship
with respect to each other.  It is still possible to map the PO-MSA onto 
a linear coordinate system (i.e. to ``linearize'' the partial order): as long
as the graph contains no cycles, we can map the nodes {\em i,j,k,...} of the graph
onto a linear coordinate system {\em x,y,z,...} such that for any pair of
nodes {\em i,j} mapped to coordinates {\em x<y}, we assert NOT {\em j<i}.  This is
called the {\em linearized partial order} (LPO). This maps the PO-MSA onto
a standard Row-Column MSA format, where the LPO coordinate (just an integer
sequence 0,1,2...) can be considered the index value of each alignment column.

\item
{\em nested list}: The actual alignment data are stored in the form of
({\em start,stop}) pairs representing aligned intervals.  Since this representation
uses intervals, not individual letters, it takes no more memory to store
an alignment of two 100 kb regions than it does to align two individual letters.
This is important for scalable storage (and query) of large multi-genome
alignments.  (Each alignment interval takes 24 bytes: five \class{int} for
the {\em (start,stop)} pairs and target sequence ID, plus one \class{int}
for the sublist ID).
These interval databases are stored using nested lists.  Specifically, 
the alignment is stored as 1) a mapping of each aligned sequence interval
onto an LPO coordinate interval; 2) a reverse mapping of each LPO interval onto
all the sequence intervals that are aligned there.  To find the alignment of
a sequence interval onto the other sequences in the alignment, that interval
is first mapped onto the LPO, and from there mapped back to intervals in the
other sequences.  A nested list database is stored for {\em each} of these 
mappings (i.e. for an alignment of {\em N} sequences, there will be {\em N+1}
nested list databases to store the MSA).  Furthermore, if the size of the LPO
coordinate system (i.e. number of columns in its RC-MSA format)
grows larger than the range representable by \class{int} (typically $2^{31}$  = 2 GB),
the LPO will have to be split into separate nested list databases of a size
smaller than the maximum range representable by \class{int}.  This is necessary
for handling alignments of large genomes (e.g. the human genome is approximately 3 GB).
Pygr takes care of all this for you automatically.  Note, as an entirely separate 
issue, that Pygr's cnestedlist
module uses the \class{long long} data type for file offsets and
the \function{fseeko()} POSIX interface for large file support (i.e. 64-bit
file sizes), which is supported by current versions of Linux, Mac OS X, etc;
otherwise, check if your filesystem supports this.

\end{itemize}

This functionality is encapsulated in the NLMSA class, which has a number of methods
and attributes.

Construction Methods:

\begin{funcdesc}{NLMSA}{pathstem='', mode='r', seqDict=None, mafFiles=None, maxOpenFiles=1024, maxlen=None, nPad=1000000, maxint=41666666,trypath=None,bidirectional=True,use_virtual_lpo= -1,maxLPOcoord=None}
  Constructor for the class.  \var{pathstem} specifies a path and filename prefix for
  the NLMSA files (since multiple files are used to store one NLMSA, it will automatically add a
  number of suffixes automatically to open the necessary set of files for the NLMSA).
  \var{mode} is either ``r'' to open an existing NLMSA (from the \var{pathstem} disk files); 
  ``w'' to create a new one (which will be saved to the \var{pathstem} disk files);
  or ``memory'' to create a new in-memory NLMSA (i.e. stored in your computer's RAM
  instead of using files on your hard disk).  Obviously, this limits you to 
  the amount of RAM in your computer, but will make the NLMSA much, much faster.

  \var{seqDict} specifies a dictionary which maps sequence names to actual sequence
  objects representing those sequences.  If \var{seqDict} is None, the constructor
  will look for a file \var{pathstem}\code{.seqDict} that is a \class{prefixUnionDict}
  header file for opening all the sequence database files for you automatically.
  This header file will itself specify a list of sequence database files; the
  \var{trypath} option, if provided, specifies a list of directories in which to look for these
  sequence database files.

  The \var{bidirectional} option indicates whether you wish the NLMSA to
  save each alignment relationship A:B in {\em both} possible directions
  (i.e. nlmsa[A] will yield B, and nlmsa[B] will yield A).  In general, the
  \var{bidirectional=True} mode is most appropriate for true multiple sequence
  alignments, i.e. where it is guaranteed that for a given pair of sequences A,B
  each interval of A maps to a unique interval in B, which in turn maps back
  to the same interval of A (and {\em only} that interval in A).  There are
  many possible scenarios where you might prefer \var{bidirectional=False} mode:
\begin{itemize}
\item When you WANT your alignment to have a specific directionality.  For example,
if \code{nlmsa} is a mapping of the human genome sequence onto the mouse genomic
sequence, then \code{nlmsa[s]} should only yield a result if \code{s} is a human
genome sequence interval; a mouse genome sequence interval should raise a \code{KeyError}.

\item When the input alignment data may contain multiple, inconsistent alignments of
a given pair of sequences.  For example, a BLAST all-vs-all will return TWO alignments
of A,B: one when A is blasted against the database (finding B), and another when 
B is blasted against the database (finding A).  These two alignments could be different!
In this case, a \var{bidirectional=True} alignment would return BOTH alignments
(i.e. \code{nlmsa[A]} will return TWO alignments of B, which might be identical...
or might be significantly different).  This is undesirable behavior.  Instead,
use \var{bidirectional=False} so that \code{nlmsa[A]} will simply return the 
alignments that were found when A was blasted against the database.

\item In general, using \var{bidirectional=True} can yield multiple, potentially
inconsistent results when the input data are not a true multiple-sequence alignment
(e.g. BLAST alignment data is strictly pairwise, not a true multiple-sequence alignment).
\end{itemize}

  \var{use_virtual_lpo=True} indicates a PAIRWISE sequence alignment, in which
  the stored alignment relationships each consist of a pair of sequence intervals
  that are aligned.  Note: this pairwise format can store the alignment of {\em any}
  number of sequences, but the key point is that the individual alignment relations
  are pairwise, sequence-to-sequence.  The opposite model (\var{use_virtual_lpo=False})
  indicates a true MULTIPLE sequence alignment, in which the stored alignment
  relationships each consist of an integer coordinate interval (the alignment's internal
  coordinate system, for technical reasons called the ``LPO'') and a sequence
  interval that is aligned to it.  Under normal circumstances, you will not need
  to specify a value for the \var{use_virtual_lpo} option; the NLMSA will infer
  the correct setting automatically based on the input data.  Note: the pairwise format
  (\var{use_virtual_lpo=True}) and multiple alignment format (\var{use_virtual_lpo=False})
  cannot be mixed in a single NLMSA.  It must be either one format or the other.

  \var{mafFiles} can be used to specify a list of
  filenames storing a multiple sequence alignment in the UCSC MAF format.

  \var{maxOpenFiles} limits the open file descriptors the NLMSA will use.
  {\em This option is no longer of much importance.  In versions prior to pygr 0.5,
  however, it was important because each sequence in the alignment had its
  own index file (in v.0.5 and later this problem is solved by unionization;
  for details see below)}.  Since
  each sequence has a separate nested list database file, a large multi-genome alignment
  (with each genome containing 20 different chromosomes, say) can rapidly open a large
  number of file descriptors.  Note: NLMSA only opens a given sequence's nested list database
  when one of your queries actually requires access to that sequence; it then
  keeps that file descriptor open to make subsequent queries to it fast.  If the number
  of open file descriptors would exceed \var{maxOpenFiles}, it will close other open
  database files, which may slow down query performance (due to having to open and close
  databases repeatedly to process queries).

  \var{maxlen} specifies the maximum coordinate
  value for an LPO database.  Its default value is 2GB, to prevent \class{int} overflow.
  Using a smaller value can be useful, to 1) limit the size of the LPO in memory
  during initial construction, and 2) to limit the size of LPO database files on disk
  (if for example, your file system does not support files above some maximum size).
  During initial construction of the NLMSA (from MAF files or user-specified interval
  alignments), the algorithm performs a one-pass sort of the LPO intervals.  Thus,
  this set of intervals is briefly held in RAM for this sort.  If you have insufficient
  RAM, the construction step may raise a MemoryError.  If so, you can avoid this problem
  by using a smaller \var{maxlen} value.

  The \var{maxint} option provides another way of limiting the size of LPO
  databases.  It specifies the maximum number of intervals to store per database.
  Since each interval takes 24 bytes, the default setting limits each LPO to
  a total size of 1 GB.  Note that the current NLMSA construction algorithm
  requires loading each database index into memory as one-time operation
  during construction.  If your NLMSA build fails due to running out of memory,
  simply reduce this value.

  The \var{nPad} option sets the maximum number of LPO coordinate systems
  (specifically, the offset for the start of real sequence IDs in the NLMSA
  sequence index).  You are unlikely to need to change this default value.
\end{funcdesc}


\begin{funcdesc}{__iadd__}{sequence}
  As part of constructing an alignment, adds \var{sequence} to the alignment graph,
  so that you can subsequently save specific alignments of intervals of
  \var{sequence}, using code like \code{nlmsa[s]+=s2}, where \code{s} is
  an interval of \var{sequence} and \code{s2} is some other sequence interval.
  If \var{sequence} had not been added to the alignment, this later operation
  will raise a \code{KeyError}.
\end{funcdesc}

\begin{funcdesc}{addAnnotation}{annotation}
  adds an alignment relationship to \var{annotation} from its underlying
  sequence interval.  Note: to use this, the NLMSA must have been created with the
  {\em use_virtual_lpo=True} option.
\end{funcdesc}

\begin{funcdesc}{__getitem__}{seqInterval}
  prepare to store an alignment relationship for the sequence interval \var{seqInterval},
  i.e. get a BuildMSASlice object representing \var{seqInterval}, to which you can
  then add other sequence intervals to align them.  I.e. \code{nlmsa[s1]+=s2}
  saves the alignment of intervals s1 and s2.
  You can also use a regular Python {\em slice} object using integer indices
  ie. \code{nlmsa[1:45]}, in which case, it indicates that 
  region of the LPO coordinate system.
  If the sequence containing
  interval \var{s2} is not already in the NLMSA, it will be added for you automatically
  (i.e. creating the necessary indexing, nested list database files, etc.).  In this
  case, the sequence must supply a unique string identifier, which will be used
  on subsequent attempts to open the NLMSA database, to match the individual sequence
  nested-list databases against corresponding sequence objects (using {\em seqDict},
  see above).
\end{funcdesc}


\begin{funcdesc}{build}{}
  to construct the final nested list databases,
  after all the desired alignment intervals have been saved (using the
  \method{iadd/getitem} above).  This method
  simply calls the build() method on all the constituent NLMSASequence objects
  in this alignment.  NOTE: you do not need to call \method{build}() if
  you provided a \var{mafFiles} constructor argument, since that automatically
  calls \method{build}().
\end{funcdesc}


Alignment Usage Methods:

\begin{funcdesc}{__getitem__}{s1}
  get the alignment slice for the sequence interval \var{s1},
  i.e. get an NLMSASlice object representing the set of intervals aligned to \var{s1}.
  You can also use a regular Python {\em slice} object using integer indices
  ie. \code{nlmsa[1:45]}, in which case, it gets the NLMSA slice corresponding to that 
  region of the LPO coordinate system.
\end{funcdesc}

\begin{funcdesc}{doSlice}{s1}
  If you subclass NLMSA and provide a \method{doSlice} method, the NLMSA will
  call your \method{doSlice(seq)} method to find alignment results for \code{seq},
  instead of querying its stored alignment data.  You can thus use this 
  to provide an NLMSA interface around virtually any source of alignment information
  that you have.  To see an example, see the \class{xnestedlist.NLMSAClient} class.  
\end{funcdesc}

\begin{itemize}
\item
\member{seqs}: This attribute provides a dictionary of the sequences in
the NLMSA, whose keys are top-level sequence objects, and whose values are
the associated NLMSASequence object for each sequence.  Ordinarily you will have
no need to access the NLMSASequence object directly; only do so if you know what
you're doing (details below).  This dictionary is of type NLMSASeqDict (see below).

\end{itemize}

\subsubsection{dump_textfile, textfile_to_binaries}
These two functions enable you to dump a constructed NLMSA binary database
to a platform-independent text format, and to restore an NLMSA binary database
from this text format.  This can be useful for 
\begin{itemize}
\item speeding up the process of installing an NLMSA database on multiple
machines.  Since the restore operation does not involve a build step, it 
can be substantially faster than building the NLMSA separately on each machine.

\item moving an NLMSA database from one machine to a machine with a different
binary architecture.  Since the binary database format depends on platform-specific
details (e.g. big-endian vs. little-endian integer representation), it is not
compatible between different architectures.

\item using an NLMSA database on a machine that has insufficient RAM memory
to perform the binary database build.  You can build the NLMSA binary database
on another machine with sufficient RAM, dump it to text, then restore it on
the desired machine where you wish to be able to use it.

\item using the text format to ``package'' an NLMSA database for distribution
on the Internet.  Users need only to obtain a single file and run a single command
to restore the NLMSA database.  Users only need sufficient disk space to hold
the NLMSA; they do not need large amounts of RAM (because they will not have to
perform a ``build'' step).

\end{itemize}

\begin{funcdesc}{dump_textfile}{pathstem,outfilename=None}
  Dumps a text representation of an existing NLMSA binary database.
  \var{pathstem} must be the path to the NLMSA.  For
  example if you have an NLMSA database index file \code{/loaner/hg17_NLMSA/hg17_msa.idDict}
  (and many other index files with different suffixes),
  then you would supply a \var{pathstem} value of \code{/loaner/hg17_NLMSA/hg17_msa}.

  \var{outfilename} gives the path for the output text file into which the
  NLMSA database will be dumped.  If None, it will default to \var{pathstem} with a
  \code{.txt} suffix added.
\end{funcdesc}


\begin{funcdesc}{textfile_to_binaries}{filename}
  Creates an NLMSA binary database from input text file \var{filename}.
  The NLMSA binary database will be created in the current directory,
  and will be given the same name as it originally had prior to being dumped to text.
  Since no build is required, this function does not require significant amounts
  of RAM memory.
\end{funcdesc}



\subsubsection{xnestedlist.NLMSAServer, xnestedlist.NLMSAClient}
These two classes, provided by the separate \module{xnestedlist} module,
provide an XMLRPC client-server mechanism for querying NLMSA databases
over a network.  

\class{NLMSAServer} is constructed exactly the same as a normal \class{NLMSA};
it {\em is} a normal NLMSA with just two methods added for serving XMLRPC client
requests.  See the \class{coordinator.XMLRPCServerBase} reference 
documentation below for details about starting an XMLRPC server.

\class{NLMSAClient} provides a read-only client interface for querying
data in a remote \class{NLMSAServer}.  It takes two extra arguments for
its constructor: \var{url}, the URL for the XMLRPC server; \var{name},
the name of the NLMSAServer server object in the XMLRPC server's dictionary.
For example, to use an NLMSA stored on a remote XMLRPC server,
assuming that \code{myPrefixUnion} stores a dictionary of all the
sequence databases used by that NLMSA alignment, would just be:
\begin{verbatim}
from pygr import xnestedlist
nlmsa=xnestedlist.NLMSAClient(url='http://leelab.mbi.ucla.edu:5000',
                              name='ucsc17',seqDict=myPrefixUnion)
\end{verbatim}


\subsubsection{NLMSASlice}
A temporary object created on-the-fly to represent (an interface to provide 
information about) the portion of the alignment associated with a specific
sequence interval.  This is the main class for querying information about
alignments, and provides a number of useful methods for getting 
detailed information about alignment relationships.

In addition, the NLMSASlice is the basic unit of {\em sequence caching}
control, by which you can ensure that pygr alignment analysis accesses
sequence databases in the most efficient way.  Here's how it works:
\begin{itemize}
\item When you perform an NLMSA query by creating an NLMSASlice, it assembles
a list of covering intervals for all sequences in this part of the alignment
(i.e. for each sequence, the smallest interval that contains all of its
aligned intervals in this NLMSASlice).  

\item NLMSASlice then attempts to call the \code{cacheHint} method for each
sequence database object containing the relevant sequences (if this method 
exists; if it doesn't, this step is skipped).  It passes the \code{cacheHint} method 
the covering interval information for the aligned sequence, and a reference to 
itself (the NLMSASlice object) as the {\em owner} of this cache hint.

\item If any operation subsequently attempts to access the actual sequence
for any interval that is contained within this covering interval, the sequence
database will instead load the entire covering interval, which it stores in 
its cache, associated with the specified {\em owner}.  It then returns the
appropriate subinterval of sequence requested, as usual.

\item Any subsequent requests for sequence strings that fall within this 
covering interval will simply be obtained from this cache, instead of 
retrieving the sequence from disk files.

\item This cache information is retained until the {\em owner} (in this case,
the original NLMSASlice) is deleted (by Python garbage collecting).  Thus, to
control sequence caching, all you have to do is hold on to the NLMSASlice as
long as you want to work with its associated sequence intervals.  As soon as 
you drop it, its associated cache information will also be automatically deleted,
freeing up memory.
\end{itemize}

An NLMSASlice acts like a dictionary whose keys are
sequence intervals that are aligned to this region, and whose values are
\class{Seq2SeqEdge} objects providing detailed information about the alignment of
the target interval (key) to the source interval (the sequence interval
used to create the NLMSASlice in the first place).  You can use this
dictionary interface in several ways:


\begin{funcdesc}{__iter__}{}
  iterates over all sequence intervals that have
  a 1:1 mapping (i.e. a block of alignment containing no indels) to
  all or part of the source interval.
\end{funcdesc}


\begin{funcdesc}{keys}{maxgap=0, maxinsert=0, mininsert= 0, filterSeqs=None, mergeAll=False, minAlignSize=None, pIdentityMin=None, ivalMethod=None, sourceOnly=False, indelCut=False, seqGroups=None, minAligned=1, pMinAligned=0.,seqMethod=None, **kwargs}
  Provides a more general interface than {\em iter()}, with two types of 
  group-by capabilities, ``group-by'' operations on the alignment intervals
  contained within this slice (``horizontal'' grouping), 
  and on the sets of sequences aligned
  to this slice (``vertical'' grouping).

  1. ``group-by'' operations on the alignment intervals
  contained within this slice.  It allows the user to supply
  various parameters for controlling when alignment intervals will be
  merged or split in the results that it returns.  \var{mergeAll}
  forces it to combine intervals of a given sequence irrespective
  of the size of gaps or inserts separating them.  \var{maxgap} sets the
  maximum gap size for merging two adjacent intervals.  If the target sequence
  for the two alignment intervals has a gap longer than \var{maxgap} 
  letters between the two alignment intervals, they will be returned as
  separate intervals; otherwise they will be merged as a single alignment
  region.  \var{maxinsert} sets the maximum length of insert in the target
  sequence that allows to adjacent intervals to be merged as a single alignment
  region in the results.  \var{mininsert} is specifically for handling
  alignments that may have small ``cycles'' (due to slight inconsistencies
  in the reported alignment intervals, for example, if a portion of sequence
  can align at both the end of one interval or at the beginning of another, and
  the intervals are actually added to the NLMSA that way, then the {\em start}
  of the second interval will actually be {\em before} the {\em stop} of 
  the first interval; this corresponds to a negative insert value).  A
  \var{mininsert} value of zero (the default), prevents any such interval
  pairs from being merged.  Giving a negative \var{mininsert} value will allow
  interval pairs whose insert value is greater than or equal to this value, 
  to be merged.  \var{filterSeqs}, if not None, should be a dict of sequences
  used to filter the group-by analysis; i.e. only alignment intervals 
  containing these sequences are considered in the analysis.  More
  specifically, \var{filterSeqs} can be used to mask the group-by analysis
  to a specific interval of a sequence, by having \var{filterSeqs}
  return only the intersection between the interval it is passed as a key,
  and the masking interval that it stores.  If there is no overlap, it
  must raise \code{KeyError}.  The \class{sequence.SeqFilterDict} class
  provides exactly this masking capability, i.e.
\begin{verbatim}
d=sequence.SeqFilterDict(someIntervals)
overlap=d[ival] # RETURNS INTERSECTION BETWEEN ival AND someIntervals, OR KeyError
\end{verbatim}
  \var{minAlignSize} if not None, sets a minimum size for filtering the resulting
  alignment regions.  Regions smaller than the specified size will be culled
  from the output.  
  \var{pIdentityMin} if not None, sets a minimum fractional sequence identity
  for filtering the resulting alignment regions.  Regions with lower levels
  of identity will be clipped from the output.  Specifically, within each
  region, the largest contiguous segment (possibly including indels, if
  permitted by \var{maxgap} and \var{maxinsert}) whose sequence identity is above the
  threshold will be returned (but only if it is larger than \var{minAlignSize}
  if set).  
  \var{ivalMethod},
  if not None, allows the user to provide a Python function that performs
  interval grouping.  Specifically it is called as
  \function{ivalMethod(l, ns,msaSlice=self, **kwargs)}, where \var{l} is the
  list of intervals for NLMSASequence \var{ns} within the current slice 
  \var{msaSlice}; all other args are passed as a dict in \var{kwargs}.

  2. merge groups of sequences using "vertical" group-by rules.
  \var{seqGroups}: a list of one or more lists of sequences to group.
  If None, the whole set of sequences will be treated as a single group.
  Each group will be analyzed separately, as follows:
  \var{sourceOnly}: output intervals will be reported giving only
  the corresponding interval on the source sequence; redundant
  output intervals (mapping to the same source interval) are
  culled.  Has the effect of giving a single interval traversal
  of each group.
  \var{indelCut}: for \var{sourceOnly} mode, do not merge separate 
  intervals that the groupByIntervals analysis separated due to an indel).
  \var{minAligned}: the minimum number of sequences that must be aligned to
  the source sequence for masking the output.  Regions below
  this threshold are masked out; no intervals will be reported
  in these regions.
  \var{pMinAligned}: the minimum fraction of sequences (out of the
  total in the group) that must be aligned to the source
  sequence for masking the output.
  \var{seqMethod}: you may supply your own function for grouping.
  Called as \function{seqMethod(bounds,seqs,**kwargs)}, where
  \var{bounds} is a sorted list of
  \var{(ipos,isStart,i,ns,isIndel,(start,end,targetStart,targetEnd))}
  and \var{seqs} is a list of sequences in the group.
  Must return a list of \var{(sourceIval,targetIval)}.  See the docs.

\end{funcdesc}


\begin{funcdesc}{iteritems}{**kwargs}
  same keys as {\em iter}, but for each provides the source interval
  to target interval mapping (\class{Seq2SeqEdge}).
  Uses same group-by arguments as \method{keys()}.
\end{funcdesc}


\begin{funcdesc}{edges}{**kwargs}
  same interval mappings as {\em iteritems}, but for
  each provides a tuple of three objects:
  the source interval, the corresponding target interval,
  and the \class{Seq2SeqEdge} providing detailed
  information about the alignment between the source and target intervals
  (such as percent identity, etc.).
  Uses same group-by arguments as \method{keys()}.
\end{funcdesc}


\begin{funcdesc}{__getitem__}{s1}
  treats \var{s1} as a key (target sequence
  interval), and returns an \class{Seq2SeqEdge} object providing detailed
  information about the alignment between this target interval
  and the source interval.
\end{funcdesc}


\begin{funcdesc}{__len__}{}
  returns the number of distinct sequences that
  are aligned to the source interval.  {\em Note}: this is NOT necessarily 
  equal to the number of items that will be returned by the above iterators,
  since a single target sequence might have multiple 1:1 intervals of
  alignment to the source interval, due to indels.
\end{funcdesc}



In addition to these standard dictionary methods, NLMSASlice provides
several additional methods and attributes:


\begin{itemize}
\item
\member{letters}: this attribute provides an interface to 
the individual alignment columns (NLMSANode objects) containing the
source interval, in order from \var{start} to \var{stop}.  This provides
an easy way to obtain detailed information about the letter-to-letter
alignment of different sequences within this region of the alignment.
For details on the kinds of information you can obtain for each
alignment column, see NLMSANode, below.

It also provides a graph interface to subset of the partial order alignment 
graph corresponding to this slice.  For details, see NLMSASliceLetters, below.
\end{itemize}

\begin{funcdesc}{split}{**kwargs}
  this method provides a way to perform group-by operations on the slice;
  the output of split() is one or more NLMSASlice objects; if the
  group-by analysis results in no splitting of the current slice, then
  it is returned unchanged (i.e. the method just returns {\em self}).
  Uses same group-by arguments as \method{keys()}.
  For further details on group-by operations, see \method{keys()} above.
\end{funcdesc}

\begin{funcdesc}{regions}{**kwags}
  performs the same group-by analysis as {\em split()}, but replaces
  the source interval by the corresponding interval in the LPO.  The main
  practical consequence of this is that target sequence {\em inserts}
  are included in the resulting slice (because they are present in the LPO
  interval corresponding to the original source interval), whereas they
  were NOT included in the original slice (because they are not aligned
  to the source interval).  The main place where this matters is in graph
  traversal of the slice's {\em letters} attribute: whereas the nodes
  and edges corresponding to these inserts are not considered to be part
  of the {\em letters} graph for the original slice, they {\em are} part of the
  LPO slice.  Also, the ``source interval'' in any subsequent operations
  with the LPO slice will be LPO coordinates instead of subintervals of the
  original source sequence interval.
  Uses same group-by arguments as \method{keys()}.
\end{funcdesc}

\begin{funcdesc}{groupByIntervals}{maxgap=0, maxinsert=0, mininsert= 0, filterSeqs=None, mergeAll=True, ivalMethod=None, **kwargs}
  This method performs the interval grouping analysis for all the iterators
  described above.  Users will not need to call it directly.  Its arguments
  are described above (see \method{keys()}).  It returns a dictionary
  whose keys are sequences aligned to this slice, and whose values are
  the list of intervals produced by the group-by analysis for the corresponding
  sequence.  The values are tuples of the form
  \var{(source_start, source_stop, target_start, target_stop)}, showing the
  mapping of a source sequence interval onto a target sequence interval.
  This dictionary is the primary input to the \method{groupBySequences()}
  method below.
\end{funcdesc}

\begin{funcdesc}{filterIvalConservation}{seqIntervals,pIdentityMin=None,filterFun=None,**kwargs}
  This method is used by \method{groupByIntervals}() to filter the results
  using the specified \var{filterFun} filter function, which should either
  return \var{None} if the specified alignment region does not pass the filter,
  or return the filtered interval.  For an example
  filter function, see \method{conservationFilter}, which is used by default 
  in \method{filterIvalConservation}.  \var{seqIntervals} must be passed in
  the same format as expected by \method{groupBySequences}; it is modified in
  place by \method{filterIvalConservation}, which always returns \var{None}.
\end{funcdesc}

\begin{funcdesc}{conservationFilter}{seq,m,pIdentityMin=None,minAlignSize=None,maxAlignSize=None,**kwargs}
  Tests an alignment mapping \var{m} for the specified size and sequence 
  identity criteria.  Returns the (possibly clipped) interval \var{m} if
  the criteria are met, and \var{None} if the criteria are not met.  \var{m}
  is expected to be a tuple of integers \code{(srcStart,srcEnd,destStart,destEnd)}.
  \var{seq} must be the destination sequence object (sliceable by the destination
  interval coordinates).  The conservation criteria and clipping are performed
  using \method{Seq2SeqEdge.conservedSegment}().
\end{funcdesc}

\begin{funcdesc}{groupBySequences}{seqIntervals, sourceOnly=False, indelCut=False, seqGroups=None, minAligned=1, pMinAligned=0., seqMethod=None, **kwargs}
  This method performs the sequence grouping analysis for all the iterators
  described above.  \var{seqIntervals} must be a dictionary of sequences
  and their associated list of intervals (produced by \method{groupByIntervals()}
  above).  It returns a list of output sequence intevals, which is either
  a list of source sequence intervals (\var{sourceOnly} mode), or a list
  of tuples of the form \var{(source_interval, target_interval)}.
\end{funcdesc}


\begin{funcdesc}{matchIntervals}{seq=None}
  this method returns the set of
  1:1 match intervals for the target sequence {\em seq} (or all
  aligned sequences, if {\em seq} is None), as a dictionary
  whose keys are target sequence intervals, and whose values are
  the corresponding source sequence intervals to which they are
  aligned.
\end{funcdesc}

\begin{funcdesc}{findSeqEnds}{seq}
  returns the largest possible interval of
  {\em seq} that is aligned to this slice, i.e. it merges all 
  alignment intervals in this slice containing {\em seq}, and
  returns the merged sequence interval based on the minimum {\em start}
  value and maximum {\em stop} value found.
\end{funcdesc}

\subsubsection{NLMSASliceLetters}
represents the {\em letters} graph of a specific NLMSASlice.  It is
a graph whose nodes are the NLMSANode objects in this slice, and whose
edges are sequence.LetterEdge objects. {\em Note}: currently the edge objects
are just returned as None -- please implement!

This graph has the following methods:

\begin{funcdesc}{__iter__}{}
  generates all the nodes in the slice, in order from left to right.
\end{funcdesc}

\begin{funcdesc}{items}{}
  also \method{iteritems()}. Generate the same set of nodes as above,
  as keys, but for each also returns a value representing its outgoing
  directed edges (see getitem, below).
\end{funcdesc}

\begin{funcdesc}{__getitem__}{node}
  gets a dictionary indicating all the outgoing
  directed edges from {\em node} to subsequence nodes, whose keys are
  the target nodes, and whose edges are the 
  \class{sequence.LetterEdge} objects representing each edge.
\end{funcdesc}

\subsubsection{NLMSANode}
A temporary object (created on-the-fly) 
representing a single letter ``column'' in the alignment.  It acts like
a container of the sequence letters aligned to the source sequence in
this column.  It has the following methods:

\begin{funcdesc}{__iter__}{}
  generates all the individual sequence letters 
  (as SeqPath intervals, presumably of length 1) that are aligned to 
  the source sequence, in this column of the alignment.
\end{funcdesc}

    
\begin{funcdesc}{edges}{}
  generates the same list of of target sequence letters as
  the iterator, but as a tuple of (target letter, source letter, edge).
  Currently, edge is just None.
\end{funcdesc}

\begin{funcdesc}{__len__}{}
  returns the number of distinct sequences aligned to
  the source interval, in this column.
\end{funcdesc}

Other, internal methods that regular users are unlikely to need:

\begin{funcdesc}{getSeqPos}{seq}
  returns the sequence interval of \var{seq}
  that is aligned to this column, or raises \code{KeyError} if it is not
  aligned here.
\end{funcdesc}


\begin{funcdesc}{getEdgeSeqs}{node2}
  returns a dictionary of sequences
  that traverse the edge directly from this node to {\em node2},
  i.e. if letter {\em i} of seq is aligned to this node, then
  letter letter {\em i+1} is aligned to {\em node2}.  The
  dictionary's keys are top-level sequence objects, and its
  value for each is the letter position index {\em i} as defined above.
\end{funcdesc}

\begin{funcdesc}{nodeEdges}{}
  returns a dictionary of the outgoing edges
  from this node, whose keys are target nodes, and whose values
  are the corresponding edge objects (of type sequence.LetterEdge).
\end{funcdesc}


\subsubsection{NLMSASequence}
You are unlikely to need to manipulate NLMSASequence objects directly;
they perform the back-end work for accessing the nested list disk storage
of the alignment of the associated sequence.

However, one thing you should know is that for a sequence to be stored
in a NLMSA, it needs to have a unique string identifier.
NLMSASequence obtains a string identifier for the sequence in one of the following
ways (in decreasing order of precedence): 1) the sequence ``object'' can itself just
be a Python string, in which case that string is used as the identifier. 2) otherwise,
the object should be a SeqPath instance.  If it has a {\em name} attribute, that will
be used as the identifier. 3) Otherwise, if it has a {\em id} attribute (which is present
by default on sequence.Sequence objects), that will be used.




\subsection{Seqdb Module}
\label{seqdb}

{\em Pygr interface to sequence databases stored in FASTA, BLAST or relational databases.}


\subsubsection{Overview}

The seqdb module provides a simple, consistent interface to sequence databases from a variety of different storage sources such as FASTA, BLAST and relational databases.  Sequence databases are modeled (like other Pygr container classes) as dictionaries, whose keys are sequence IDs and whose values are sequence objects.  Pygr sequence objects use the Python sequence protocol in all the ways you'd expect: a subinterval of a sequence object is just a Python slice (s[0:10]), which just returns a sequence object representing that interval; the reverse complement is just -s; the length of a sequence is just len(s); to obtain the actual string sequence of a sequence object is just str(s).  Pygr sequence objects work intelligently with different types of back-end storage (e.g. relational databases or BLAST databases) to efficiently access just the parts of sequence that are requested, only when an actual sequence string is needed.

\subsubsection{External Requirements}
This module makes use of several external programs:
\begin{itemize}
\item
{\em NCBI toolkit}: The BLAST database functionality in this module 
requires that the NCBI toolkit
be installed on your system.  Specifically, some functions will call the command line
programs \code{formatdb}, \code{fastacmd}, \code{blastall}, and \code{megablast}.

\item
{\em RepeatMasker}: the \method{BlastDB.megablast()} method calls the command line
program \code{RepeatMasker} to mask out repetitive sequences from seeding alignments,
but to allow extension of alignments into masked regions.

\item
{\em Python DB-API 2.0}: the \class{SQLTable} class, and dependent classes such as 
\class{SQLSequence} and \class{StoredPathMapping}, conform to the Python DB-API 2.0.
Typically you must supply a DB-API 2.0-compliant database cursor to the 
\class{SQLTable} constructor.  To do so, you must have some DB-API 2.0-compliant
module (such as \module{MySQLdb}) installed for connecting to a database server.
\end{itemize} 

If you are lacking one or more of these requirements, you can still install Pygr
and use all Pygr functionality that does not depend on the missing requirements.
If you try to use a function for which a requirement is missing, Pygr will raise
an appropriate exception (e.g. unable to run \code{blastall}).

\subsubsection{BlastDB}
Interface to an existing BLAST database or FASTA sequence file; in the latter case, it will automatically construct BLAST database files for you using the NCBI tool formatdb. Here's a simple example of opening a BLAST database and searching it for matches to a specific piece of sequence:

\begin{verbatim}
from pygr.seqdb import *
db=BlastDB('sp') # OPEN SWISSPROT BLAST DB
s=NamedSequence(str(db['CYGB_HUMAN'][40:-40]),'boo')
m=db.blast(s) # DO BLAST SEARCH
myg=db['MYG_CHICK']
for i in m[s][myg]:
    print repr(i.srcPath),repr(i.destPath),i.blast_score,i.percent_id
\end{verbatim}

Let's go through this example line by line:

\begin{itemize}

\item    
construction of a BlastDB object: This looks for either a FASTA file with the path 'sp' or BLAST database formatted files based on this path (e.g. 'sp.psd' for protein sequences, or 'sp.nsd' for nucleotide sequences).

\item
db['CYGB_HUMAN'] obtains a sequence object representing the SwissProt sequence whose ID is CYGB_HUMAN.  The slice operation [40:-40] behaves just like normal Python slicing: it obtains a sequence object representing the subinterval omitting the first 40 letters and last 40 letters of the sequence.  The str() operation obtains the actual string representation of this subinterval.

\item
NamedSequence(letter_string, name) creates a new sequence object whose sequence is letter_string, and whose ID is name.

\item
Running the db.blast(s) method searches the BLAST database for homologies to s, using NCBI BLAST.  It chooses reasonable parameters based upon the sequence types of the database and supplied query.  However, you can specify extra parameter options if you wish.  It returns a Pygr sequence mapping (multiple alignment) that represents a standard Pygr graph of alignment relationships between s and the homologies that were found.

\item
The expression m[s][myg] obtains the "edge information" for the graph relationship between the two sequence nodes s and myg.  (if there was no edge in the m graph representing a relationship between these two sequences, this would produce a 
\code{KeyError}).  This edge information consists of a set of interval alignment relationships (described in detail below), which are printed out in this example.

\end{itemize}

Options for constructing a BlastDB:

\begin{funcdesc}{BlastDB}{filepath=None,skipSeqLenDict=False,ifile=None,idFilter=None,
                 blastReady=False}
  Open a sequence file as a ``database'' object, giving the user access to its sequences,
  easy searching via \code{blast} or \code{megablast}, etc.
  \var{skipSeqLenDict} prevents construction of a sequence length index file 
  (which will be named ``{\em filepath}.\code{seqlen}'') and a fast
  sequence access file (which will be named ``{\em filepath}.\code{pureseq}'').
  Setting this option to \code{True} can be useful if you either wish to
  speed up initial opening of the BlastDB (note: construction of these indexes is
  only a one-time event) or avoid the extra disk space required by these indexes.
  {\em Explanation}: To facilitate the rapid creation of sequence objects (which requires the length of the sequence), it creates a sequence length index (as a Python shelve).  This enables it to avoid actually loading the sequence string into memory each time a sequence object is created; instead it just looks up the sequence length.  While this speeds up access to genomic sequence databases (where each sequence tends to be extremely long), this initial step may be slow for databases of short sequences.  Setting \var{skipSeqLenDict} option to \code{True}, will prevent construction of this sequence length index.
  \var{ifile} lets you open the database directly from a file object rather
  than a filename.  If you have a file object, you can pass it directly to BlastDB instead of a filepath.  NB: the BlastDB() constructor will close ifile when it is done reading from the file object.
  \var{idFilter} allows you to provide a function for re-mapping the FASTA sequence
  identifiers read from the sequence file.  This can be useful in the case of
  NCBI FASTA files, since NCBI often treats the sequence ID as a ``blob'' into
  which any number of database fields can be stuffed, rather than a true ID.
  \var{blastReady} option specifies whether BLAST index files should be automatically
  constructed (using \code{formatdb}).  Note, if you attempt to run the \code{blast}()
  method, it will automatically create the index files for you if they are missing.
\end{funcdesc}

Useful methods:

\begin{funcdesc}{iter}{}
  iterate over all IDs in the BLAST database.
\end{funcdesc}

\begin{funcdesc}{len}{}
  returns number of sequences in the BLAST database.
\end{funcdesc}

\begin{funcdesc}{blast}{seq,al=None,blastpath='blastall',blastprog=None,expmax=0.001,maxseq=None}
  run a BLAST search on sequence object seq.  Maxseq will limit the number of returned hits to the best maxseq hits. 
\end{funcdesc}

\begin{funcdesc}{megablast}{seq,al=None,blastpath='megablast',expmax=1e-20,maxseq=None,minIdentity
=None,maskOpts='-U T -F m',rmOpts=''}
  first performs repeat masking on the sequence by converting repeats to lowercase,
  then runs megablast with command line options to prevent seeding new alignments
  within repeats, but allowing extension 
  of alignments into repeats.  minIdentity should be a number (maximum value, 100)
  indicating the minimum percent identity for hits to be returned.
\end{funcdesc}

\begin{funcdesc}{__invert__}{}
  The invert operator (\textasciitilde, the ``tilde'' character) 
  enables reverse-mapping of sequence objects to their string ID.
\begin{verbatim}
id=(~db)[seq] # GET IDENTIFIER FOR THIS SEQUENCE FROM ITS DATABASE
\end{verbatim}
\end{funcdesc}


Useful attributes:
\begin{itemize}

\item
\member{itemClass}: the object class to use for instantiating new sequence objects from this BLAST database.  You can set this to create customized sequence behaviors.  
This is used by \class{pygr.Data} to propagate correct attribute schemas to
items / slices from database containers managed by it.

\item
\member{itemSliceClass}: the object class to use for instantiating new sequence slice objects (i.e. subintervals of sequences from this BLAST database).  You can set this to create customized sequence behaviors.
This is used by \class{pygr.Data} to propagate correct attribute schemas to
items / slices from database containers managed by it.

\end{itemize}

\subsubsection{BlastDBXMLRPC}
A subclass of \class{BlastDB} that adds a couple methods needed to serve
the data to clients connecting over XMLRPC.  For example, to make an XMLRPC
server for a blast database, accessible on port 5020:
\begin{verbatim}
import coordinator
server=coordinator.XMLRPCServerBase(name,port=5020)
db=BlastDBXMLRPC('sp') # OPEN BlastDB AS USUAL, BUT WITH SUBCLASS
server['sp']=db # ADD OUR DATABASE TO THE XMLRPC SERVER
server.serve_forever() # START SERVING XMLRPC REQUESTS, UNTIL KILLED.
\end{verbatim}

\subsubsection{XMLRPCSequenceDB}
Class for a client interface that accesses a Blast database over
XMLRPC (from the the \class{BlastDBXMLRPC} acting as the server).
\begin{funcdesc}{__init__}{url,name}
  \var{url} must be the URL (including port number) for accessing the 
  XMLRPC server; \var{name} must be the key of the BlastDBXMLRPC object
  in that server's dictionary (in the example above, it would be 'sp').
  Thus to access the server above (assuming it is running on leelab port 5020):
\begin{verbatim}
db=XMLRPCSequenceDB('http://leelab:5020','sp')
hbb=db['HBB_HUMAN'] # GET A SEQUENCE OBJECT FROM THE DATABASE...
\end{verbatim}
\end{funcdesc}
Currently, this class provides sequence access.  You can work with sequences
exactly as you would with a \class{BlastDB}, but cannot perform actual BLAST searches
(i.e. the \method{blast} and \method{megablast} methods don't work over XMLRPC).

\subsubsection{FileDBSequence}
The default class for sequence objects returned from BlastDB.  It provides efficient,
fast access to sequence slices (subsequences).  When the BlastDB is initially opened,
Pygr constructs a length and offset index that enables FileDBSequence to \code{seek()}
to the correct location for any substring of the sequence.  New in Pygr 0.4.

\subsubsection{BlastSequence}

This was previously the default class for sequence objects returned from BlastDB,
but has been deprecated because we found that NCBI \code{fastacmd} was much too slow
and consumed enormous amounts of memory.  \class{BlastSequence} relies on
\code{fastacmd} for ``fast'' access to individual sequence slices.  The advantage is
that it only requires BLAST database files (produced by Pygr using \code{formatdb}),
whereas the new \class{FileDBSequence} requires a specially indexed sequence file
(constructed by default by BlastDB), which may be a disadvantage if you are low
on disk space.

\class{BlastSequence} has several optimizations for working with BLAST databases:

\begin{itemize}
\item
it uses the NCBI tool fastacmd to retrieve sequence efficiently from a BLAST database, when your program requests an actual string of sequence text.  Moreover, for subintervals (slices) of the sequence, it uses fastacmd's -L option to request just the desired subinterval of the sequence, rather than the whole sequence.  This makes it efficient for requesting specific intervals of large genomic contigs.  Basically, just use Python slicing and str() methods on sequence objects, and subsequences will be obtained in an efficient manner.

\item 
the len() method is implemented using the seqLenDict, a precalculated index of the sequence lengths.  So again no sequence has to be read by Python.

\end{itemize}

\subsubsection{AnnotationDB}
This class provides a general interface for sequence annotation databases.
This interface follows several principles:
\begin{itemize}
\item An {\em annotation object} acts both like a sequence interval
(representing the region of sequence that it annotates), and as an object
with annotation attributes that provide further information or relations
for that annotation.  For example, an object \code{e} representing an exon annotation
would act both like the genomic sequence interval that constitutes that exon
(i.e. you can use any of the usual \class{SeqPath} methods to get information
about that sequence interval), but also might have attributes that link it
to its {\em splice graph}.  E.g.  \code{str(e)} would return the actual
nucleotide sequence of the exon; \code{for e2,splice in e.next.items()} would iterate
through the list of exons it is connected to by a forward splice, etc.

\item An annotation object always has an \member{annot} attribute that
points to the complete annotation.  Since an annotation object is a sequence interval,
it can itself be sliced (e.g. \code{e[:10]} gets the slice representing
the first ten bases of the exon); such annotation slices also have
an \member{annot} attribute, which will simply point to the annotation
object representing the complete exon (i.e. just \code{e} itself).
To check whether an annotation \code{e} is the complete annotation
(as opposed to just a slice of part of the annotated region), use the
logical test \code{e.annot is e}.

\item The mapping of an annotation object to the sequence region it
represents is trivial, i.e. the object itself behaves as that sequence interval.
The reverse mapping (for any region of sequence, find the annotation(s) 
that map to that region) is best performed by creating an NLMSA alignment
object and saving the mapping as follows:
\begin{verbatim}
nlmsa=cnestedlist.NLMSA('myAnnotDB','w', # STORE SEQ->ANNOT MAPPING AS AN ALIGNMENT
                        use_virtual_lpo=True,bidirectional=False)
for a in annoDB.itervalues(): # GET EACH ANNOTATION OBJ IN DATABASE
  nlmsa.addAnnotation(a) # SAVE ALIGNMENT OF ITS SEQ INTERVAL TO THIS ANNOTATION
nlmsa.build() # CREATE FINAL INDEXES FOR THE ALIGNMENT DATABASE
\end{verbatim}
Later you can get the list of annotations in some sequence interval \code{s}
as easily as 
\begin{verbatim}
for a in nlmsa[s]: # FIND ANNOTATIONS THAT MAP TO s
  # DO SOMETHING...
\end{verbatim}

\end{itemize}
\begin{funcdesc}{__init__}{sliceDB,seqDB,itemClass=AnnotationSeq,itemSliceClass=AnnotationSlice,itemAttrDict=None,sliceAttrDict={}}
  Constructs an annotation database using several arguments:

  \var{sliceDB}, a database that takes an annotation ID as a key, and returns
  a slice information object with attributes that give the sequence ID and start/stop
  coordinates of the sequence interval representing the annotation.

  \var{seqDB}, a sequence database that takes a sequence ID as a key, and
  returns a sequence object.

  \var{itemClass}: the class to use for constructing an annotation object 
  to be returned from the AnnotationDB.__getitem__.  You can extend the
  behavior of annotation objects by subclassing \class{AnnotationSeq}.
  If the AnnotationDB participates in important schema relations,
  pygr.Data may add properties to the \var{itemClass} that implement
  its schema relations to other database containers.  (See the reference
  docs on \module{pygr.Data} below for details).

  \var{itemSliceClass}: the class to use for slices of annotation
  objects returned from the AnnotationDB.__getitem__.  You can extend the
  behavior of annotation objects by subclassing \class{AnnotationSlice}.
  If the AnnotationDB participates in important schema relations,
  pygr.Data may add properties to the \var{itemSliceClass} that implement
  its schema relations to other database containers.  (See the reference
  docs on \module{pygr.Data} below for details).

  \var{sliceAttrDict}, a dictionary providing the attribute names for the
  sequence ID, start and stop coordinates in each object returned from \var{sliceDB}.
  For example,
\begin{verbatim}
sliceAttrDict=dict(id='chromosome',start='gen_start',stop='gen_stop')
\end{verbatim}
  would make it use \code{s.chromosome,s.gen_start,s.gen_stop} as the ID and interval
  coordinates for each slice information object \code{s}.  Note: the start,stop
  coordinates should follow the \class{SeqPath} sign convention, i.e. positive
  coordinates mean an interval on the positive strand, and negative coordinates
  mean an interval on the negative strand (i.e. the reverse complement of
  the positive strand.  See the reference documentation on \class{SeqPath} above
  for details).
\end{funcdesc}

\begin{funcdesc}{__getitem__}{id}
  Get the annotation object with primary key \var{id}.  This annotation object
  is both a sequence interval (representing the region of sequence that it
  annotates, e.g. for an exon, the region of genomic sequence that constitutes
  that exon), and also an annotation (i.e. it may have additional attributes
  from the slice information object, that give useful information about this
  annotation).
\end{funcdesc}

\subsubsection{AnnotationSeq}
\begin{funcdesc}{originalIval}{}
  Returns a sequence interval object representing this annotation region.
  The only difference (versus the annotation object itself) is that the
  sequence interval object will not have an \member{annot} attribute or
  any other attributes that would mark it as an annotation.  This is mainly
  important for NLMSA alignments, which treat annotation objects differently
  (as an annotation) from sequence objects.
\end{funcdesc}



\subsubsection{VirtualSeq}
This class provides an empty sequence object that
acts purely as a reference system.
Automatically elongates if slice extends beyond current stop.
This class avoids setting the {\em stop} attribute, taking advantage
of SeqPath's mechanism for allowing a sequence to grow in length.
\begin{verbatim}
s=VirtualSeq('FOOG_HUMAN')
len(s) # ONLY 1 LETTER LONG BY DEFAULT
s1=s[100:215] # GET A SLICE OF THIS SEQUENCE
len(s) # NOW IT'S 215
\end{verbatim}

The associated VirtualSeqDB class provides a ``sequence database''
that returns a VirtualSeq object for every identifier requested of
it.  It acts like a Python dictionary:
\begin{verbatim}
db=VirtualSeqDB()
s=db['FOOG_HUMAN'] # ASK FOR A SEQUENCE BY ITS IDENTIFIER
s1=s[100:215] # GET A SLICE OF THIS SEQUENCE
\end{verbatim}
For a given identifier it always returns the same VirtualSeq
object (i.e. the object returned from the first request for that identifier).
In other words, if the identifier was previously requested,
it returns the VirtualSeq for that identifier; if not, it 
creates a new one.
This can be convenient when performing operations that just
need a coordinate reference system, not actual sequence.


\subsubsection{PrefixUnionDict}
This class acts as a wrapper for a set of dictionaries, each
of which is assigned a specific string ``prefix''.  It provides
a dictionary interface that accepts string keys of the form
``prefix.suffix'', and returns d['suffix'] where {\em d} is
the dictionary associated with the corresponding prefix.  This
is useful for providing a unified ``database interface'' to a
set of multiple databases.
\begin{verbatim}
hg17=BlastDB('/usr/tmp/ucsc_msa/hg17')
mm5=BlastDB('/usr/tmp/ucsc_msa/mm5')
... # LOAD A BUNCH OF OTHER GENOMES TOO...
genomes={'hg17':hg17,'mm5':mm5, 'rn3':rn3, 'canFam1':cf1, 'danRer1':dr1,
'fr1':fr1, 'galGal2':gg2, 'panTro1':pt1} # PREFIX DICTIONARY FOR THE UNION 
					 # OF ALL OUR GENOMES
genomeUnion=PrefixUnionDict(genomes)
ptChr7=genomeUnion['panTro1.chr7'] # GET CHIMP CHROMOSOME 7

if 'panTro1.chr5' in genomeUnion: # CHECK IF THIS ID IN OUR UNION
    pass # DO SOMETHING...

s= -(ptChr7[1000:2000]) # GET A BIT OF THIS SEQUENCE
if s in genomeUnion: # THIS IS HOW TO CHECK IF s DERIVED FROM OUR UNION
    pass # DO SOMETHING... 
\end{verbatim}

It provides a \method{__contains__} method that tests whether
a given sequence object is derived from the \class{PrefixUnionDict}
(see example above).  Here are some additional methods:

\begin{funcdesc}{__init__}{prefixDict=None,separator='.',filename=None,dbClass=BlastDB}
  You can create a \class{PrefixUnionDict} either using
  a \var{prefixDict} (whose keys are string prefixes, and whose 
  values are sequence databases), or using a previously created
  header file \var{filename}.  
  Using the header file, the constructor will
  automatically open all the sequence databases for you.
  When opening from a header file, you can also specify a
  \var{dbClass} to be used for opening individual sequence databases
  listed in the header file; the default is \class{BlastDB}.
  The database class constructor must take a single argument,
  which is the filepath for opening the database.  The 
  \var{separator} character is used to form ``prefix.suffix''
  identifiers.
\end{funcdesc}

\begin{funcdesc}{__invert__}{}
  The invert operator (\textasciitilde, the ``tilde'' character) 
  enables reverse-mapping of sequence objects to their string ID.
  This is the recommended way to get the ``fully qualified sequence ID'', i.e. with
  the appropriate prefix prepended. 
\begin{verbatim}
id=(~db)[seq] # GET PROPERLY PREFIXED-IDENTIFIER FOR THIS SEQUENCE
\end{verbatim}
  For a given sequence object \var{seq} derived from the union
  (or a slice of a sequence from the union), return a string identifier
  in the form of ``foo.bar''.  
\end{funcdesc}

\begin{funcdesc}{getName}{path}
  This method is deprecated; instead use the \method{__invert__} operator
  above.
\end{funcdesc}

\begin{funcdesc}{writeHeaderFile}{filename}
  Save a header file for this union, to reopen later.
  It saves the separator character, and a list of prefixes
  and filepaths to the various sequence databases (which
  must have a \member{filepath} attribute).  This header
  file can be used for later reopening the prefix-union
  in a single step.
\end{funcdesc}

\begin{funcdesc}{newMemberDict}{}
  Returns a new member dictionary for testing membership in
  the distinct prefix groups.  See \class{PrefixUnionMemberDict}.
\end{funcdesc}

\begin{funcdesc}{cacheHint}{owner,ivalDict}
  Communicates a set of caching hints to the appropriate member
  databases.  \var{ivalDict} must be a dictionary whose keys are
  sequence ID strings, and whose values are each a (start,stop) tuple
  for the associated covering interval coordinates to
  cache for each sequence.  \var{owner} should be a python object
  whose existence controls the lifetime of these cache hints.
  When \var{owner} is garbage-collected by Python (due to its 
  reference count going to zero), the member databases will clear
  these cache hints from their cache storage.

  On \class{PrefixUnionDict}, this method simply passes along
  the cache hints to the appropriate member databases by calling
  their \method{cacheHint} method, without itself doing anything
  to cache the information.
\end{funcdesc}



\subsubsection{PrefixUnionMemberDict}
Implements membership testing on distinct prefix groups.  Specifically,
you can bind a given prefix to a value
\begin{verbatim}
d['prefix1']=value
\end{verbatim}
then test whether a given object \var{k} is a member of any of the
prefix groups in the dictionary:
\begin{verbatim}
v=d[k] # raises KeyError if k not a member of 'prefix1' or other prefix group in d
\end{verbatim}

\begin{funcdesc}{__init__}{puDict,default=None,attrMethod=lambda x:x.pathForward.db}
  \var{puDict} must be a \class{PrefixUnionDict}, whose prefix groups constitute the
  allowed possible key groups for this membership dictionary.  \var{default}
  provides a default value to apply to any key whose prefix has not been explicitly
  given a value in this dictionary.  If no \var{default} is set, this dictionary
  will raise a \code{KeyError} for any key whose prefix has not been 
  explicitly given a value in this dictionary.
  \var{attrMethod} specifies a method for obtaining
  the actual prefix container object from a given member key object.  The default
  \var{attrMethod} treats the key as a sequence object and tries to determine what
  database container it is from.
\end{funcdesc}

\begin{funcdesc}{possibleKeys}{}
  Returns an iterator for the key values (prefix strings) that are allowed for 
  this dictionary, obtained from the bound \class{PrefixUnionDict}.
\end{funcdesc}

\subsubsection{PrefixDictInverse}
Provides the interface to the inverse mapping of the \class{PrefixUnionDict}.
\begin{funcdesc}{__getitem__}{k}
  Returns the fully-qualified string ID for sequence object \var{k}.  
  Properly handles both sequence annotation object and regular sequence
  objects.
\end{funcdesc}

\subsubsection{PrefixDictInverseAdder}
Adds the capability of automatically adding new sequence databases to the
\class{PrefixUnionDict}, if needed.  This is implemented by extending
the standard \method{__getitem__} method:
\begin{funcdesc}{__getitem__}{k}
  Returns the fully-qualified string ID for sequence object \var{k}.  
  Properly handles both sequence annotation object and regular sequence
  objects.  If sequence object \var{k} is from a sequence database that
  is not in the \class{PrefixUnionDict}, it will be automatically added
  to the prefixUnion, if the prefixUnion has an \member{addAll} attribute
  set to \var{True}; if not, a \code{KeyError} is raised.
  This is used in the standard \class{NLMSA} write mode 'w'
  to allow users to add sequences to the alignment without having to 
  previously add the sequence databases containing those sequences, 
  to the prefixUnion for the NLMSA.
\end{funcdesc}



\subsubsection{BlastSequenceCache}

This class is deprecated; the \class{FileDBSequence} class and associated
database container caching mechanisms provide a more powerful mechanism
that is intended to replace \class{BlastSequenceCache}.

Implements a variant of BlastSequence designed to merge and cache requests for local intervals of sequence so that repeated accesses to these regions are bundled and cached for efficiency.  You work with sequence objects of this type normally, using Python slicing to obtain subintervals, and str() to get the sequence string for a subinterval.  But behind the scenes, it does two things:

\begin{itemize}
\item
all slicing operations are recorded, in the form of a cache of superintervals.  Overlapping or adjacent intervals are merged into a superinterval up to a maximum superinterval size (default 20000).  It will automatically create as many superintervals as needed to cover the requested subinterval slices.  Each superinterval is represented by an object of the FastacmdIntervalCache class.

\item 
when the sequence string of a subinterval is requested, the cache actually retrieves (and caches) the entire superinterval containing that subinterval.  Fastacmd only needs to be called once for this superinterval.  Subsequent subinterval string requests that fall within this cached superinterval are simply returned directly from the cache, without calling fastacmd.

\end{itemize}

\subsubsection{SQLSequence}

Implements a subclass inheriting from SQLRow and NamedSequenceBase, to use a relational database table to obtain the actual sequence.  There are three minor variants DNASQLSequence, RNASQLSequence, ProteinSQLSequence (so that the sequence does not have to analyze itself to determine what kind of sequence it is).  Its constructor takes the same arguments as SQLRow(table, id), where table is the SQLTable object representing the table in which the sequence is stored, and id is the primary key of the row representing this sequence.  However, normally this class is simply passed to the Table object itself so that it will use it to instantiate new row objects whenever they are requested via its dictionary interface.  Here's a simple example:

\begin{verbatim}
class YiProteinSequence(ProteinSQLSequence): # CREATE A NEW SQL SEQUENCE CLASS
    def __len__(self): return self.protein_length  # USE LENGTH STORED IN DATABASE
protein=jun03[protein_seq_t] # protein IS OUR SQLTable OBJECT REPRESENTING PROTEIN SEQUENCE TABLE
protein.objclass(YiProteinSequence) # FORCE PROTEIN SEQ TABLE TO USE THIS TO INSTANTIATE ROW OBJECTS
pseq=protein['Hs.1162'] # GET PROTEIN SEQUENCE OBJECT FOR A SPECIFIC CLUSTER
\end{verbatim}

Let's go through this line by line:

\begin{itemize}

\item
we create a subclass of ProteinSQLSequence to show how Python makes it easy to create customized behaviors that can make database access more efficient.  Here we've simply added a __len__ method that uses the protein_length attribute obtained directly from the database, courtesy of SQLRow.__getattr__, which knows what columns exist in the database, and provides them transparently as object attributes.  (The ordinary NamedSequenceBase __len__ method calculates it by obtaining the whole sequence string and calculating its length.  Clearly it's more efficient for the database to retrieve this number (stored as a column called protein_length) and return it, rather than making it send us the whole sequence).

\item
next we call the protein.objclass() method to inform the table object that it should use our new class for instantiating any row objects for this table.  It will call this class with the usual SQLRow contructor arguments (table, id).
\end{itemize}

\subsubsection{StoredPathMapping}

Note: This class is deprecated; the \class{NLMSA} alignment database class provides
a much more powerful interface that is intended to replace older mechanisms
such as \class{StoredPathMapping}.

A second major area in Pygr is representation and query of multiple sequence alignment databases in a way that is scalable to whole genomes.  We have previously showed (in our work on Partial Order Alignment) that graphs provide both a compact and algorithmically powerful way to store alignments.  Combining this with "interval alignment" makes it scalable and gives a simple interface.  In Pygr, alignments are just another kind of graph, whose nodes are sequence intervals, and edges are alignment relations.  This provides a general-purpose facility for working with sets of sequence intervals, sequence annotation databases, and multiple sequence alignments, all queryable via Pygr graph queries.  We have implemented different container subclasses to work with these data in memory or to work transparently with data stored in relational databases.  The consistency and simplicity of the Pygr framework makes it a good interface both to run external tools like BLAST, and to store or query the results in persistent storage like a MySQL database.

\begin{verbatim}
hg17=BlastDB('/data/ucsc/hg17') # GET CONTAINER FOR HUMAN GENOME DATABASE
bcl2m=hg17['chr22'][16544303:16588541] # GET INTERVAL WITH BCL2L13 GENE
al=hg17.megablast(mouse_bcl2,maxseq=1) # GET REPEAT-MASKED MEGABLAST ALIGNMENT, ONLY TOP HIT
al[bcl2m[1000:1100] ]+=mrna[210:310] #ADD ALIGNMENT OF A 100nt SEGMENT TO mrna SEGMENT
al.storeSQL('test.table',db_cursor) # STORE COMPLETE ALIGNMENT IN RELATIONAL DATABASE
for e in MAFStoredPathMapping(bcl2m,'ucsc_maf8',u).edges(): #GET ITS MULTIGENOMEALIGNMENTS
   print str(e.srcPath),str(e.destPath) # PRINT THE ACTUAL ALIGNED SEQUENCE INTERVALS
\end{verbatim}

Note: {\em We will be unifying all sequence alignment functionality under the 
NLMSA interface design sometime in the near future.  Specifically, the
\class{PathMapping} and related classes, while similar to \class{NLMSA},
will be replaced with interfaces that are identical to the \class{NLMSA}
interface.}

\subsubsection{SliceDB}
For most applications, \class{AnnotationDB} is a better choice than
this older class.
This class enables you to apply ``slicing information'' from 
one database to sequences from a second database.  For example,
you could have a database that lists genes as intervals (slices)
on genomic sequences stored in a BlastDB database.  The only
requirements are:
\begin{itemize}
\item
{\em slice database}: must accept a string identifier as a key,
and return a slice information object as a value.

\item
{\em slice information}: a slice information object must
have the following attributes: \member{name} gives the identifier
of the sequence containing the slice; \member{start} and \member{stop}
give the coordinates of the sequence interval (which should be positive
integers following standard
Python slice coordinate conventions); \member{ori} gives the sequence
orientation as an integer (1 for positive orientation, -1 for
negative orientation).

\item
{\em sequence database}: must accept a string identifier as a key,
and return a sliceable sequence object as a value.

\end{itemize}

Both databases should raise \code{KeyError} for bad identifiers.
The current \class{SliceDB} implementation caches sequence objects so
that subsequent calls for the same identifier will not require
repeating the database queries to the two databases.  To
remove a sequence object from the cache, just use
\code{del db[id]} as usual.

SliceDB inherits from the builtin Python \class{dict} class,
so all standard methods can be used.

\begin{verbatim}
db=SliceDB(sliceDB,seqDB) # CREATE OUR DATABASE
gene=db[cluster_id] # USE IT TO GET A GENE SEQUENCE...
\end{verbatim}



\subsubsection{Functions}
The seqdb module also provides several convenience functions:

\begin{funcdesc}{read_fasta}{ifile, onlyReadOneLine=False}
  a generator function
  that yields tuples of \var{id,title,seq} from \var{ifile}.  
  The \var{onlyReadOneLine} option is useful if you only want to 
  determine the sequence type (e.g. DNA vs protein) and the
  whole sequence might be extremely long (e.g. a genome!).
\end{funcdesc}

\begin{funcdesc}{write_fasta}{ofile, s, chunk=60, id=None}
  writes the sequence \var{s}
  to the output file \var{ofile}, using \var{chunk} as the line width.
  \var{id} can provide an identifier to use instead of the default 
  \code{s.id}.
\end{funcdesc}

\subsection{pygr.Data Module}
\label{pygrData-module}
This module provides a simple but powerful interface for creating
a ``data namespace'' in which users can access complex datasets
by simply requesting the name chosen for a given dataset -- much
like Python's \code{import} mechanism enables users to access
a specified code resource by name, without worrying about where it
should be found or how to assemble its many parts.  For an introduction,
see the pygr.Data tutorial.

\subsubsection{What kinds of data can be saved in pygr.Data?}
There are a few basic principles you should be aware of:
\begin{itemize}
\item The object should be a database (container) or mapping (graph),
not an individual item of data.  pygr.Data is intended to provide
a name space for commonly used resources, i.e. an entire database,
which in turn enable you to access the items they contain.

\item The object must be {\em picklable} using Python's \module{pickle}
module.  pygr.Data uses \module{pickle} both to save your object to 
a persistent storage (either a python \module{shelve}, MySQL database,
or XMLRPC server), and to analyze its {\em dependencies} on other
Python objects.  The default pickling procedure (save a dictionary of
your object's attributes) works fine for simple Python classes.
However, if your class accesses external data (i.e. data not actually
stored in its attributes), you will have to define \method{__getstate__}
and \method{__setstate__} methods that save and restore just the 
relevant information for it to be able to access the information
it needs (e.g. if your class reads a file, \method{__getstate__} must
save its filename).  If your class inherits from \class{dict}, you
will also have to define a \method{__reduce__} method.  For a simple
example, see the classes \class{seqdb.AnnotationDB,seqdb.SeqDBbase,seqdb.XMLRPCSequenceDB}, and the \module{pickle} module documentation.

\item pygr.Data provides a {\em namespace} for commonly used data resources.
Once you import pygr.Data, you can save resources into it just as you would into
any python namespace.  For example to save an alignment object \code{nlmsa}
as the resource ID ``Bio.Seq.MSA.ucsc17'':
\begin{verbatim}
import pygr.Data # MODULE PROVIDES ACCESS TO OUR DATA NAMESPACE
pygr.Data.Bio.Seq.MSA.ucsc17=nlmsa # NOW SAVE THE ALIGNMENT
\end{verbatim}
The crucial point is that this namespace is {\em persistent} between
Python interpreter sessions.  The actual data is not saved in the pygr.Data 
module file, but in {\em resource databases} either on your disk, in
a remote XMLRPC server, or in a MySQL database (for details see below).

\item To get a named resource from pygr.Data, you again just use this
namespace, but with a ``constructor syntax'', i.e. add a call at the end of
the resource name:
\begin{verbatim}
import pygr.Data # MODULE PROVIDES ACCESS TO OUR DATA NAMESPACE
nlmsa=pygr.Data.Bio.Seq.MSA.ucsc17() # SYNTAX EMPHASIZES CONSTRUCTION OF INSTANCE
\end{verbatim}
The actual resource object is not obtained until you call the constructor.

\item pygr.Data also stores {\em schema information} for the resources.
These represent relationships between one resource and another resource
(or their contents).  For example
\begin{verbatim}
pygr.Data.schema.Bio.Genomics.ASAP2.hg17.splicegraph= \
  pygr.Data.ManyToManyRelation(exons,exons,splices, # ADD ITS SCHEMA RELATIONS
                               bindAttrs=('next','previous','exons'))
\end{verbatim}
indicates that the pygr.Data resource \code{Bio.Genomics.ASAP2.hg17.splicegraph}
is a many-to-many mapping of the pygr.Data resource \var{exons} onto itself,
with additional ``edge information'' for each exon-to-exon mapping
provided by the pygr.Data resource \var{splices}.  Furthermore, this mapping
is to be bound directly to items of \var{exons} (i.e. objects returned
from \code{exons.__getitem__}) as their \member{next} attribute (giving the
forward mapping), their \member{previous} attribute (giving the reverse
mapping), and the \member{exons} attribute on items of \var{splices}
(giving the mapping of the splice object to its pair of (source,target) exons
as a tuple).

\item when a user requests a resource that itself depends on other
resources, pygr.Data automatically loads them.  Thus users do not need
to know about the complex set of dependencies between data; all they
have to do ask is ask for the specific data resource they want,
and pygr.Data will take care of all the details behind the scenes.
For example, a database of exon annotations is not very useful without
also loading the genomic sequence database that these annotations
refer to.  Using pygr.Data, we can simply load the exon annotation
resource, and it will automatically get the genomic sequence data
for us.  Thus to get an exon's sequence all we have to do is:
\begin{verbatim}
exons=pygr.Data.Bio.Genomics.ASAP2.hg17.exons() # ANNOTATION DATABASE
str(exons[464]) # GET THE SEQUENCE OF THIS SPECIFIC EXON
\end{verbatim}


\item When one resource {\em depends} on another, the order in which you
save them to pygr.Data matters.  Specifically, you should save them in
order of their dependencies, i.e. if \var{B} depends on \var{A}, first save
\var{A}, then \var{B}.  If you do it this way, \var{B} will be stored
in pygr.Data with a reference to the pygr.Data resource ID you assigned
to \var{A}, whereas if you saved \var{B} before \var{A}, it will be stored
with a direct reference to object \var{A} instead of to its resource ID.
This will still work, but will make pygr.Data less intelligent about how
it retrieves resource \var{A}.  For example, if you opened \var{B} on
another computer that didn't have \var{A} accessible in exactly the same
way as on your original computer, this would simply fail -- even if
pygr.Data had resource rules for obtaining the \var{A} resource ID
remotely.

\item It should be noted that at the moment there is only one name
(\code{Bio}) at the top-level of the pygr.Data module namespace (since currently
this is only being used for bioinformatics).  However it's 
trivial to add new names as \class{ResourcePath} objects to the pygr.Data
module.

\end{itemize}

\subsubsection{How does pygr.Data access resource databases?}
The list of resource databases is read from the environment variable
PYGRDATAPATH.  If this variable is empty or missing, the default path
for pygr.Data to search is the user's home directory (\$HOME) and
current directory, in that order.  PYGRDATAPATH should be a comma separated list
of ``resource path'' strings, which must be one of the following:
\begin{itemize}
\item A directory path (e.g. /usr/local/pygrdata), in which pygr.Data should
look for (or, if none present, create) a database file called ``.pygr_data''.  
You can use the tilde character to indicate your home directory path.
These are accessed by pygr.Data using its \class{ResourceDBShelve} class.

\item a URL for accessing an XMLRPC server that is serving a pygr.Data
resource database index (previously started by you or someone else).
The URL must begin with ``http://''.
These are accessed by pygr.Data using its \class{ResourceDBClient} class.

\item a MySQL server, indicated by a path entry of the form 
``mysql:\var{DBNAME.TABLENAME}'',
where \var{DBNAME} is the name of the database in your MySQL
server that contains the pygr.Data resource index,
and \var{TABLENAME} is the name of the table which contains this index.
pygr.Data will get the host, user, and password information for connecting 
to the MySQL server as usual from your
.my.cnf configuration file in your home directory.
Such resource databases are accessed by pygr.Data using its 
\class{ResourceDBMySQL} class.
\end{itemize}

\subsubsection{pygr.Data Layers}
To provide an intuitive way to refer to different resource databases,
pygr.Data associates ``layer names'' with them.  For example, the layer
name for the first resource database whose path is given relative to 
your home directory is \code{my}, and the first one whose path is given
relative to current directory is \code{here}.  Remote resource databases
(XMLRPC; MySQL) each store their own layer name.  For example, within the
Lee lab, we keep a MySQL resource database whose layer name is ``leelab''.

\begin{itemize}
\item You can specify precisely which layer you want to access by prefixing
your pygr.Data resource name with the desired layer name, e.g.
\begin{verbatim}
nlmsa=pygr.Data.leelab.Bio.Seq.MSA.ucsc17()
\end{verbatim}

\item Similarly, you can specify which layer you want to store a resource
or schema, in the same way:
\begin{verbatim}
pygr.Data.leelab.schema.Bio.Genomics.ASAP2.hg17.splicegraph= \
  pygr.Data.ManyToManyRelation(exons,exons,splices, # ADD ITS SCHEMA RELATIONS
                               bindAttrs=('next','previous','exons'))
\end{verbatim}

\item If you do not specify a layer, pygr.Data uses the first resource
database in its list that returns the desired resource.

\item You can delete a resource and its schema rules from a specific resource
database by specifying its layer name:
\begin{verbatim}
del pygr.Data.leelab.Bio.Seq.MSA.ucsc17
\end{verbatim}

\item pygr.Data provides a set of default layer names:
the first resource database whose path is given relative to 
your home directory is \code{my}; the first one whose path is given
relative to current directory is \code{here};the first one whose path is given
relative to the root directory / is \code{system};
the first one whose path begins ``http://'' is \code{remote};
the first one whose path begins ``mysql:'' is \code{MySQL}.

\end{itemize}

\subsubsection{pygr.Data Schema Concepts}
Parallel to the pygr.Data namespace, pygr.Data maintains a schema namespace 
that records schema information for pygr.Data resources.  Broadly speaking,
{\em schema} is any relationship that holds true over a set of data in a given
collection (e.g. in the human genome, ``genes have exons'', a one-to-many relation).
In traditional (relational) databases, this schema information is usually
represented by {\em entity-relationship diagrams} showing foreign-key
relationships between tables.  A pygr.Data resource is a collection
of objects (referred to in these docs as a ``container'' or ``database'');
thus in pygr, schema is a relation between pygr.Data resources, i.e.
a relationship that holds true between the items of one pygr.Data resource
and the items of another.  For examples, items in a ``genes'' resource
might each have a mapping to a subset of items in an ``exons'' resource.
This is achieved in pygr.Data by saving the mapping object itself as a pygr.Data
resource, and then specifying its schema to pygr.Data (in this example,
its schema would be a one-to-many relation between the ``genes''
resource and the ``exons'' resource).  Saving the mapping object 
as a pygr.Data resource, and saving its schema information, are
two separate steps.
\begin{verbatim}
pygr.Data.Bio.Genomics.ASAP2.hg17.geneExons=geneToExons # SAVE MAPPING
pygr.Data.schema.Bio.Genomics.ASAP2.hg17.geneExons= \
  pygr.Data.OneToManyRelation(genes,exons,bindAttrs=('exons','gene'))
\end{verbatim}
assuming that \code{genes} and \code{exons} are the pygr.Data resources
that are being mapped.  This would allow a user to obtain the mapping
from pygr.Data and use it just as you'd expect, e.g. assuming that
\code{gene} is an item from \code{genes}:
\begin{verbatim}
geneToExons=pygr.Data.Bio.Genomics.ASAP2.hg17.geneExons()
myexons=geneToExons[gene] # GET THE SET OF EXONS FOR THIS GENE
\end{verbatim}
In practice, pygr.Data accomplishes this by automatically setting
\code{geneToExon}'s \code{sourceDB} and \code{targetDB} attributes
to point to the \code{genes} and \code{exons} resources, respectively.

Since most users find it easier to remember object-oriented behavior
(e.g. ``a gene has an exons attribute'', rather than ``there exists a 
mapping between gene objects and exon objects, called geneToExons''),
pygr.Data provides an option to bind attributes of the mapped
resource items.  In the example above, we bound an \member{exons} attribute
to each item of \code{genes}, which automatically performs this mapping,
e.g. we can iterate over all exons in a given gene as easily as
\begin{verbatim}
for exon in gene.exons: # gene.exons IS EQUIVALENT TO geneToExons[gene]
  # DO SOMETHING...
\end{verbatim}
Note: in this usage, the user does not even need to know about the 
existence of the \code{geneToExons} resource; pygr.Data will load it
automatically when the user attempts to access the \code{gene.exons}
attribute.  It can do this because it knows the schema of the pygr.Data
resources!

One additional aspect of pygr.Data schema relations goes a bit beyond
ordinary mapping: a mapping between one object (source) and another
(target) can have {\em edge information} that describes this specific
relationship.  For example, the connection
between one exon and another in the alternative splicing of an mRNA
isoform, is a {\em splice}.  For alternative splicing analysis, it is
actually crucial to have detailed information about the splice (e.g.
what experimental evidence exists for that splice; what tissues it was
observed, in what fraction of isoforms etc.) in addition to the exons.
Therefore, pygr.Data allows us to save edge information also as part 
of the schema, e.g. for a \code{splicegraph} representing the set of
all splices (edges) between pairs of exons (nodes), we can
store the schema as follows:
\begin{verbatim}
pygr.Data.Bio.Genomics.ASAP2.hg17.splicegraph=splicegraph # ADD A NEW RESOURCE
pygr.Data.schema.Bio.Genomics.ASAP2.hg17.splicegraph= \
  pygr.Data.ManyToManyRelation(exons,exons,splices, # ADD ITS SCHEMA RELATIONS
                               bindAttrs=('next','previous','exons'))
\end{verbatim}
This type of mapping (``edge'' relations between pairs of ``nodes'')
is referred to in mathematics as a {\em graph}, and has very general
utility for many applications.  For further information on graphs in 
pygr, see the tutorial or the \module{mapping} module reference below.

What information does pygr.Data schema actually store?  In practice,
the primary information stored is {\em attribute} relations: 
i.e. for a specified resource ID, a specified attribute name
should be added to the resource object (or to items obtained
from it), which in turn maps to some specified target resource
(or items of that resource).  

Although users do not need to know
how this information is saved, I will outline the methodology
as a reference for developers who want to work directly with this
internal data (skip this section otherwise).
\begin{itemize}
\item In a given resource database (dictionary), information for constructing a 
given resource \code{id} is stored with its resource ID as the key.  
i.e. if \code{rdb} is a resource database, \code{rdb[id]} gives
the string to unpickle to construct the resource.  Schema information
for that resource is stored as \code{rdb['SCHEMA.'+id]}.

\item This schema information (for a given resource) is itself
a dictionary, whose keys are attribute names to bind to this 
resource, and whose associated values are themselves dictionaries
specifying the rules for what to bind to this attribute and how.
See below for further details.

\item Attributes are added as ``shadow attributes'' provided by 
descriptors added to the class object for the resource or to
its \member{itemClass} or \member{itemSliceClass} object if the
attribute is to be bound to {\em items of the resource}.  Descriptors
(also referred to in the Python documentation as ``properties'')
are the major mechanism by which Python new-style classes
(i.e. subclasses of \class{object} in Python 2.2 and later)
can execute code in response to a user attempt to get an
object attribute, and are definitely preferable over writing
\method{__getattr__} method code if all that's desired
is an attribute with a specified name.  For more information
on descriptors, see the Python Reference Manual.

\item The basic principles of these ``shadow attributes'' are that
1. they are bound to the class object, not the instance object;
2. they are only invoked if the specified attribute name is 
missing from the instance object's \member{__dict__}; 
3. once invoked, they save their
result on the instance object (in its \member{__dict__})
as the same-named attribute; 4. thus, the descriptor method
will only be called once; thereafter the attribute will be 
obtained directly from the value cached on the instance object;
5. the descriptor only loads its target resource(s) when the user
attempts to read the value of the attribute.  Thus no extra
resources are loaded until the user actually demands information
that requires them.

\item Currently, these shadow attributes are implemented by
three different descriptor classes in pygr.Data: 
\class{OneTimeDescriptor}, for binding attributes directly on a resource
object (container);
 \class{ItemDescriptor}, for binding attributes on items (or slices of
items) obtained from a resource object (via its __getitem__ method);
\class{SpecialMethodDescriptor}, for binding special Python methods like
\method{__invert__}.

\item The rule information for a given attribute is itself a dictionary,
with the following string keys governing the behavior of the shadow attribute.
\var{targetID}: the pygr.Data resource ID of the resource that this
attribute links to.
\var{itemRule}: True if the attribute should be bound to {\em items}
(and slices of items, if defined) of the source resource, rather than
directly to the source resource object itself (if itemRule=False).
\var{invert}: True if the target resource should first be inverted
(i.e. query its reverse-mapping rather than its forward-mapping), False otherwise.
\var{getEdges}: True if the attribute should query the target resource's
\member{edges} mapping (i.e. the mapping provided by its \member{edges} attribute)
rather than its forward mapping, False otherwise.
\var{mapAttr}: if not None, use this named attribute of our source object,
instead of the source object itself, as the key for search the target resource
mapping.
\var{targetAttr}: if not None, return this named attribute of the result of
the search, rather than the result of the search itself.
\end{itemize}

\subsubsection{ResourceFinder}
The core functionality of the pygr.Data module is provided by the
\class{ResourceFinder} class, an instance of which is created at the
top-level of the module as \code{pygr.Data.getResource}.  It 
provides methods for adding, deleting and controlling pygr.Data
resources and schema.

\begin{funcdesc}{getResource}{id,layer=None,*args,**kwargs}
  Look up pygr.Data resource \var{id}, using the specified abstract
  resource \var{layer} if provided.  Searches the resouce database(s)
  for \var{id}, constructs it from the saved resource rule (e.g. from
  a local resource database, by unpickling the object).  Saves the 
  object in its cache so that subsequent calls for the same resource
  ID will return the same object.  Applies the stored pygr.Data schema
  rules to it using \method{applySchema}().  Marks the object with
  its \member{_persistent_id} attribute, whose value is just \var{id}.
\end{funcdesc}

\begin{funcdesc}{getResource.addResource}{id,obj,layer=None}
  Add \var{obj} to pygr.Data as resource ID \var{id}, specifically within
  abstract resource \var{layer} if provided.  Saves \var{obs} to 
  the resource database, and marks it with its \member{_persistent_id}
  attribute, whose value is just \var{id}.  For a resource \var{id} 'A.Foo.Bar'
  this method is equivalent to the assignment statement
\begin{verbatim}
pygr.Data.A.Foo.Bar=obj
\end{verbatim}
  This method is provided mainly to enable writing code that automates
  saving of resources, e.g. via code like
\begin{verbatim}
for id,genome in nlmsa.seqDict.prefixDict.items(): # 1st SAVE THE GENOMES
    pygr.Data.getResource.addResource('Bio.Seq.Genome.'+id,genome)
\end{verbatim}
\end{funcdesc}

\begin{funcdesc}{getResource.deleteResource}{id,layer=None}
  Delete resource \var{id} from the resource database specified by
  \var{layer} if provided (or the default resource database otherwise).
  Also delete its associated schema information.
\end{funcdesc}

\begin{funcdesc}{getResource.newServer}{name,serverClasses=None,clientHost=None,withIndex=False,**kwargs}
  Create and return a new XMLRPC server to serve all pygr.Data resources 
  currently loaded in memory that are capable of XMLRPC client-server
  operation.  The server \var{name} will be used for 
  purposes of XMLRPC communication.  The \var{withIndex=True} option
  will cause the server to also act as a pygr.Data resource database
  accessible via XMLRPC (i.e. add its URL to your PYGRDATAPATH environment
  variable, to make its resources accessible to any Python script).
  In this case, the server will add itself as new pygr.Data layer
  \var{name}, for any Python script that accesses its resource index.

  \var{serverClasses} allows you to specify a list of tuples of
  classes that can be served via XMLRPC.  Each tuple should consist of
  three values: \var{(dbClass,clientClass,serverClass)}, where 
  \var{dbClass} is a normal pygr class, \var{clientClass} is the 
  class to use for the XMLRPC client version of this data, and
  \var{serverClass} is the class to use for the XMLRPC server of
  this data.  If no value is provided to this option, the current
  default is 
\begin{verbatim}
[(seqdb.BlastDB,seqdb.XMLRPCSequenceDB,seqdb.BlastDBXMLRPC),
 (cnestedlist.NLMSA,xnestedlist.NLMSAClient,xnestedlist.NLMSAServer)] 
\end{verbatim}
  The \var{clientHost} option allows you to override the hostname
  that clients will be instructed to connect to.  The default is simply
  the fully qualified hostname of your computer.  But if, for example,
  you wished to access your server by port-forwarding localhost port 5000
  to your server port via SSH, you could pass a \var{clientHost}='localhost'
  setting.

  Once you create a server using this method, you start it using its
  \method{serve_forever()} method.  If the server does not provide its
  own index (i.e. \var{withIndex=False}), then you should first register
  it to your local resource database server (so that clients of that server
  will know about the new services your new server is providing), by
  calling its \method{register()} method.
\end{funcdesc}

The following methods are mainly for internal use, and are unlikely to be
needed by users of pygr.Data.
\begin{funcdesc}{update}{}
  Update \code{getResource}'s list of resource databases, by parsing the environment
  variable PYGRDATAPATH and attempting to connect to the resource databases
  listed there.  Does not return anything.
\end{funcdesc}

\begin{funcdesc}{addLayer}{layerName,rdb}
  Add the resource database \var{rdb} to the current resource database list,
  as a named layer given by the string \var{layerName}.  Over-writing an 
  existing layer name is not allowed, for security reasons; 
  the previous layer entry must first be deleted.
\end{funcdesc}

\begin{funcdesc}{getLayer}{layerName}
  Get the specified resource database, by its layer name.  If \var{layerName}
  is None, returns the default (first) resource database in its list.
\end{funcdesc}

\begin{funcdesc}{resourceDBiter}{}
  Generates all the resource databases currently listed by \code{getResource}.
\end{funcdesc}

\begin{funcdesc}{registerServer}{locationKey,serviceDict}
  Registers the set of resources specified by \var{serviceDict} to the
  first resource database index in PYGRDATAPATH that will accept them.
  \var{serviceDict} must be a dictionary whose keys are resource IDs and
  whose associated values are pickled resource objects (encoded as strings).
  \var{locationKey} should be a string name chosen to represent the ``location''
  where the data are stored.  This can be anything you wish, and is mainly used
  to let the user know where the data will come from.  This might be used
  in future versions of pygr.Data to allow preferential screening of where
  to get data from (local disk is better than NFS mounted disk, which in turn
  might be preferable over remote XMLRPC data access).
\end{funcdesc}

\begin{funcdesc}{findSchema}{id}
  Returns a dictionary for the schema (if any) found for the pygr.Data resource 
  specified by \var{id}.  The dictionary keys are attribute names (representing
  attributes of the specified resource or its contents that should have 
  schema relations with other pygr.Data resources), and whose values are
  themselves dictionaries specifying the precise schema rules for constructing
  this specific attribute relation.
\end{funcdesc}

\begin{funcdesc}{schemaAttr}{id,attr}
  Return the target data linked to by attribute \var{attr} of pygr.Data
  resource \var{id}, based on the stored pygr.Data schema.  The target resource
  object will be obtained by pygr.Data.getResource as usual.
\end{funcdesc}

\begin{funcdesc}{applySchema}{id,obj}
  Apply the pygr.Data schema for resource \var{id} to the actual data
  object representing it (\var{obj}), by decorating it (and / or its itemClass
  and itemSliceClass) with properties representing its schema attributes.
  These properties are implemented by adding descriptor attributes to the
  associated class, such as \class{OneTimeDescriptor} or \class{ItemDescriptor}.
\end{funcdesc}

\begin{funcdesc}{saveSchema}{id,attr,bindingDict,layer=None}
  Save a schema attribute relation for attribute \var{attr} of pygr.Data
  resource \var{id}, to the specified resource database \var{layer} (or the default,
  first resource database in the list, if no layer specified).
  \var{bindingDict} must be a dictionary specifying the rules for
  binding the attribute to a pygr.Data resource target; see below for details.
\end{funcdesc}

\begin{funcdesc}{delSchema}{id,layer=None}
  Delete schema bindings for all attributes of the resource \var{id}, in
  the specified resource database \var{layer}, as well as all schema relations
  on other resources that are targeted to resource \var{id}.
\end{funcdesc}

\subsubsection{ResourceDBMySQL}
Implements an interface to storage of a resource database in a MySQL
database table.
\begin{funcdesc}{__init__}{tablename,finder=None,createLayer=None}
  \var{tablename} is the table to use in the database, in the format
  ``\var{DBNAME.TABLENAME}'', where \var{DBNAME} is the name of the
  database in the MySQL server, and \var{TABLENAME} is the name of
  the table in that database that you wish to use to store the
  resource database.  Host, port, user and password info are obtained
  from your .my.cnf config file as usual for the mysql client.

  \var{finder}, if specified gives the \class{ResourceFinder} instance
  in which the new resource DB should be registered.  If None provided,
  defaults to pygr.Data.getResource.

  \var{createLayer}, if specified forces it to create a new table
  in the MySQL database (instead of assuming that it already exists),
  and saves \var{createLayer} as the layer name of this resource database.

  Example: create a new resource database, give it the layer name ``leelab'',
  and register it in our list of resource databases.
\begin{verbatim}
rdb=pygr.Data.ResourceDBMySQL('pygrdata.index',createLayer='leelab')
\end{verbatim}
\end{funcdesc}

\begin{funcdesc}{__getitem__}{id}
  Get resource \var{id} from this resource database, or \code{KeyError} 
  if not found.
\end{funcdesc}

\begin{funcdesc}{__delitem__}{id}
  Delete resource \var{id} from this resource database, or \code{KeyError} 
  if not found.
\end{funcdesc}

\begin{funcdesc}{__setitem__}{id,obj}
  Save resource \var{id} to this resource database, by pickling it
  with \code{self.finder.dumps(obj)}.
\end{funcdesc}

\begin{funcdesc}{registerServer}{locationKey,serviceDict}
  Saves the set of resources specified by \var{serviceDict} to the
  database.
  \var{serviceDict} must be a dictionary whose keys are resource IDs and
  whose associated values are pickled resource objects (encoded as strings).
  \var{locationKey} should be a string name chosen to represent the ``location''
  where the data are stored.  This can be anything you wish, and is mainly used
  to let the user know where the data will come from.  This might be used
  in future versions of pygr.Data to allow preferential screening of where
  to get data from (local disk is better than NFS mounted disk, which in turn
  might be preferable over remote XMLRPC data access).
\end{funcdesc}

\begin{funcdesc}{setschema}{id,attr,ruleDict}
  Save schema information for attribute \var{attr} on resource \var{id}
  by pickling the \var{ruleDict}.
\end{funcdesc}

\begin{funcdesc}{delschema}{id,attr}
  Delete schema information for attribute \var{attr} on resource \var{id}.
\end{funcdesc}

\begin{funcdesc}{getschema}{id}
  Get schema information for resource \var{id}, in the form of a dictionary
  whose keys are attribute names, and whose values are the associated
  schema \var{ruleDict} for each bound attribute.
\end{funcdesc}

\subsubsection{ResourceDBShelve}
Implements an interface to storage of a resource database in a Python
\module{shelve} (i.e. BerkeleyDB file) stored on local disk.
Provides the same interface as \class{ResourceDBMySQL}, except for
no \method{registerServer} method.  Note: any method call that would
save information to the database temporarily re-opens the database
file in write mode, saves the required information, and immediately
closes and re-opens
the databae in read-only mode.  Thus, unless two clients try
to save information to the same file at exactly the same time,
successive writes by multiple clients will not interfere with each
other.
\begin{funcdesc}{__init__}{dbpath,finder,mode='r'}
  \var{dbpath} is the path to the directory in which the shelve
  file is found (or should be created, if none present).
\end{funcdesc}

\subsubsection{ResourceDBClient}
Implements a client interface to storage of a resource database in an XMLRPC
server.  For security reasons, only provides the \method{__getitem__},
and \method{registerServer} methods.

\subsubsection{ResourceDBServer}
Implements a server interface for storage of a resource database in 
a standard Python dict, served to clients via an XMLRPC
server (use \class{coordinator.XMLRPCServerBase} as the XMLRPC
server to serve this object).  

\begin{funcdesc}{__init__}{layerName,readOnly=False}
  \var{layerName} is the layer name that this server will provide
  to pygr.Data clients.  \var{readOnly} if True, makes the server reject
  any requests to add new database rules received via XMLRPC, i.e.
  only allows \method{getName} and \method{getResource} calls via XMLRPC.
  If False, also allows calls to \method{registerServer} and \method{delResource}.
\end{funcdesc}

\subsubsection{ResourcePath}
Used for providing the dynamically extensible pygr.Data namespace
that provides the normal interface for users to access pygr.Data resources.
\begin{funcdesc}{__init__}{namepath,layerName=None}
  \var{namepath} specifies the ID string to use for this resourcePath.
  \var{layerName} if specified, gives the layer name that should be used
  for finding this resource and any subattributes of it.

  For example, \code{Bio} is added at the top-level of the pygr.Data module
  by the following code:
\begin{verbatim}
Bio=ResourcePath('Bio')
\end{verbatim}
\end{funcdesc}

\begin{funcdesc}{__getattr__}{attr}
  extends the resource path by one step, returning a
  \class{ResourcePath} object representing the requested attribute.
\end{funcdesc}

\begin{funcdesc}{__setattr__}{attr,obj}
  saves \var{obj} as the specified resource ID, by calling
  \method{getResource.addResource}, with our layer name (if any).
\end{funcdesc}

\begin{funcdesc}{__delattr__}{attr}
  deletes the specified resource ID, by calling
  \method{getResource.deleteResource}, with our layer name (if any).
\end{funcdesc}

\begin{funcdesc}{__call__}{*args,**kwargs}
  Construct the specified resource ID, by calling \method{getResource},
  with our layer name (if any), and the specified arguments (if any).
\end{funcdesc}

\subsubsection{SchemaPath}
Class for top-level object representing a schema namespace.  e.g. in the pygr.Data
module,
\begin{verbatim}
schema=SchemaPath() # CREATE ROOT OF THE schema NAMESPACE
\end{verbatim}

\subsubsection{ResourceLayer}
Class for top-level object representing a pygr.Data layer.  e.g. in the pygr.Data
module,
\begin{verbatim}
here=ResourceLayer('here') # CREATE TOP-LEVEL INTERFACE TO here LAYER
\end{verbatim}

\subsubsection{ForeignKeyMap}
Provides a mapping between two containers, assuming that items of the target
container have a foreign key attribute that gives the ID of an item in the source
container.
\begin{funcdesc}{ForeignKeyMap}{foreignKey,sourceDB=None,targetDB=None}
  \var{foreignKey} must be a string attribute name for the foreign key on
  items of the \var{targetDB}.  Furthermore, \var{targetDB} must provide
  a \method{foreignKey} method that takes two arguments: the \var{foreignKey} attribute name,
  and an identifier that will be used to search its items for those whose attribute
  matches this identifier.  It must return an iterator or list of the matching items.
\end{funcdesc}

\begin{funcdesc}{__getitem__}{id}
  get a list of items in \var{targetDB} whose attribute matches this \var{id}.
\end{funcdesc}

\begin{funcdesc}{__invert__}{}
  get an interface to the reverse mapping, i.e. mapping object that takes an
  item of \var{targetDB}, and returns its corresponding item from \var{sourceDB},
  based on the input item's foreign key attribute value.
\end{funcdesc}

For example, given a container of clusters, and a container of exons (that each
have a \member{cluster_id} attribute), we create a mapping between them as follows:
\begin{verbatim}
m=ForeignKeyMap('cluster_id',clusters,exons)
for exon0 in m[cluster0]: # GET EXONS IN THIS CLUSTER
    do something...
cluster1=(~m)[exon1]  # GET CLUSTER OBJECT FOR THIS EXON
\end{verbatim}


\subsubsection{DirectRelation, ItemRelation, InverseRelation}
\class{DirectRelation} is a convenience class for constructing 
a single schema attribute relation on a pygr.Data resource,
linking it to another pygr.Data resource.
\begin{funcdesc}{__init__}{target}
  \var{target} gives a reference to a pygr.Data resource, which will
  be the target of a bound schema attribute.  \var{target} can be either
  a string resource ID, a \class{ResourcePath} object, or
  an actual pygr.Data resource (automatically marked with its ID
  as the \member{_persistent_id} attribute).
\end{funcdesc}

\begin{funcdesc}{schemaDict}{}
  returns a basic \var{ruleDict} dictionary for saving this schema binding.
  Can be over-ridden by subclasses to customize schema binding behavior.
\end{funcdesc}

\begin{funcdesc}{saveSchema}{source,attr,layer=None,**ruleDict}
  Saves a schema binding for attribute \var{attr} on pygr.Data resource
  \var{source} to the specified resource database \var{layer} (or
  to the default resource database if not specified).  \var{ruleDict}
  if specified provides additional binding rules (which can add to or
  over-ride those returned by the \method{schemaDict} method).
  \var{source} can be either
  a string resource ID, a \class{ResourcePath} object, or
  an actual pygr.Data resource (automatically marked with its ID
  as the \member{_persistent_id} attribute).
\end{funcdesc}

\class{ItemRelation} provides a subclass of \class{DirectRelation}
that binds to the {\em items} of resource \var{source} rather than to the
\var{source} object itself.

\class{InverseRelation} provides a subclass of \class{DirectRelation},
that binds \var{source} and \var{target} as each other's inverse mappings.
That is, it binds an \member{inverseDB} attribute to each resource
that points to the other resource.  When either resource is loaded,
a special \method{__invert__} method will be added, that simply
loads and returns the resource pointed to by the \member{inverseDB}
binding.

\subsubsection{ManyToManyRelation}
Convenience class for constructing schema relations for
a general graph mapping from a sourceDB to targetDB with edge info.
\begin{funcdesc}{__init__}{sourceDB,targetDB,edgeDB=None,bindAttrs=None}
  \var{sourceDB},\var{targetDB}, and \var{edgeDB} can be either
  a string resource ID, a \class{ResourcePath} object, or
  an actual pygr.Data resource (automatically marked with its ID
  as the \member{_persistent_id} attribute).
  \var{bindAttrs}, if provided, must give a list of string attribute names to be
  bound, in order, to items of \var{sourceDB}, \var{targetDB},
  and \var{edgeDB}, in that order.  A None value in this list simply
  means that no attribute binding will be made to the corresponding
  pygr.Data resource.
\end{funcdesc}
Note: this class simply records the information necessary for this
schema relation.  The information is not actually saved to the resource
database until its \method{saveSchema} method is called by 
the \class{SchemaPath} object.  In addition to saving attribute
bindings given by \var{bindAttrs}, this will also create bindings
on the mapping resource object itself (i.e. the resource whose
schema is being set; see an example in the tutorial).  Specifically,
it will save bindings for its \member{sourceDB},\member{targetDB},
and \member{edgeDB} attributes to the corresponding resources
given by the \var{sourceDB},\var{targetDB},
and \var{edgeDB} arguments.

\subsection{sqlgraph Module}
\label{sqlgraph-module}
This module provides back-end database access.

\subsubsection{SQLTable}
Provides a \class{dict}-like interface to an SQL table.  It accepts
an identifier as a key, and returns a Python object representing
the corresponding row in the database.  Typically, these ``row''
objects have an \member{id} attribute that represents the
primary key, and all column names in the SQL table can be
used as attribute names on the row object.

This class assumes that the database table has a primary key,
which is used as the key value for the dictionary.  For tables
with no primary key see other variants below.

This class and its variants follow a simple rule for controlling
how data is loaded into memory: if you simply iterate over IDs
(i.e. \code{for id in mytable}) data is not pre-loaded into memory;
each object will be fetched individually when you try to access it
(e.g. \code{obj=mytable[id]}).  By contrast, if you call the table's
\method{items} method, it will load data for the entire table into
memory, on the rational basis that making this call signals that you
intend to work with each and every object in the database table.  Other
``value'' iterators such as \method{iteritems}, \method{values},
and \method{itervalues} also load all the data.

\begin{funcdesc}{__iter__}{}
  Iterate over all IDs (primary key values) in the table,
  without loading the entire table into memory.
\end{funcdesc}

\begin{funcdesc}{items}{}
  return a list of all (id,obj) pairs representing all data in the table,
  after first loading the entire table into memory.
\end{funcdesc}

\begin{funcdesc}{__getitem__}{id}
  get the object whose primary key is \var{id}, and cache it in
  our local dictionary (so that subsequent requests will return the
  same Python object, immediately, with no need to re-run an SQL query).
  For non-caching versions of \class{SQLTable}, see below.
\end{funcdesc}


\begin{funcdesc}{load}{oclass=None}
  Load all data from the table, using \var{oclass} as the row object
  class if specified (otherwise use the oclass for this table).
  All rows are loaded from the database and saved as row objects
  in the Python dictionary of this class.
\end{funcdesc}

\subsubsection{SQLTableBase}
The base class for \class{SQLTable} and other variants below.
This class is derived from
the Python builtin \class{dict} class, so all standard methods of \class{dict}
can be used.

\begin{funcdesc}{__init__}{name,cursor=None,itemClass=None,attrAlias=None,clusterKey=None}
  Open a connection to the existing SQL table specified by \var{name}.
  You can supply a Python DB API \var{cursor} providing a connection
  to the database server.  If \var{cursor} is None, it will attempt
  to connect to a MySQL server using authentication information from your
  .my.cnf configuration file as usual.

  \var{itemClass} indicates 
  the class that should be used for constructing item objects (representing
  individual rows in the database).

  \var{attrAlias}, if provided, must be a dictionary whose keys are
  attribute names that should be bound to items from your database,
  and whose values are an SQL column name or SQL expression that should
  be used to obtain the value of the bound attribute.

  \var{clusterKey}, if provided, is a caching hint for speeding up
  database access by ``clustering'' queries to load an entire block
  of rows that share the same value of the specified \var{clusterKey} column.
  This caching hint is only used by the \class{Clustered} SQLTable variants
  described in detail below.
\end{funcdesc}

\begin{funcdesc}{objclass}{itemClass}
  Specify a object class to use for creating new ``row'' objects.
  \var{itemClass} must accept a single argument, a tuple object representing
  a row in the database.  

  Otherwise, the default \var{oclass} for SQLTable is
  the \class{TupleO} class, which provides a named attribute interface
  to the tuple values representing the row.
\end{funcdesc}

\begin{funcdesc}{select}{whereClause,params=None,oclass=None,selectCols='t1.*'}
  Generate the list of objects that satisfy the \var{whereClause}
  via a SQL SELECT query.  This function is a generator, so you
  use it as an iterator.  \var{params} is passed to the
  cursor execute statement to allow additional control over
  the query.  \var{selectCols} allows you to control what subset of
  columns should actually be retrieved.
\end{funcdesc}

\begin{funcdesc}{_attrSQL}{attr}
  Get a string expression for accessing attribute \var{attr} in SQL.
  This might either simply be an alias to the corresponding column
  name in the SQL table, or possibly an SQL expression that computes
  the desired value, executed on the database server.
\end{funcdesc}



There are several variants of this class:
\subsubsection{SQLTableClustered}
A subclass of \class{SQLTable} that groups its retrieval
of data from the table (into its local dictionary, where it
is cached), into ``clusters'' of rows that share the same value of
a column specified by the \var{clusterKey} argument to the \class{SQLTableBase}
constructor.  For data that naturally subdivide into large clusters,
this can speed up performance considerably.  If the clustering
closely mirrors how users are likely to access the data, this
performance gain will have relatively little cost in terms
of memory wasted on loading rows that the user will not need.


\subsubsection{SQLTableNoCache}
Provide on-the-fly access to rows in the database, 
but never cache results.  Use this when memory constraints or other 
considerations (for example, if the data in the database may change
during program execution, and you want to make sure your program
is always working with the latest version of the data) 
make it undesirable to cache recently used row objects, as the
standard \class{SQLTable} does.  Instead it returns (by default)
\class{SQLRow} objects that simply provide an interface
to obtain desired data attributes via database SQL queries.
Of course this reduces performance; every attribute access
requires an SQL query.  You can customize the class used for
providing this interface by specifying a different \var{itemClass}
to the constructor.

\subsubsection{SQLMultiTableNoCache}
Drops the assumption of a one-to-one
mapping between each key and a row object (i.e. removes the
assertion that the key is unique, a ``primary key''), allowing
multiple row objects to be returned for a given key.  Therefore,
the standard \method{__getitem__} must act as a generator, returning
an iterator for one or more row object.  You must set a 
\member{_distinct_key} attribute to inform it of which 
column to use as the key for searching the database;
this defaults to ``id''.

\subsubsection{SQLGraph}
Provides a graph interface to data stored in a table
in a relational database.  It follows the standard pygr 
graph interface, i.e. it behaves like a dictionary whose
keys are IDs of {\em source nodes}, and whose associated
values are dictionaries whose keys are IDs of {\em target nodes},
and whose associated values are IDs for each {\em edge} between
a pair of nodes.  This class is a subclass of 
\class{SQLTableMultiNoCache}.  By default, it assumes that
the column names for source, target and edge IDs are simply
``source_id'', ``target_id'', and ``edge_id'' respectively.
To use different column names, simply provide an \var{attrAlias}
dictionary to the constructor, e.g.
\begin{verbatim}
g=SQLGraph('YOURDB.YOURTABLE',attrAlias=dict(source_id='left_exon_form_id',
                                             target_id='right_exon_form_id',
                                             edge_id='splice_id'))
\end{verbatim}
For good performance, the columns storing the source_id, target_id,
and edge_id should each be indexed.

\begin{funcdesc}{__contains__}{id}
  Test whether \var{id} exists as a source node in this graph.
\end{funcdesc}

\begin{funcdesc}{__invert__}{}
  Return an \class{SQLGraph} instance representing the reverse 
  directed graph (i.e. swap target nodes for source nodes).
\end{funcdesc}

\subsubsection{SQLGraphClustered}
Provides a graph interface with improved performance based on
using \class{SQLTableClustered} as the interface to the database
table.  This has several implications: 1. the table should have
a primary key; 2. the table should have a \var{clusterKey}
column that provides the value for clustering rows in the table.
This class can offer much better performance than \class{SQLGraph}
for several reasons: 1. it caches data so that subsequent requests
for the same node or edge will be immediate, with no need to query
the SQL database; 2. it employs clustering to group together 
data retrieval of many rows at a time sharing the same cluster key
value, instead of one by one; 3. it provides a \method{load}
method for loading the entire graph into cache (local dictionary);
4. use of the \method{items} method and other ``value iterator'' methods
will automatically perform a load of the entire graph, so that 
only a single database query is used for the entire dataset, 
rather than a separate query for each row or cluster.

As for \class{SQLTable}, getting a list of node IDs using
\method{__iter__} or \method{keys} does not force an automatic load of 
the entire table into memory, but calling \method{items} or
other ``value'' list / iterator methods will.

\begin{funcdesc}{__init__}{table,source_id='source_id',target_id='target_id',edge_id='edge_id',clusterKey=None}
  Similar to the \class{SQLTableBase}, but not exactly the same format.
  \var{table} can either be a string table name, or an actual 
  \class{SQLTableClustered} object.
\end{funcdesc}

\begin{funcdesc}{load}{l=None}
  Load all data from the table, and store in our local cache (a
  Python dictionary).  If \var{l} is not None, it provides a
  list of tuples obtained via the \method{select} method that
  should be added to the cache, instead of loading the entire
  database table.
\end{funcdesc}

\begin{funcdesc}{__contains__}{id}
  Test whether \var{id} exists as a source node in this graph.
\end{funcdesc}

\begin{funcdesc}{__invert__}{}
  Return an \class{SQLGraphClustered} instance representing the reverse 
  directed graph (i.e. swap target nodes for source nodes).
\end{funcdesc}

\subsubsection{TupleO}
Default class for ``row objects'' returned by \class{SQLTable}.
Provide attribute interface to a tuple.  To subclass this,
add an \member{_attrcol} attribute
that maps attribute names to tuple index values (integers).
Constructor takes a single tuple argument representing a
row in the database.

\subsubsection{SQLRow}
Default class for row objects from NoCache variants of SQLTable.
Provides transparent interface to a row in the database: attribute access
will be mapped to SELECT of the appropriate column, but data is not cached
on this object.  Constructor takes two arguments: a database table
object, and an identifier for this row.  Actual data requests will
be relayed by \class{SQLRow} to the database table object.


\subsection{mapping module: graphs and graph query}
\label{graphs-query}

The basic idea of Pygr is that all Python data can be viewed as a graph whose nodes are objects and whose edges are object relations (in Python, references from one object to another).  This has a number of advantages. 

   1. All data in a Python program become a database  that can be queried through simple but general graph query tools.  In many cases the need to write new code for some task can be replaced by a database query. 

   2. Graph databases are more general and flexible in terms of what they can represent and query than relational databases, which is very important for complex bioinformatics data.

   3. Indeed, in Pygr, a query is itself just a graph that can be stored and queried in a database, opening paths to automated query construction.

   4. Pygr graphs are fully indexed, making queries about edge relationships (which are often unacceptably slow in relational databases) fast.

   5. The interface can be very simple and pythonic: it's just a Mapping.  In Python "everything is a dictionary", also known as "the Mapping protocol": a dictionary maps some set of inputs to some set of outputs. e.g. m[a]=b maps a onto b, as a unique relation.  In Pygr, if we want to be able to map a node to multiple target nodes (i.e. allow it to have multiple edges), we simply add another layer of mapping: m[a][b]=edgeInfo (where edgeInfo is optional edge info.)

Examples of the Pygr syntax:

\begin{verbatim}
graph += node1 # ADD node1 TO graph
graph[node1] += node2 # ADD AN EDGE FROM node1 TO node2
graph[node1][node2]=edge_info # ADD AN EDGE WITH ASSOCIATED edge_info
# ADD SCHEMA BINDING WITH graph[node] BOUND AS node.attr
setschema(node,attr,graph) 
# SEARCH graph FOR SUBGRAPH {1->2; 1->3; 2->3}, 
# I.E. EXONSKIP, WHERE THE SPLICE FROM 2 -> 3 HAS ATTRIBUTE type 'U11/U12' 
for m in GraphQuery(graph,{1:{2:None,3:None},\
                   2:{3:dict(filter=lambda edge,**kwargs:edge.type=='U11/U12')},\
                   3:{}}):
    print m[1].id,m[2].id,m[1,2].id
\end{verbatim}

Let's examine these examples one by one:
\begin{itemize}

\item
adding a node to a graph is distinct from creating edges between it and other nodes.  The graph+=node notation simply adds node to the graph, initially with no edges to other nodes.

\item 
A similar syntax (graph[node1]+=node2) can be used to add an edge between two nodes, but with no edge information.  In this case the edge information stored for this relation is simply the Python None value.  Note that in Pygr the default type of graph has directed edges; that is a->b does not imply b->a.  In the default dictGraph graph class, these are two distinct edges that would have to be added separately if you truly want to have an edge going both from a to b and from b to a.

\item 
To add an edge between two nodes with edge information, use the graph[node1][node2]=edge_info syntax. 

\item 
You can bind an object attribute to a graph, using setschema(obj,attr,graph).  This acts like Python's built-in setattr(obj,attr,value), but instead of obj.attr simply storing the specified value, it is bound to the graph so that obj.attr is equivalent to graph[obj].  Both syntaxes are interchangeable and can be mixed in different pieces of code accessing the same object.

\item 
Since Pygr adopts the Mapping protocol as its model for storing graphs, you can create graphs simply by creating Python dict objects e.g. {foo:bar}.  In this example we construct a query graph whose "nodes" are just the integers 1, 2, and 3.  Since any kind of object is a valid key in Python mappings, they can therefore also be used as "nodes" in a Pygr graph.  This query graph illustrates a few simple principles:

\item 
a Pygr graph is just a two-level Python mapping.  For example, {1:{2,None}} is a graph with a single edge from 1 to 2, with no edge information.  Pygr graphs can have multiple edges from or to a given node. 

\item    
edge information in a query graph can be used to specify extra query arguments, again in the form of a Python dictionary.  This dictionary is interpreted as a set of "named arguments" to be used by the GraphQuery search method.  For example, a filter argument is interpreted as a callable function that is passed a set of named arguments describing the current edge / node matching being tested, and whose return value (True or False) will determine whether this edge "matches" our query graph.  In this example, we used it to check whether the edge.type attribute is "U11/U12" (an unusual type of splicing in gene structure graphs).

\item         
Graph query in Pygr simply means finding a subgraph of the datagraph that has node-to-node match to the edge structure given in the query graph.  In this example it is a simple exon-skip structure (3 exons, one of which can either be included or skipped).  The GraphQuery class provides a general mechanism for performing graph queries on any Python data (see below for full details).  It can be used as an iterator that will return all matches to the query (if any). 

\item          
Matches are themselves returned as a mapping of nodes and edges of the query graph (in this example, its nodes are the integers 1, 2 and 3) onto nodes and edges of the data graph.  In this example the match is returned as m, so m[1] is the node in the data graph corresponding to node 1 in the query graph.  This example assumes that object has an id attribute, which is printed out.  To refer to an edge, just use a tuple corresponding to a pair of nodes in the query graph.  In this example, 1,2 refers to the edge from node 1 to node 2 in the query graph, so m[1,2] is the edge in data graph between nodes m[1] and m[2].  This example also attempts to print an id attribute from that edge object.

\item         
Note on current behavior: currently, GraphQuery will throw a KeyError exception if it tries to search for a query node in the query graph and does not find it.  That's why we have to add the "node with no edges" entry 3:{} for node 3.  This will probably be addressed in the future, since this seems like a potential source of many annoying little bugs.

\end{itemize}

\subsubsection{dictGraph}

dictGraph is Pygr's main in-memory graph class.  For persistent
graph storage and query (e.g. stored in a relational database table
or BerkeleyDB file), see the \class{IDGraph} class below.

This class provides all the standard behaviors described above.  The current reference implementation uses standard Python dict objects to store the graph.  All the usual Mapping protocol methods can be used on dictGraph objects (top-level interface, in the examples above graph) and dictEdge objects (second-level interface; in the examples above graph[node]). e.g.

\begin{itemize}

\item
for node in graph: iterator method returns all nodes in the graph; you could also use graph.items() to get node,dictEdge pairs, etc.

\item
for node in graph[node]:  iterator method returns all nodes that are targets of edges originating at node.  Again, you could use graph[node].items() to get node,edgeInfo pairs.  Note: if node is not in graph, this will throw a KeyError exception just like any regular Python dict.

\item
if node in graph:  contains method checks whether node is present in the graph, using dict indexing.

\item
if node2 in graph[node1]:  test whether node1 has an edge to node2.  Again, if node1 isn't in graph, this will throw a KeyError exception.

\end{itemize}

\subsubsection{Directionality and Reverse Traversal}

Note that dictGraph stores directed edges, that is, a->b does not imply b->a; those are two distinct edges that would have to be added separately if you want an edge going both directions.  Moreover, the current implementation of dictGraph does not provide a mechanism for traveling an edge backwards.  To do so with algorithmic efficiency requires storing each edge twice: once in a forward index and once in a reverse index.  Since that doubles the memory requirements for storing a graph, the default dictGraph class does not do this.  If you want such a "forward-backwards" graph, use the dictGraphFB subclass that stores both forwad and reverse indexes, and supports the inverse operator ($\sim$).  $\sim$ graph gets the reverse mapping, e.g. ($\sim$ graph)[node2] corresponds to the set of nodes that have edges to node2.  This area of the code hasn't been tested much yet.

\subsubsection{IDGraph}
This class provides a graph interface that can work with external storage
in an SQL database table or BerkeleyDB file, based on storing node ID and
edgeID values in the external storage.
\begin{funcdesc}{__init__}{proxyDict=None,sourceDB=None,targetDB=None,edgeDB=None,**kwargs}
  \var{proxyDict} must be a graph-style interface that stores the graph
  purely in terms of node ID and edge ID values.  This could be an \class{IntShelve}
  or \class{sqlgraph.SQLGraphClustered} instance, for example.  If None provided,
  the constructor will create an \class{IntShelve} storage for you, passing
  on \var{kwargs} to its constructor.

  \var{sourceDB} must be a database container (dictionary interface) whose
  keys are source node IDs, and whose values are the associated node objects.

  \var{targetDB} must be a database container (dictionary interface) whose
  keys are target node IDs, and whose values are the associated node objects.

  \var{edgeDB} must be a database container (dictionary interface) whose
  keys are edge IDs, and whose values are the associated edge objects.
\end{funcdesc}

\begin{funcdesc}{__iadd__}{node}
  Add \var{node} to the graph, with no edges.  \var{node} must be an
  item of \var{sourceDB}.
\end{funcdesc}

\begin{funcdesc}{__delitem__}{node}
  Delete \var{node} from the graph, and its edges.  \var{node} must be a
  source node in the graph.  \method{__isub__} does exactly the same thing.
\end{funcdesc}

\begin{funcdesc}{__invert__}{}
  Returns an \class{IDGraph} interface to the inverse mapping
  (i.e. where source and target nodes are swapped).
\end{funcdesc}

The object's \member{edges} attribute provides an interface to iterating
over or querying its edge dictionary.

\subsubsection{IntShelve}
Provides an interface to the Python \module{shelve} persistent dictionary
storage, that can accept \class{int} values as keys.
\begin{funcdesc}{__init__}{filename=None,mode='r'}
  Open the specified \module{shelve} BerkeleyDB file, using the specified
  mode.
\end{funcdesc}

\begin{funcdesc}{close}{}
  calls the shelve file's \method{close} method.
\end{funcdesc}
In other respects the \class{IntShelve} behaves like a regular shelve
(dictionary interface).

\subsubsection{Schema: binding object attributes to graphs}

The goal of Pygr is to provide a single consistent model for working with data explicitly modeled as graphs (i.e. dictGraph-like objects) and standard Python objects that were not originally designed to be queried (or thought of) as a "graph".  Since Python uses the Mapping concept throughout the language and object model, and provides introspection, there is no reason why Pygr can't work with both kinds of data transparently.  One mechanism for making this idea explicit is the idea of binding an object attribute to a graph, via the new method we've called setschema(obj,attr,graph).  The idea here is that once you bind an object attribute to a graph, the two different data models obj.attr (object model) or graph[obj] (graph model) are made equivalent and interchangeable.  Operating on one affects the other and vice versa; they are two ways of referring to the same relation.  This concept can be applied at several different levels

\begin{itemize}
\item
individual objects: just like getattr() and setattr(), you can apply schema methods to individual objects: getschema(obj,attr) (returns the bound graph) or setschema(obj,attr,graph) (binds the object attribute to the graph). 

\item
all instances of a class: you can bind specific attributes of a given class to a graph using the following class attribute syntax:

\end{itemize}
\begin{verbatim}
class ExonForm(object): # ADD ATTRIBUTES STORING SCHEMA INFO
    __class_schema__=SchemaDict(((spliceGraph,'next'),(alt5Graph,'alt5'),(alt3Graph,'alt3')))
\end{verbatim}

In this class we bound the next attribute to spliceGraph, alt5 attribute to alt5Graph, and alt3 attribute to alt3Graph.  That means, every instance obj of this class will have an attribute obj.next that is equivalent to spliceGraph[obj], etc.  Note that this is schema, not the actual operation of adding the object as a node to the graph.  Indeed, when obj is first created, it is not automatically added to spliceGraph; that is up to the user.  Unless your code has added the node to the graph (e.g. spliceGraph+=obj), obj.next should throw a KeyError exception.

The general method getschema(obj,attr) works regardless of whether the schema was stored on an individual object or at the class level.

\subsubsection{GraphQuery}

The GraphQuery class implements simple node-to-node matching, in which each new node-set is generated by an iterator associated with a specific node in the query graph.  This iterator model is general: since indexes (mappings) support the iterator protocol, a given iterator may actually be an index lookup (or other clever search algorithm).  The GraphQuery constructor takes two arguments: the default data graph being queried, and the query graph.  The query graph is just a graph; its nodes can be any object that can be a graph node (i.e. any object that is indexible, e.g. by adding a __hash__() method).  Its node objects will not be modified in any way by the GraphQuery.  Its edges are expected to be dictionaries that can be checked for specific keyword arguments:

\begin{itemize}

\item
filter: must be a callable function that accepts keyword arguments and returns True (accept this edge as a match to the queryGraph) or False (do not accept this edge as a match).  This function will be called with the following keyword arguments:
       \begin{itemize}
          \item
          toNode: the target node of this edge, in the data graph
          \item
          fromNode: the origin node of this edge, in the data graph
          \item
           edge: the edge information for this edge in the data graph
          \item 
           queryMatch: a mapping of the query graph to the data graph, based on the partial matchings made so far
           \item
           gqi: the GraphQueryIterator instance associated with this matching operation.  Much more data is available from specific attributes of this object.
	\end{itemize}

\item 
dataGraph: graph in which the current edge should be search for.  This allows a query to traverse multiple graphs.  In other words, when searching for edges from the current node, look up dataGraph[node] instead of defaultGraph[node].

\item
attr: object attribute name to use as the iterator, instead of the defaultGraph.In other words, generate edges from the current node via getattr(node,attr) instead of defaultGraph[node].  The object obtained from this attribute must act like a mapping; specifically, it must provide an items() method that returns zero or more pairs of targetNode,edgeInfo, just like a standard Pygr dictEdge object. 

\item
attrN: object attribute name to use as the iterator, instead of the defaultGraph. In other words, generate edges from the current node via getattr(node,attr) instead of defaultGraph[node].  The object obtained from this attribute must act like a sequence; specifically, it must provide an iterator that returns zero or more targetNode.  The edgeInfo for any edges generated this way will be None.

\item
f: a callable function that must return an iterator producing zero or more pairs of targetNode,edgeInfo.  Typically f is a Python generator function containing a statement like yield targetNode,edgeInfo.

\item
fN: a callable function that must return an iterator producing zero or more targetNode.  Typically fN is a Python generator function containing a statement like yield targetNode.  The edgeInfo for any edges generated this way will be None.

\item 
subqueries: a tuple of query graphs to be performed.  Since GraphQuery traversalcorresponds to logical AND (i.e. all the query graph nodes must be successfully matched to return a match), the subqueries are currently treated as a union (logical OR), by simply returning every match from each subquery as a match (at least for this node).  Each subquery is itself just another query graph.  Moreover, since query graphs can share nodes (i.e. the same object can appear as a node in multiple query graphs), subqueries can make reference to nodes that are already matched by the higher query.  This is an area that has not been explored much yet, but provides a pretty general model for powerful queries.

\end{itemize}
The attr - subqueries options are all implemented as extremely simple subclasses of GraphQuery.  If you want to see just how easy it is to write new subclasses of GraphQuery functionality, look at the graphquery.py module (the entire graph query module is only 237 lines long).

Note: an easy way to pass keyword dictionaries (e.g. as edge information) is simply using the dict() constructor, e.g. dict(dataGraph=myGraph,filter=my_filter).  I think this is a little more readable than {'dataGraph':myGraph, 'filter':my_filter}.

Note on current behavior: currently, the GraphQuery iterator returns the same mapping object for each iteration (simply changing its contents).  So to save these multiple values safely in a list comprehension we have to copy each one into a new dict object via dict(m).

\subsubsection{What is GraphQuery actually doing?}

A GraphQuery is basically an iterator that returns all possible mappings of the query graph onto the datagraph that match all of the nodes and edges of the query graph onto nodes and edges of the data graph.  As an iterator, it does not instantiate a list of the matches, but simply returns the matches one by one.  The current design is very simple.  The GraphQuery constructor builds an "iterator stack" of GraphQueryIterators, each representing one node in the query graph; they are enumerated in order by a breadth-first-search of the query graph.  The GraphQuery iterator processes the stack of GraphQueryIterators: any match simply pushes the stack to the next level; any match at the deepest level of the stack is a complete match (yield the queryMatch mapping); the end of any GraphQueryIterator simply pops the stack.  One obvious idea for improving all this is to replace this "interpreter" with a "compiler" that compiles Python for loops that are equivalent to this stack, and run that... likely to be many fold faster.






\subsection{coordinator Module}
\label{coord-module}

{\em Framework for running subtasks distributed over many computers, in a pythonic way, using SSH for secure process invocation and XMLRPC for message passing. Also provides simple interface for queuing and managing any number of such "batch jobs".}

I will first describe some classes for simple XMLRPC services, then proceed
to the job control classes.

\subsubsection{XMLRPCServerBase}
Base class for creating an XMLRPC server to serve data from multiple objects.
On the server-side, this object can be treated as a dictionary whose
keys are object names, and whose associated values are the server
objects that will serve functionality to XMLRPC clients.
It provides an XMLRPC method \method{methodCall} that takes an object name,
method name, and arguments, and if the call is permitted by its security
rules, calls the designated method on that object.
\begin{funcdesc}{__init__}{name,host=None,port=5000,logRequests=False}
  \var{name} is an arbitrary string identifier for the XMLRPC server.

  \var{host} allows you to override the default hostname to use for this
  server (which defaults to the fully-qualified domain name of this computer).
  Setting it to 'localhost' will typically make the XMLRPC server only accessible
  to processes running on this computer.

  \var{port} specifies the port number on which this server should run.

  \var{logRequests} is passed on to \class{SimpleXMLRPCServer} as
  a flag determining whether it outputs verbose log information.
\end{funcdesc}

\begin{funcdesc}{__setitem__}{name,obj}
  Save \var{obj} as the service called \var{name} in this XMLRPC server.
  \var{obj} must have an \member{xmlrpc_methods} dictionary whose
  keys are the names of its methods that XMLRPC clients are allowed
  to call.
\end{funcdesc}
\begin{funcdesc}{__delitem__}{name}
  Delete the service called \var{name} in this XMLRPC server.
\end{funcdesc}
\begin{funcdesc}{register}{url=None,name='index',server=None}
  Send information describing the services in this XMLRPC server,
  stored by the user on its \member{registrationData} attribute,
  to the resource database server, which can be specified in 
  several ways.  If \var{url} is not None, it will make an XMLRPC
  connection to the resource database server using \var{url} (as the
  URL for the XMLRPC server) and \var{name} (as the name of the server
  object that stores the resource database dictionary).  Otherwise,
  if \var{server} is not None, it is assumed to be a resource database
  object (or XMLRPC connection to such a database) providing a
  \method{registerServer} method that takes two arguments,
  a \var{locationKey} and the registration data.  Otherwise,
  it tries to connect to pygr.Data's default resource database
  by calling its \code{getResource.registerServer} method with the
  same arguments.
\end{funcdesc}
\begin{funcdesc}{serve_forever}{}
  Start the XMLRPC server, after detaching it from
  stdin, stdout and stderr; this call will never exit.
\end{funcdesc}

This XMLRPC server provides several interface methods to
XMLRPC clients contacting it:
\begin{funcdesc}{objectList}{}
  Returns a dictionary of its server objects, whose keys are their
  names, and whose values are in turn dictionaries whose keys are
  their allowed method names.
\end{funcdesc}
\begin{funcdesc}{objectInfo}{objname}
  Returns a dictionary whose keys are the allowed method names for
  the server object named \var{objname}.
\end{funcdesc}
\begin{funcdesc}{methodCall}{objname,methodname,args}
  Calls the designated method on the named server object, with the
  provided \var{args}, and returns its result to the XMLRPC client.
\end{funcdesc}

Example server objects that can be added to a \class{XMLRPCServerBase}
include \class{seqdb.BlastDBXMLRPC}, \class{xnestedlist.NLMSAServer}.


\subsubsection{XMLRPCClient, get_connection}
Client for accessing a \class{XMLRPCServerBase} server.  Provides 
a dictionary interface whose keys are names of available server objects,
and whose associated values are client objects that provide a transparent
interface to the server objects (i.e. calling a method on the client
object returns the value of the result of calling the same named method
on the server object).
\begin{funcdesc}{__init__}{url}
  Makes a connection to the XMLRPC server running on the specified \var{url},
  typically consisting of both a host name and port number.
\end{funcdesc}
\begin{funcdesc}{__getitem__}{name}
  Obtain a client object for the server object specified by \var{name}.
  It will be decorated with the set of methods on the server object
  that are allowed to be accessed by XMLRPC.
\end{funcdesc}

As a convenience, the \module{coordinator} module provides a function
\code{get_connection} that provides an efficient connection to XMLRPC
server objects.  Specifically, it caches past requests, so that multiple
requests for the same server object will re-use the same client object,
and requests for different server objects on the same XMLRPC server will
share the same \class{XMLRPCClient} connection.  It is simply used as follows:
\code{get_connection(url,name)}, where \code{url} is the URL of the XMLRPC
server, and \code{name} is the name of the server object you wish to access.
For example:
\begin{verbatim}
myclient=coordinator.get_connection('http://leelab.mbi.ucla.edu:5000','ucsc17')
\end{verbatim}

\subsubsection{coordinator Module Functionality Overview}

The \module{coordinator} module provides a simple system for running a large collection of tasks on a set of cluster nodes.  It assumes:

\begin{itemize}

\item
authentication is handled using ssh-agent.  The coordinator module does no authentication itself; it simply tries to spawn jobs to remote nodes using ssh, assuming that you have previously authenticated yourself to ssh-agent. 

\item
the client nodes can access your scripts using the same path as on the initiating system.  In other words, if you launch a coordinator job /home/bob/mydir/myscript.py, your client nodes must also be able to access /home/bob/mydir/myscript.py (e.g. via NFS).

\item
your job consists of a large set of task IDs, and a computation to be performed on each ID.  To run this job, you provide an iterator that generates the list of task IDs for the Coordinator to distribute to your client nodes.  You start your script to run a Coordinator that serves your list of task IDs to the client nodes.  You also provide  a function that performs your desired computation on each task ID it receives from the Coordinator.  Typically, you provide both the server function (i.e. the iterator that generates the list of task IDs) and the client function (that runs your desired computation for each ID) within a single Python script file.  Running this script without extra flags starts the Coordinator, which in turn launches your script as a Processor on one or more client nodes.  The Processors andCoordinator work together to complete all the task IDs.

\item
a ResourceController performs load balancing and resource allocation functions, including: dividing up loads from one or more Coordinators over a set of hosts (each with one or more CPUs); serving a Resource database to Processors requesting specific resources; resource-locking on a per node basis for preventing Processors from using a Resource that is under construction by another Processor.  For very large files that are used repeatedly by your computation, it is preferable to first copy them to local disk on each cluster node (fast), rather than reading them over and over again from NFS (slow).  Resources provide a simple mechanism for doing this.

\end{itemize}
To see how to use this, let's look at an example script, mapclusters5.py:

\begin{verbatim}

from pygr.apps.leelabdb import *
from pygr import coordinator

def map_clusters(server,genome_rsrc='hg17',dbname='HUMAN_SPLICE_03',
                 result_table='GENOME_ALIGNMENT.hg17_cluster_JUN03_all',
                 rmOpts='',**kwargs):
    "map clusters one by one"
    # CONSTRUCT RESOURCE FOR US IF NEEDED
    genome=BlastDB(ifile=server.open_resource(genome_rsrc,'r'))
    # LOAD DB SCHEMA
    (clusters,exons,splices,genomic_seq,spliceGraph,alt5Graph,alt3Graph,mrna, \
     protein,clusterExons,clusterSplices)=getSpliceGraphFromDB(spliceCalcs[dbname])

    for cluster_id in server:
        g=genomic_seq[cluster_id]
        m=genome.megablast(g,maxseq=1,minIdentity=98,rmOpts=rmOpts) # MASK, BLAST, READ INTO m
        # SAVE ALIGNMENT m TO DATABASE TABLE test.mytable USING cursor
        createTableFromRepr(m.repr_dict(),result_table,clusters.cursor,
                            {'src_id':'varchar(12)','dest_id':'varchar(12)'})
        yield cluster_id # WE MUST FUNCTION AS GENERATOR

def serve_clusters(dbname='HUMAN_SPLICE_03',
                   source_table='HUMAN_SPLICE_03.genomic_cluster_JUN03',**kwargs):
    "serve up cluster_id one by one"
    cursor=getUserCursor(dbname)
    t=SQLTable(source_table,cursor)
    for id in t:
        yield id

if __name__=='__main__':
    coordinator.start_client_or_server(map_clusters,serve_clusters,['hg17'],__file__)
\end{verbatim}

Let's analyze the script line by line:

\begin{itemize}

\item
mapclusters() is a client generator function to be run in a Processor on a client node.  It takes one argument representing its connection to the server (a Processor object), and optional keyword arguments read from the command line.  It first does some initial setup (opens a BLAST database and loads a schema from a MySQL database), then iterates over task IDs returned to it from the server.  A few key points:

\item
server.open_resource(genome_rsrc,'r') requests a resource given by the genome_rsrc argument from the ResourceController, does whatever is necessary to copy this resource to local disk, and then opens it for reading, returning a file-like object.  This can then be used however you like, but you MUST call its close() method (just as you should always do for any file object) to indicate that you're done using it.  Failure to close() the file object will leave the Resource "hg17" permanently locked on this specific node.  (You would then have to unlock it by hand using the ResourceController.release_rule() method).

\item
yield cluster_id: the client function must be a Python generator function (i.e. it must use the yield statement), and it must yield the list of IDs that it has processed.  Python's generator construct is extremely convenient for many purposes: here it lets us perform both our initializations and iteration over IDs within a single function, while at the same time wrapping each iteration within the Processor's error trapping code (to prevent a single error in your code from causing the entire Processor to shut down).  The Processor will trap any errors in your code and and send tracebacks to your Coordinator, which will report them in its logfile.  The Processor will tolerate occasional errors and continue processing more IDs.  However, if more than a certain number of IDs in a row fail with errors (controlled by the Processor.max_errors_in_a_row attribute), the Processor will exit, on the assumption that either your code or this specific client node don't work correctly.

\item
serve_clusters() is the server generating function to be run in the Coordinator.  It returns an iterator that generates all the task IDs that we want to run.  Again, the Python generator construct provides a very clean way of doing this: we simply yield each ID that we want to process in our client Processors.

\item
if __name__=="__main__": this final clause automatically launches our script as either a Coordinator or Processor depending on the command line options (which are automatically parsed by start_client_or_server()).  All we have to do is pass the client generator function, the server generator function, a list of the resources this job will use, and the name of the script file to be run on client nodes.  Since that is just this script itself, we use the Python builtin symbol __file__ (which just evaluates to the name of the current script).

\item     
Command-line arguments are parsed (GNU-style, ie. --foo=bar) by start_client_or_server() and passed to your client and server functions as Python named parameters.  Because the same list of arguments is passed to your client and server functions, and each of these functions won't necessarily want to get all the named arguments, you should include the **kwargs at the end of the argument list.  Any unmatched arguments will be stored in kwargs as a Python mapping (dictionary).  If you fail to do this, your client or server function will crash if called with any named parameters other than the ones it expects.
\end{itemize}

\subsubsection{Log and Error Information}

Process logging and error information go to three different types of logs:

\begin{itemize}

\item
Processor logfile(s): every individual Processor (and all subprocesses run by it) send stdout and stderr to a logfile on local disk of the host on which it is running.  Currently the filename is /usr/tmp/NAME_N.log, where NAME is the name you assigned to the job when you started the Coordinator, and N is the numeric ID of the Processor assigned by the coordinator (just an auto-increment integer beginning at 0, and increasing by one for each Processor the Coordinator starts).  This logfile is the place to look if your job is failing mysteriously--look in the logfile and see its last words before its demise.  You can get a complete list of the logfiles for all the Coordinator's Processors by inspecting the logfile attribute of the CoordinatorMonitor (see below).

\item
Coordinator logfile: all XMLRPC requests from client Processors, as well as error messages from them, are logged here.  All Python errors (tracebacks) in your client (Processor) code are reported here.  Also, the actual SSH commands used to invoke your Processors on cluster nodes, are logged here.  This is usually the place to start, to see whether things are going well (you should see a long stream of next requests as Processors finish a task and request the next one), or failing with errors.

\item
ResourceController logfile: all XMLRPC requests from Processors and Coordinatorsare logged here, including register() and unregister(), resource requests, and load reporting from cluster nodes.  If things are working well, you should see a stream of regular report_load() messages showing steady, full utilization of all the host processors.  Excessive register/unregister churning (jobs that start and immediately exit) is a common sign of trouble with your jobs.

\end{itemize}
\subsubsection{Coordinator}

To start a job coordinator (which in turn will the run the whole job by starting Processors on cluster nodes using SSH):

\begin{verbatim}
python mapclusters5.py mm5_jan02 --errlog=/usr/tmp/leec/mm5_jan02.log \ 
  --dbname=MOUSE_SPLICE --source_table=genomic_cluster_jan02 \
  --genome_rsrc=mm5 --result_table=GENOME_ALIGNMENT.mm5_cluster_jan02_all \ 
  --rmOpts=-rodent \
\end{verbatim}

Here we have told the Coordinator to name itself "mm5_jan02" in all its communications with the ResourceController.  Since we gave no command-line flags, the Coordinator will assume that a ResourceController is already running on port 5000 of the current host.    You must have an ssh-agent running BEFORE you start the Coordinator, since the Coordinator will attempt to spawn jobs using SSH.  The Coordinator will exit with an error message if it is unable to connect to ssh-agent.  A few notes:

\begin{itemize}

\item
The Coordinator will run as a demon process (i.e. in the background, and detached from your terminal session), and redirect its  output into a file (here, given by the --errlog option). If you don't specify an --errlog filename, it will create a filename determined by the name we told it to run as, in this case "mm_jan02.log".  

\item
You must ensure that SSH can launch processes on your client nodes "unattended" i.e. without a connection to a controlling terminal.  If SSH has to ask for userconfirmations when connecting to a given host (e.g. if it asks whether you want to accept the host key), the Coordinator will not be able to use that host.

\item
Python errors (tracebacks) in your will be GNU-style command-line options (e.g. --port=8889) are automatically parsed by start_client_or_server() and passed to the Coordinator.__init__() as keyword arguments.  This constructor takes the following optional arguments: 
    \begin{itemize}
    \item
    port: the port number on which this Coordinator should run
    
    \item
    priority: a floating point number specifying the priority level at which this Coordinator should be run by the ResourceController.  The default value is 1.0.  A value of 2.0 will give it twice as many Processors as a competing Coordinator of priority 1.0.

    \item
    rc_url: the URL for the ResourceController.  Defaults to http://THISHOST:5000
    \item
    errlog: logfile path for saving all output to.  Defaults to NAME.log, where NAME is the name you assigned to this Coordinator. Can be an absolute path.

    \item
    immediate: if True, make the job run immediately, without waiting for previous jobs to finish.  Default: False.
    
    \item
    demand_ncpu: if set to a non-zero value, specifies the exact number of Processors you want to run your job.

    \item
    NB: command line arguments are also passed to your server function, and to your client function, as Python named parameters.  See the mapclusters5.py example above.
    \end{itemize}
\end{itemize}

\subsubsection{ResourceController}

Whereas you start a separate Coordinator for each set of jobs you want to run, you only need a single ResourceController running. To start the ResourceController, run:

\begin{verbatim}
python coordinator.py --rc=bigcheese
\end{verbatim}

This starts the ResourceController (running as a demon process in the background) and names it "bigcheese"; a name argument (given by the --rc flag) is REQUIRED.  Since you didn't specify command-line flags, it will run on the default port 5000.  It will use several files based on the name you gave it:
\begin{itemize}
 
\item
bigcheese.hosts: a list of cluster nodes and associated maximum load (separated by whitespace, one pair per line).  It will attempt to fill these nodes with jobs, up to the maximum load level specified for each, sharing the load between whatever set of Coordinators contact it.

\item
bigcheese.log: all output from the ResourceController (showing requests made to it by Coordinators and Processors) is logged to this file.

\item
bigcheese.rules: this file is a Python shelve created by the ResourceController as its rules database.

\item
bigcheese.rsrc: this file is a Python shelve created by the ResourceController as its resource database.GNU-style command-line options (e.g. --port=5001) are automatically parsed by start_client_or_server() and passed to the ResourceController.__init__() as keyword arguments.  This constructor takes the following optional arguments:

\item
port: the port number on which this ResourceController should run

\item
overload_margin: how much "extra" load above the standard level is allowable.  This prevents temporary load spikes from causing Processors to exit.  Set by default to 0.6.  I.e. if the maxload for a host was set to 2.0, any load above 2.6 would cause the ResourceController to start shutting down Processor(s) on that host.

\item
rebalance_frequency: the time interval, in seconds, for rerunning the ResourceController.load_balance() method.  Defaults to 1200 sec.

\item
errlog: logfile path for saving all output to.  Defaults to NAME.log, where NAME is the name you assigned to this ResourceController. Can be an absolute path.

\end{itemize}

\subsubsection{RCMonitor}

The coordinator module also provides a convenience interface for interrogating and controlling jobs.  In an interactive Python shell, import the coordinator module, and create an RCMonitor object::

\begin{verbatim}
from pygr import coordinator
m=coordinator.RCMonitor()
\end{verbatim}

Since you did not specify any arguments, it will default to searching for the ResourceController on the current host, port 5000.  You can specify a host and or port as additional arguments.  It also loads an index of coordinators currently registered with this ResourceController, accessible on its coordinators attribute:

\begin{verbatim}
for name,c in m.coordinators.items():
  print name,len(c.client_report)
\end{verbatim}

will print a list of the coordinators and how many Processors each is currently running.  Each coordinator is represented by a CoordinatorMonitor object in this coordinators index.

Both RCMonitor and CoordinatorMonitor objects give you access to the XMLRPC methods of the ResourceController and Coordinators they represent.  That is, running a method on the RCMonitor actually runs the identically-named method on the ResourceController.  Some of the most useful ResourceController methods are:

\begin{itemize}

\item
report_load(host,pid,load): inform RC that the current load on host is load.

\item
load_balance(): make the RC rebalance load, using all available nodes and coordinators

\item
setrule(rsrc,rule): set a production rule for the resource named rsrc.  rule must be a tuple consisting of the local filepath to be used for the resource, and a shell command that will construct it, with a %s where you want the filename to be filled in.

\item
delrule(rsrc): deletes the rule for rsrc from the rules database.

\item
set_hostinfo(host,attr,val) set an attribute for host.  For example, to set the maximum load for this host: rcm.set_hostinfo(host,'maxload',2.0).  This should usually be the number of CPUs on this host.  NB: these settings will apply only to the current ResourceController, and are not saved back to its NAME.hosts file.  If you want to make these settings permanent (i.e. to apply to ResourceControllers you start anew in the future), then edit the NAME.hosts file.

\item
retry_unused_hosts(): make the RC search its hosts database for hosts that are not currently in use (e.g. jobs may have died) and try to reallocate them to the existing coordinators.

\end{itemize}
Both RCMonitor and CoordinatorMonitor objects have a get_status() method that updates them with the latest information from their associated ResourceController or Coordinator.

Here are some typical monitor usages:

\begin{verbatim}
c=m.coordinators['mapclusters3'] # GET MY COORDINATOR
c.client_report.sort() # MAKE IT SORT CLIENTS BY HOSTNAME
c.client_report # PRINT THE SORTED LIST, SHOWING HOST, PID, #TASKS DONE
c.pending_report # PRINT LIST OF TASK IDS CURRENTLY RUNNING
c.nsuccess # PRINT TOTAL #TASKS DONE
c.nerrors  # PRINT TOTAL #TASKS FAILED
c.logfile # PRINT LIST OF ALL PROCESSOR LOGFILES

m.rules # PRINT THE CURRENT RULES DATABASE
m.resources # PRINT THE CURRENT RESOURCES DATABASE
m.setrule('hg17',
('/usr/tmp/ucsc_msa/hg17',
'gunzip -c /data/yxing/databases/ucsc_msa/human_assembly_HG17/*.fa.gz
>%s'))
m.get_status() # UPDATE OUR RC INFO
m.set_hostinfo('llc22','maxload',2.0) # ADD A NEW HOST TO OUR DATABASE
m.setload('llc1','maxload',0.0) # STOP USING llc1 FOR THE MOMENT
m.load_balance() # MAKE IT ALLOCATE ANY FREE CPUS NOW...
m.locks # SHOW LIST OF RESOURCES CURRENTLY LOCKED, UNDER CONSTRUCTION
\end{verbatim}

\subsubsection{Security}

Internal communication between Processors, Coordinators and ResourceController is performed using XMLRPC and thus is not secure. However, since no authentication information or actual commands are transmitted by XMLRPC, and the coordinator module does not enable the processes that use it to do anything that they are not ALREADY capable of doing on their own (i.e. spawn ssh processes), the main security vulnerabilty is Denial Of Service (i.e. an attacker listening to the XMLRPC traffic could send messages causing Processors to shutdown, or Coordinators to be blocked from running any Processors).  In other words the security philosophy of this module is to avoid compromising your security, by leaving the security of process invocation entirely to your existing security mechanisms (i.e. ssh and ssh-agent).  Commands are only sent using SSH, not XMLRPC, and the XMLRPC components are designed to prevent known ways that an XMLRPC caller might be able to run a command on an XMLRPC server or client. (I blocked known security vulnerabilities in Python's SimpleXMLRPCServer module).

In the same spirit, the current implementation does not seek to block users from issuing commands that could let them "hog" resources, for the simple reason that in an SSH-enabled environment, they would be able to do so regardless of this module's policy.  I.e. the user can simply not use this module, and spawn lots of processes directly using SSH.  In the current implementation, every user can send directives to the ResourceController that affect resource allocation to other users' jobs.  This means everybody has to "play nice", only giving their Coordinator(s) higher priority if it is really appropriate and agreed by other users.  Unless a different process invocation mechanism (other than SSH by each user) were adopted, it doesn't really make sense to me to try to enforce a policy that is stricter than the policy of the underlying process invocation mechanism (i.e. SSH).  Since every user can use SSH to spawn as many jobs as they want, without regard for sharing with others, making this module's policy "strict" doesn't really secure anything.

\end{document}
