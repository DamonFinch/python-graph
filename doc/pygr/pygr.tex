\documentclass{howto}
\usepackage{distutils}

% TODO:
%   Fill in XXX comments

\title{Pygr: Docs Overview}


\input{boilerplate}

\author{Chris Lee}
\authoraddress{
\strong{UCLA, Department of Chemistry and Biochemistry}\\
	Email: \email{leec@chem.ucla.edu}
}

\makeindex

\begin{document}

\maketitle

\begin{abstract}
  \noindent
   A roadmap to the Pygr documentation.
\end{abstract}

%\begin{abstract}
%\noindent
%Abstract this!
%\end{abstract}


% The ugly "%begin{latexonly}" pseudo-environment supresses the table
% of contents for HTML generation.
%
%begin{latexonly}
\tableofcontents
%end{latexonly}


\section{Introduction}
\label{intro}

Pygr is an open source software project used to develop graph database
interfaces for the popular Python language, with a strong emphasis
on bioinformatics applications ranging from genome-wide analysis of
alternative splicing patterns, to comparative genomics queries of
multi-genome alignment data.

The following introductory examples show how to use Pygr for graph queries, sequence searching and alignment queries, annotation queries, and multigenome alignment queries.


\subsection{Example: Simple graph query}
Why would you want to use Pygr?  Interesting data often consists of specific graph structures, and these relationships are much easier to describe as graphs than they are in SQL.  For example, the simplest and most common form of alternative splicing is exon-skipping, where an exon is either skipped or included (see slide 15 of the ISMB slides for a picture).  This can be defined immediately as a graph in which three nodes (exons 1, 2, 3) are joined by edges either as 1-2-3 or 1-3.  Unfortunately, writing an SQL query for this simple pattern requires a 6-way JOIN (argh).

\begin{verbatim}
SELECT * FROM exons t1, exons t2, exons t3, splices t4, splices t5, splices t6 
WHERE t1.cluster_id=t4.cluster_id AND t1.gen_end=t4.gen_start 
  AND t4.cluster_id=t2.cluster_id AND t4.gen_end=t2.gen_start 
  AND t2.cluster_id=t5.cluster_id AND t2.gen_end=t5.gen_start 
  AND t5.cluster_id=t3.cluster_id AND t5.gen_end=t3.gen_start 
  AND t1.cluster_id=t6.cluster_id AND t1.gen_end=t6.gen_start 
  AND t6.cluster_id=t3.cluster_id AND t6.gen_end=t3.gen_start;
\end{verbatim}

Such a six-way JOIN is painfully slow in a relational database; in general such queries just aren't practical.  More fundamentally, the relational schema is forced to represent the graph relation with combinations of foreign keys and other data that the user really should not have to remember.  All the user should know is that there is a specific relation, e.g. from this exon, the "next" exon is X, and the relation joining them is splice Y.

In Pygr, writing the query is just a matter of writing down the graph (edges from 1 to 2, 1 to 3, and 2 to 3, but no special "edge information"):

\begin{verbatim}
queryGraph={1:{2:None,3:None},2:{3:None},3:{}}
\end{verbatim}

We can now execute the query using the GraphQuery class:

\begin{verbatim}
results=[dict(m) for m in GraphQuery(spliceGraph,queryGraph)]
\end{verbatim}

This is more or less equivalent to writing a bunch of for-loops for iterating over the possible closures:

\begin{verbatim}
results=[]
for e1 in spliceGraph: # FIND ALL EXONS
    for e2 in spliceGraph[e1]: # NEXT EXON
        for e3 in spliceGraph[e2]: # NEXT EXON
            if e3 in spliceGraph[e1]: # MAKE SURE SPLICE FROM e1 -> e3
                results.append({1:e1,2:e2,3:e3}) # OK, SAVE MATCH
\end{verbatim}

It is often convenient to bind an object attribute to a graph, so that you can use either the graph syntax or a traditional object attribute and mean exactly the same thing.  In the splice graph example, we bind the exon.next attribute to the spliceGraph, so the above for-loops can also be written:

\begin{verbatim}
results=[]
for e1 in spliceGraph: # FIND ALL EXONS
    for e2 in e1.next: # NEXT EXON
        for e3 in e2.next: # NEXT EXON
            if e3 in e1.next: # MAKE SURE SPLICE FROM e1 -> e3
                results.append({1:e1,2:e2,3:e3}) # OK, SAVE MATCH
\end{verbatim}

Another interesting query in the alternative splicing field is the so-called U12-adapter exon query (see slide 21 of the ISMB slides):

\begin{verbatim}
queryGraph={0:{1:dict(dataGraph=alt5Graph),
               2:dict(filter=lambda edge,**kwargs:edge.type=='U11/U12')},
            1:{3:None},
            2:{3:None},
            3:{}}
\end{verbatim}

Here we use edge information in the query graph to add a few constraints:

\begin{itemize}
\item
the dataGraph argument tells the query to search for exon 1 from exon 0 using a different graph (alt5Graph).

\item    
the filter argument provides a function that returns True only if the edge between exon 0 and exon 2 is of type U11/U12.  Therefore the query will only match sp
lice graphs that have a U12 splice between this pair of exons.

\end{itemize}

Note that the query graph "nodes" (in this example, the integers 0, 1, 2, 3) are
quite arbitrary.  We could have used strings, or other kinds of objects instead.

Now if we want to see the results right away, we use the mapping returned by GraphQuery to look at individual nodes and edges of the dataGraph that matched our query:

\begin{verbatim}
for m in GraphQuery(spliceGraph,queryGraph):
    print m[1].id,m[0,2].id # PRINT EXON ID FOR EXON 1,
                            # SPLICE ID FOR SPLICE 0 -> 2
\end{verbatim}

The match is returned by GraphQuery as a mapping from nodes and edges of the query graph to nodes and edges of the data graph.  Edges are specified simply as tuples of the nodes you want to get the edge for (in this example 0,2).
Constructing a Graph
How was the spliceGraph created in the first place?  Let's say we have an initial list of tuples giving connections between exon objects and splice objects, where each tuple consists of a pair of exons connected by a splice.

\begin{verbatim}
for exon1,exon2,splice in spliceConnections: 
    spliceGraph+=exon1 # add exon1 as a node in the graph
    spliceGraph+=exon2 # if already a node in the graph, does nothing...
    exon1.next[exon2]=splice # add an edge, with splice as the edgeinfo
\end{verbatim}

The last operation makes use of the binding of exon.next to spliceGraph, and is equivalent to

\begin{verbatim}
spliceGraph[exon1][exon2]=splice
\end{verbatim}

If we didn't want to save the edge information, we could use the simpler syntax

\begin{verbatim}
spliceGraph[exon1]+=exon2 # equivalent to exon1.next+=exon2
\end{verbatim}

This "short" form is equivalent to saving None as the edge information.


\subsection{Sequence / Alignment Tutorial}
\label{seq-align}

Sequences and alignments also can be modeled as graph structures in Pygr, providing the same consistent and simple framework for queries.

\subsubsection{Sequence Objects}
Pygr tries to provide a very "Pythonic" model for sequences.  This python interpreter session illustrates some simple features:

\begin{verbatim}
>>> from pygr.seqdb import *
>>> s=NamedSequence('attatatgccactat','bobo') #create a sequence named bobo
>>> s # interpreter will print repr(s)
bobo[0:15]
>>> t=s[-8:] #python slice gives last 8 nt of s
>>> t # interpreter will print repr(t)
bobo[7:15]
>>> str(t) #string conversion just yields the sequence as a string 
'gccactat'
>>> rc= -s #get the reverse complement
>>> str(rc[:5]) #its first five letters
'atagt'
\end{verbatim}

Several points:
\begin{itemize}

\item
   Slices of a sequence object (e.g. s[1:10] or s[-8:]) are themselves sequence objects.

\item    
The string value of a sequence object (e.g. str(s)) is just the sequence itself (as a string).

\end{itemize}

\subsubsection{Working with Sequences from Databases}

Pygr provides a variety of "back-end" implementations of sequence objects, ranging from sequences stored in a relational database table, or a BLAST database, to sequences created by the user in Python (as above).  All of these provide the same consistent interface, and in general try to be efficient.  For example, Pygr sequence objects are just "placeholders" that record what sequence interval you're working with, but if the back-end is an external database, the sequence object itself does not store the sequence, and creating new sequence objects (e.g. taking slices of the object as above) will not require anything to be done on the actual sequence itself (such as copying a portion of it).  Pygr only obtains sequence information when you actually ask for it (e.g. by taking the string value str(s) of a sequence object), and normally only obtains just the portion that you ask for (i.e. str(s[1000000:1000100]) only obtains 100nt of sequence, even if s is a 100 megabase sequence.  By contrast str(s)[1000000:1000100] would force it to obtain the whole sequence from the database, then slice out just the 100 nt you selected). 

Here's an example of working with sequences from a relational database:

\begin{verbatim}
>>> import MySQLdb # standard module for accessing MySQL, now get a cursor...
>>> rdb=MySQLdb.connect(db='HUMAN_SPLICE_03',read_default_file=os.environ['HOME'
]+'/.my.cnf')
>>> t=SQLTable('genomic_cluster_JUN03',rdb.cursor()) #interface to a table of
 sequences
>>> from pygr.seqdb import *   # pygr module for working with sequences from databases
>>> t.objclass(DNASQLSequence) #use this class as "row objects"
>>> s2=t['Hs.1162'] # get a specific sequence object by ID
>>> str(s2[1000:1050]) # this will only get 50 nt of the genomic sequence from 
MySQL
'acctgggtgatgaaataaatttttacgccaaatcccgatgacacacaatt'
\end{verbatim}

(Note: in this example we used MySQLdb.connect()'s ability to read database 
server and user authentication information directly from the standard ~/.my.cnf file normally used by the MySQL client).

Here's an example of working with sequences from a BLAST database:

\begin{verbatim}
>>> from pygr.seqdb import *
>>> db=BlastDB('sp') # open BLAST database associated with FASTA file 'sp'
>>> s=db['CYGB_HUMAN'][90:150] # get a sequence by ID, and take a slice
>>> str(s)
'TVVENLHDPDKVSSVLALVGKAHALKHKVEPVYFKILSGVILEVVAEEFASDFPPETQRA'
>>> m=db.blast(s) # get alignment to all BLAST hits in db
>>> for e in m.edges(): # print out the alignment edges
...     print e.srcPath,repr(e.srcPath),'\n',e.destPath,repr(e.destPath),e.blast
_score,'\n'
... 
TVVENLHDPDKVSSVLALVGKAHALKHKVEPVYFKILSGVILEVVAEEFASDFPP CYGB_HUMAN[90:145] 
TLVENLRDADKLNTIFNQMGKSHALRHKVDPVYFKILAGVILEVLVEAFPQCFSP CYGB_BRARE[87:142] 72

TVVENLHDPDKVSSVLALVGKAHALKHKVEPVYFKILSGVILEVVAEEFASDFPPETQRA CYGB_HUMAN[90:150] 
TVVENLHDPDKVSSVLALVGKAHALKHKVEPVYFKILSGVILEVVAEEFASDFPPETQRA CYGB_HUMAN[90:150]
 120

TVVENLHDPDKVSSVLALVGKAHALKHKVEPVYFKILSGVILEVVAEEFASDFPPETQRA CYGB_HUMAN[90:150] 
TVVENLHDPDKVSSVLALVGKAHALKHKVEPMYFKILSGVILEVIAEEFANDFPVETQKA CYGB_MOUSE[90:150]
112 
...
\end{verbatim}

This example introduces the use of a Pygr alignment object to store the mapping of s onto homologous sequences in db, obtained from BLAST.  Here's what Pygr actually does:

\begin{itemize}

\item
When you first create the BlastDB object, it looks for existing BLAST database files associated with the FASTA file 'sp'.  If present, it uses them.  If not, it creates them automatically, using the NCBI program formatdb (it figures out whether the sequences are nucleotide or protein, and gives formatdb the appropriate command line options).  It then returns a BlastDB object, which is just a placeholder that acts as a convenient interface to the BLAST database.  For example, it acts as a Python dictionary mapping sequence IDs to the associated sequence objects (i.e. if 'CYGB_HUMAN' is a sequence ID in sp, then db['CYGB_HUMAN'] is the sequence object for that sequence.  This object is instantiated from class BlastSequence, which provides an interface to the BLAST database.

\item 
When you work with such sequence objects, slicing etc. happens in the usual way, creating new sequence objects.

\item  
Only when you ask for actual sequence (by taking str(s)) does it obtain a sequence string from the database, using NCBI's fastacmd.  This is done using fastacmd's -L option to obtain just the selected slice.  So you can efficiently obtain a substring of a sequence, even if that sequence is an entire chromosome.
    * When you invoke the db.blast() method on a sequence object, it obtains the actual string of the object, and uses it to run a BLAST search.  It determines the type (nucleotide or protein) of the sequence object, and uses the appropriate search method (in this case blastp).  You can pass optional arguments for controlling BLAST.  It then reads the results into a Pygr multiple sequence alignment object, which stores the alignments as sets of matched intervals.  Specifically, it is a graph, whose nodes are sequence intervals (i.e. sequence objects that typically represent only part of a sequence), and whose edges represent an alignment between a pair of intervals.  To illustrate this, we ran a for-loop over all the "edge relations" in this graph, and printed them out.  This interface is being rewritten to make it more general, but already is fairly simple: e.srcPath and e.destPath are the two aligned sequence intervals; other edge attributes such as e.blast_score or e.percent_id give you access to the BLAST info for that alignment.  Note: print converts its arguments to strings (i.e. calls str() on them), so we used repr(e.srcPath) to get a "string representation" of each sequence interval.  When print calls str() on individualBlastSequence interval objects returned by the BLAST search, they invoke fastacmd -L to obtain the specific sequence slice representing that interval.
\end{itemize}

\subsubsection{Alignment Graph Basics}

Pygr multiple alignment objects can be treated as mappings of sequence intervals onto sequence intervals.  Here is a very simple example (continuing from the previous example), showing basic operations for constructing an alignment:

\begin{verbatim}
>>> m2=PathMapping() # create an empty alignment object
>>> m2[s]=db['MYG_CHICK'][83:143] # add an edge mapping interval s -> an interva
l of MYG_CHICK
>>> m2[s[:10] ] # get alignment for first 10 letters, and print representation
MYG_CHICK[83:93]
>>> m2[s]=db['MYG_CANFA'][45:105] # add an additional edge
>>> for s2 in m2[s[:10] ]: # get aligned seqs for the first 10 letters again...
...     print repr(s2)
...
MYG_CHICK[83:93]
MYG_CANFA[45:55]
\end{verbatim}
In this case we created an alignment of sequence intervals without edge information.  However, we can also store edge information with a given alignment edge, in the "usual" Pygr way:

\begin{verbatim}
>>> class Foo(object): pass # create a dummy class that we can add attributes to
...
>>> a=Foo() # create a dummy object
>>> a.x=7 # add an attribute
>>> m2=PathMapping() # create an empty alignment object
>>> m2+=s # add as node to the alignment graph
>>> m2[s][db['MYG_CHICK'][83:143] ]=a # add an edge mapping interval s -> an int
erval of MYG_CHICK
>>> for e in m2[s[:10] ].edges(): # get alignment edges for the first 10 letters
 again...
...     print repr(e.destPath),e.x # we can access edge attributes
...
MYG_CHICK[83:93] 7
\end{verbatim}

\subsubsection{Alignment Query}

Since Pygr alignments follow the same interface as any Pygr graph, we can query them using the standard GraphQuery class.  Let's say we have a Python script load_alignments.py that loads two alignments:

\begin{itemize}

\item
mRNA_swiss: an alignment of mRNA sequences to homologous SwissProt sequences;

\item
swiss_features: an alignment of SwissProt sequences onto annotation objects. 

\end{itemize}
To find out how the known SwissProt annotations map on to our mRNA sequences requires a join, which can be formulated as a simple Pygr graph, consisting of a mapping of an mRNA sequence interval (node 1), onto a SwissProt sequence interval (node 2), onto a feature annotation (node 3):

\begin{verbatim}
>>> from load_alignments import * # load the alignments
>>> from pygr.graphquery import *      # import the graph query code
>>> queryGraph={1:{2:None},2:{3:dict(dataGraph=swiss_features)},3:{}} # 
edge 2->3 must come from swiss_features
>>> l=[dict(d) for d in GraphQuery(mRNA_swiss,queryGraph)] # run the query and 
save the mappings
>>> len(l) # how many annotations mapped onto our mRNA sequences?
4703
\end{verbatim}

We assumed that mRNA_swiss would be passed as the default dataGraph, and specified directly that edge 2->3 should be looked up in swiss_features.  We then captured all the results from the GraphQuery iterator using a Python list comprehension.  Note that since the iterator returns each result in the same container (mapping object), if we want to save all the individual results we have to copy each one to a new mapping (dict) object, as illustrated in this example.
Storing Alignments in a Relational Database

Pygr makes it easy to store alignments persistently in a relational database.  To illustrate, here we run BLAST to generate an alignment of one sequence to manyother sequences, and store it to a MySQL table test.alignment:

\begin{verbatim}
from pygr.seqdb import *
db=BlastDB('sp') # open BLAST database associated with FASTA file 'sp'
m=db.blast(db['CYGB_HUMAN']) # get alignment to all BLAST hits in db
import MySQLdb # import the MySQL adapter
import os
# open a connection to the test database
cursor=MySQLdb.Connection(db='test',read_default_file=os.environ['HOME']+'/.my.cnf').cursor()
# save the alignment, creating table test.alignment if not exists
createTableFromRepr(m.repr_dict(),'test.alignment',cursor,
                    {'src_id':'varchar(12)','dest_id':'varchar(12)'})
\end{verbatim}

The only line here that needs comment is Pygr's createTableFromRepr() function.  It takes four arguments:

   1. an iterator, each of whose return values is a dictionary mapping column names to value representations that can be stored in a relational table.  Here we have passed m.repr_dict() as the iterator.  It is a generator function that simply yields edge.repr_dict() on each of the edges stored in m.  The edge.repr_dict() method in turn simply returns a dictionary mapping column names (like 'src_id') onto values that are storable in a relational table.
   2. a table name: in this case, 'test.alignment'
   3. a cursor for connecting to the database
   4. an optional dictionary for customizing exactly how specific columns should be stored in the database.  In this case, we specified varchar(12) for both src_id and dest_id (the identifiers of the source and destination (aligned) sequences)

This function constructs a schema and creates the table in the database if it does not already exist.  It then saves all of the output of m.repr_dict() as rows in the database table.  To see exactly what it generates, we can look at its output interactively:

\begin{verbatim}
>>> for e in m.repr_dict(): break
... 
>>> e
{'src_id': 'CYGB_HUMAN', 'blast_score': 344, 'dest_id': 'CYGB_HUMAN', 'src_end':
 190, 'src_ori': 1, 'percent_id': 90, 'dest_ori': 1, 'src_start': 0, 'dest_start' : 0, e_value': 94.301000000000002, 'dest_end': 190}

Here is the schema automatically created by createTableFromRepr():

mysql> desc alignment;
+-------------+-------------+------+-----+---------+-------+
| Field       | Type        | Null | Key | Default | Extra |
+-------------+-------------+------+-----+---------+-------+
| src_id      | varchar(12) |      | MUL |         |       |
| blast_score | int(11)     |      |     | 0       |       |
| dest_id     | varchar(12) |      | MUL |         |       |
| src_end     | int(11)     |      |     | 0       |       |
| src_ori     | int(11)     |      |     | 0       |       |
| percent_id  | int(11)     |      | MUL | 0       |       |
| dest_ori    | int(11)     |      |     | 0       |       |
| src_start   | int(11)     |      |     | 0       |       |
| dest_start  | int(11)     |      |     | 0       |       |
| e_value     | float       |      |     | 0       |       |
| dest_end    | int(11)     |      |     | 0       |       |
+-------------+-------------+------+-----+---------+-------+
11 rows in set (0.01 sec)
\end{verbatim}

Thus the basic format of stored alignments is

\begin{itemize}

\item
only alignment information is stored; no actual sequence is stored, only the sequence identifiers and begin-end points of each aligned interval.

\item
additional edge information for each alignment row is saved, as provided by the edges repr_dict() method.

\item
directional: it distinguishes one of the sequence pair as the source sequence, the other as the destination sequence.

\end{itemize}

We can now make use of the stored alignment in Pygr just as if it were a normal (in-memory) alignment object.  The only difference is we use the StoredPathMapping class instead of the standard PathMapping class.  Here is an example script that connects to the stored alignment, and prints out the alignment of one sequence:

\begin{verbatim}
import MySQLdb
from pygr.seqdb import *
cursor=MySQLdb.Connection(db='test',read_default_file=os.environ['HOME']+'/.my.cnf').cursor()
t=SQLTableMultiNoCache('test.alignment',cursor)
t._distinct_key='src_id' # use this column as the ID field to search
sp=BlastDB('sp') # OPEN SWISSPROT BLAST DB
m=StoredPathMapping(t,sp,sp) # tell it to search sp for both source and target 
sequence IDs
for e in m[sp['CYGB_HUMAN'] ].edges(): # SHOW ALL ALIGNMENTS FOR THIS SEQUENCE
    print repr(e.srcPath),repr(e.destPath),e.blast_score
\end{verbatim}

StoredPathMapping expects three arguments:

   1. an SQLTable object which provides the interface to the database table containing the alignment.  t is just an interface to the MySQL table.  Since each sequence can potentially have many rows (many alignment intervals against one or more destination sequences), we used the SQLTableMultiNoCache subclass: each key value (source sequence ID) can have multiple rows; and we chose not to have it cache rows that it retrieves (there would be no benefit from doing so, since StoredPathMapping itself performs result caching).  Also note the t._distinct_key assignment: this informs the table object that when we try to access a particular key (e.g. t['CYGB_HUMAN']), the column to search for this key is 'src_id'.
   2. a sequence dictionary object from which to obtain the source sequences.  This dictionary must map a source sequence identifier to an actual sequence object representing that sequence.  In this case, we just used our sp BLAST database as the source sequence dictionary.
   3. a sequence dictionary object from which to obtain the destination sequences.  This dictionary must map a destination sequence identifier to an actual sequence object representing that sequence.  We just used the sp database again.

Thus we informed StoredPathMapping to treat t as a mapping of sp onto itself.

The StoredPathMapping object (m) acts as an interface to the relational database, which we can treat just like a regular PathMapping alignment object.  The difference is that it looks up the alignment from the back-end database table when you request a mapping of a specific sequence.  In other words, only when we invoked m[sp['CYGB_HUMAN'] ] did it obtain the alignment information for that source sequence from the database; once retrieved, this information is cached locally so that subsequent queries of the same source sequence will just use the local data without having to re-query the database.
Example: Mapping an entire gene set onto a new genome version
To illustrate how Pygr can perform a big task with a little code, here is an example that maps a set of gene sequences onto a new version of the genome, using megablast to do the mapping, and a relational database to store the results.  Moreover, since mapping 80,000 gene clusters takes a fair amount of time, the calculation is parallelized to run over a large number of compute nodes simultaneously:

\begin{verbatim}
from pygr.apps.leelabdb import * # this accesses our databases
from pygr import coordinator     # this provides parallelization support

def map_clusters(server,genome_rsrc='hg17',dbname='HUMAN_SPLICE_03',
                 result_table='GENOME_ALIGNMENT.hg17_cluster_JUN03_all',
                 rmOpts='',**kwargs):
    "CLIENT FUNCTION: map clusters one by one"
    # CONSTRUCT RESOURCE FOR US IF NEEDED
    genome=BlastDB(ifile=server.open_resource(genome_rsrc,'r'))
    # LOAD DB SCHEMA
    (clusters,exons,splices,genomic_seq,spliceGraph,alt5Graph,alt3Graph,mrna, 
    protein, clusterExons,clusterSplices)=getSpliceGraphFromDB(spliceCalcs[dbname])
    # NOW MAP CLUSTER SEQUENCES ONE BY ONE TO OUR NEW genome
    for cluster_id in server:
        g=genomic_seq[cluster_id] # GET THE OLD GENOMIC SEQUENCE FOR THIS CLUSTER
        m=genome.megablast(g,maxseq=1,minIdentity=98,rmOpts=rmOpts) # MASK, BLAST, READ INTO m
        # SAVE ALIGNMENT m TO DATABASE TABLE result_table USING cursor
        createTableFromRepr(m.repr_dict(),result_table,clusters.cursor,
                            {'src_id':'varchar(12)','dest_id':'varchar(12)'})
        yield cluster_id # WE MUST FUNCTION AS GENERATOR TO KEEP ERROR TRAPPING 
		         # HAPPY

def serve_clusters(dbname='HUMAN_SPLICE_03',
                   source_table='HUMAN_SPLICE_03.genomic_cluster_JUN03',**kwargs):
    "SERVER FUNCTION: serve up cluster_id one by one to as many clients as you want"
    cursor=getUserCursor(dbname)
    t=SQLTable(source_table,cursor)
    for id in t:
        yield id # HAND OUT ONE CLUSTER ID TO A CLIENT

if __name__=='__main__': # AUTOMATICALLY RUN EITHER THE CLIENT OR SERVER FUNCTION
    coordinator.start_client_or_server(map_clusters,serve_clusters,['hg17'],__file__)
\end{verbatim}

First, let's just focus on the map_clusters() function, which illustrates how the mapping of each gene is generated and saved.  Let's examine the data piece by piece:
\begin{itemize}

\item
genome: a BLAST database storing our "new" genome sequence

\item
genomic_seq: another sequence database (which in this case happens to be stored in a relational database), mapping each cluster ID to a piece of the old genomic sequence version containing that specific gene.

\item   
cluster_id: a cluster ID for us to process.

\item
g: the actual sequence object associated with this cluster_id

\item
m: the mapping of g onto genome, as generated by megablast after first running RepeatMasker on g, using the RepeatMasker options passed as rmOpts.  Note that only the top hit will be saved (maximum number of hits to save maxseq=1), and only if it has at least 98% identity.  This alignment is then saved to a relational database table using createTableFromRepr().

\end{itemize}
This code will run in parallel over as many compute nodes as you have free, using Pygr's coordinator module.  The parallelization model for this particular task is simple: a single iterator (server) dispensing task IDs to many clients. 

\begin{itemize}

\item
server: the serve_clusters() function is trivial: all it does is connect to a specific database table (source_table) and iterate over all its primary keys, yielding them one by one.

\item    
client: the map_clusters() function expects an iterator as its first argument, which must give it a sequence of task IDs (cluster_id in this script).  This iterator is actually using an XMLRPC request to the server to get the next task ID, but that is done transparently by the coordinator.Processor() class.  The map_clusters() function is modeled as a generator: that is, it first does some initial setup (loading the database schema for example), then it runs its actual task loop, yielding each completed task ID. This enables coordinator.Processor to run map_clusters() within an error-trapping try: except: clause that catches and reports all errors to the central coordinator.Coordinator instance, and also to implement some intelligent error handling policies (like robustly preventing rare individual errors from causing an entire Processor() to crash, but detecting when consistent patterns of errors occur on a particular Processor, and automatically shutting down that Processor.

\item 
start_client_or_server(): this line automatically starts up the correct function (depending on whether this process is running as client or server).  To make a long story short, all you have to do is run the script once (as a server), and it will automatically start clients for you on free compute nodes (using ssh-agent), with reasonable load-balancing and queuing policies.  For details, see the coordinator module docs.
\end{itemize}

\subsubsection{Example: Working with the UCSC Multigenome Alignment Data}

David Haussler's group has constructed alignments of multiple genomes.  These alignments are extremely useful and interesting, but so large that it is cumbersome to work with the dataset using conventional methods.  For example, for the 8-genome alignment you have to work simultaneously with the individual genome datasets for human, chimp, mouse, rat, dog, chicken, fugu and zebrafish, as well as the huge alignment itself.  Pygr makes this quite easy.  Here we illustrate an example of mapping the exons of a specific gene onto the multiple alignment, and then printing out all intervals from the different genomes that are aligned to its exons.

\begin{verbatim}
from test import * # THIS LOADS seqdb MODULE AND GIVES US ACCESS TO OUR SCHEMA.i
..

# GET CONNECTIONS TO ALL OUR GENOMES
hg17=BlastDB(localCopy('/usr/tmp/ucsc_msa/hg17','gunzip -c /data/yxing/databases
/ucsc_msa/human_assembly_HG17/*.fa.gz >%s'))
mm5=BlastDB(localCopy('/usr/tmp/ucsc_msa/mm5','unzip -p /data/yxing/databases/uc
sc_msa/MouseMM5/chromFa.zip >%s'))
rn3=BlastDB(localCopy('/usr/tmp/ucsc_msa/rn3','unzip -p /data/yxing/databases/uc
sc_msa/RatRn3/chromFa.zip >%s'))
cf1=BlastDB(localCopy('/usr/tmp/ucsc_msa/cf1','unzip -p /data/genome/ucsc_msa/ca
nFam1/chromFa.zip >%s'))
dr1=BlastDB(localCopy('/usr/tmp/ucsc_msa/dr1','unzip -p /data/genome/ucsc_msa/da
nRer1/chromFa.zip >%s'))
fr1=BlastDB(localCopy('/usr/tmp/ucsc_msa/fr1','unzip -p /data/genome/ucsc_msa/fr
1/chromFa.zip >%s'))
gg2=BlastDB(localCopy('/usr/tmp/ucsc_msa/gg2','unzip -p /data/genome/ucsc_msa/ga
lGal2/chromFa.zip >%s'))
pt1=BlastDB(localCopy('/usr/tmp/ucsc_msa/pt1','unzip -p /data/yxing/databases/uc
sc_msa/chimp_pantro1/chromFa.zip >%s'))

genomes={'hg17':hg17,'mm5':mm5, 'rn3':rn3, 'canFam1':cf1, 'danRer1':dr1,
'fr1':fr1, 'galGal2':gg2, 'panTro1':pt1} # PREFIX DICTIONARY FOR THE UNION 
					 # OF ALL OUR GENOMES
genomeUnion=PrefixUnionDict(genomes) # GIVES ACCESS TO ID FORMAT 'panTro1.chr7'
for db in genomes.values(): # FORCE ALL OUR DATABASES TO USE INTERVAL CACHING
    db.seqClass=BlastSequenceCache

(clusters,exons,splices,genomic_seq,spliceGraph,alt5Graph,alt3Graph,mrna,protein, 
clusterExons,clusterSplices)=loadTestJUN03() # GET OUR USUAL SPLICE GRAPH
ct=SQLTableMultiNoCache('GENOME_ALIGNMENT.hg17_cluster_JUN03',clusters.cursor)
ct._distinct_key='src_id'
cm=StoredPathMapping(ct,genomic_seq,hg17) # MAPPING OF OUR CLUSTER GENOMIC ONTO 
					  # hg17

alTable=SQLTable('GENOME_ALIGNMENT.haussler_align2',clusters.cursor) # THE MAF ALIGNMENT TABLE
alTable.objclass() # USE STANDARD TupleO OBJECT FOR EACH ROW

c=clusters['Hs.10267'] # GET DATA FOR THIS CLUSTER ON chr22
loadCluster(c,exons,splices,clusterExons,clusterSplices,spliceGraph,alt5Graph,
	    alt3Graph)
for e in c.exons:
    for g in cm[e]: # GET A GENOMIC INTERVAL ALIGNED TO OUR EXON
        print 'exon interval:',repr(g)
        maf=MAFStoredPathMapping(g,alTable,genomeUnion) # LOAD ALIGNMENT FROM THE DATABASE
        for ed in maf.edges(): # PRINT THE ALIGNED SEQUENCES
            print '%s (%s)\n%s (%s)\n' % \
	    (ed.srcPath,genomeUnion.getName(ed.srcPath.path), ed.destPath,
            genomeUnion.getName(ed.destPath.path))
\end{verbatim}

A few notes:

\begin{itemize}

\item
We instantiate BlastDB objects as interfaces to each of our eight genomes.  The localCopy() function simply "caches" a local copy of each BLAST database in /usr/tmp, autogenerating it from the source (over NFS) if needed.  This script was designed to be run in parallel on many compute nodes, so I wanted to reduce NFS traffic to the minimum by creating local copies of each BLAST database before I started using them.

\item 
Since the UCSC multigenome alignment uses a "prefix.ID" format for specifying sequences (where prefix is the name of the genome, and ID is the identifier of a specific sequence in that genome, we create a "prefix union" object that acts like a union of all eight genome databases.  That is, it will map any key of the form prefix.ID to the specific database whose name is prefix, and use it to map ID to a sequence object.  To create this, we just made a dictionary that maps our desired "prefix" name to each individual genome database, and used that to initialize a PrefixUnionDict object.  It now acts as our interface to all 8 genomes.

\item
To speed up repeated sequence access requests, we force our genome database objects to use the BlastSequenceCache class as our sequence object class.  This implements smart bundling and caching of requests, so that multiple requests to the same region of the genome will be combined into a single query for the superinterval containing the individual requests, which then are simply accessed from this cache.

\item
To make our job a little harder, let's imagine that the exons we want to map were originally annotated on a different version of the human genome than the version (hg17) that is included in the UCSC alignment.  That means we first have to map each exon onto the correct coordinates in hg17 before we can even query the alignment.  That mapping is taken care of by our cm StoredPathMapping (which if you're curious was generated and saved to a MySQL database in the previous example).  To make a long story short, once we have an exon e, its mapping onto the hg17 genome is just cm[e].

\item
The UCSC multigenome alignment (itself about 24 GB in size) was parsed using a simple Pygr script written by Alex Alekseyenko, and stored as a StoredPathMapping in a MySQL database table GENOME_ALIGNMENT.hassler_align2.  This data is a bit more complicated than a standard Pygr alignment, because it maps each sequence not to another sequence, but to UCSC's "internal alignment coordinate" system.  To make a long story short, we wrote a subclass MAFStoredPathMapping that simply hides this "intermediate step" from the user; it acts like a normal Pygr PathMapping, mapping an input sequence to the set of other sequence intervals that are aligned to it.  Also, crucial for working with huge sequences (entire chromosomes, in this case), MAFStoredPathMapping allows you to specify exactly what part of the alignment database you want to fetch, by simply specifying a part of a sequence to map.  MAFStoredPathMapping takes three arguments: the sequence interval whose alignment you want to fetch; the database table object to fetch it from; and the sequence database in which to look up all the sequence identifiers (in this case we used our genomeUnion).

\item
Once we have our mapping object, we just use it like any Pygr alignment object, for example printing out all the intervals of sequence from other genomes that are aligned to our exons, as illustrated here. 

\end{itemize}
Running this on the whole gene (outputting the actual sequences of about a hundred aligned intervals), exon by exon, takes about a minute.  After importing this test script (from maftest import *), you can very easily query alignments for other genes or regions in the interactive python interpreter.  As the code promptly spits our various aligned intervals of sequence from chimp, chicken or fish, consider the wide variety of resources that Pygr is marshalling for you behind its simple front-end interface.  For every query you perform:

\begin{itemize}

\item
your exon sequence (stored in a MySQL database) is mapped to hg17 (BLAST database) using a MySQL database (cm). 

\item
the resulting hg17 intervals are then queried against the UCSC multigenome alignment, stored in a MySQL database (even just searching this 24GB database with decent speed requires careful indexing). 

\item
This yields UCSC's "internal coordinates" (virtual sequences, if you will) which are then themselves searched against the multigenome alignment (in the same MySQL database) to obtain output genome alignment interval information.

\item
Pygr turns these intervals into sequence objects that link to the eight genome databases (each a large BLAST database).

\item
When the print  statement requests str() representations of these sequence objects, Pygr uses fastacmd -L to extract just the right piece of the corresponding chromosomes from the eight BLAST databases.

\end{itemize}

(Actually, because of Pygr's caching / optimizations, considerably more is going on than indicated in this simplified sketch.  But you get the idea: Pygr makes it relatively effortless to work with a variety of disparate (and large) resources in an integrated way.)

	
\subsubsection{More examples}
\label{more-exam}

Additional examples of how to use pygr can be found in the tests/ directory within the pygr distribution package.


\section{Module Documentation}
\label{module-doc}

The following subsections provide details about how to use specific
modules of Pygr functionality. 

\subsection{graphs and graph query module}
\label{graphs-query}

The basic idea of Pygr is that all Python data can be viewed as a graph whose nodes are objects and whose edges are object relations (in Python, references from one object to another).  This has a number of advantages. 

   1. All data in a Python program become a database  that can be queried through simple but general graph query tools.  In many cases the need to write new code for some task can be replaced by a database query. 

   2. Graph databases are more general and flexible in terms of what they can represent and query than relational databases, which is very important for complex bioinformatics data.

   3. Indeed, in Pygr, a query is itself just a graph that can be stored and queried in a database, opening paths to automated query construction.

   4. Pygr graphs are fully indexed, making queries about edge relationships (which are often unacceptably slow in relational databases) fast.

   5. The interface can be very simple and pythonic: it's just a Mapping.  In Python "everything is a dictionary", also known as "the Mapping protocol": a dictionary maps some set of inputs to some set of outputs. e.g. m[a]=b maps a onto b, as a unique relation.  In Pygr, if we want to be able to map a node to multiple target nodes (i.e. allow it to have multiple edges), we simply add another layer of mapping: m[a][b]=edgeInfo (where edgeInfo is optional edge info.)

Examples of the Pygr syntax:

\begin{verbatim}
graph += node1 # ADD node1 TO graph
graph[node1] += node2 # ADD AN EDGE FROM node1 TO node2
graph[node1][node2]=edge_info # ADD AN EDGE WITH ASSOCIATED edge_info
# ADD SCHEMA BINDING WITH graph[node] BOUND AS node.attr
setschema(node,attr,graph) 
# SEARCH graph FOR SUBGRAPH {1->2; 1->3; 2->3}, 
# I.E. EXONSKIP, WHERE THE SPLICE FROM 2 -> 3 HAS ATTRIBUTE type 'U11/U12' 
for m in GraphQuery(graph,{1:{2:None,3:None},\
                   2:{3:dict(filter=lambda edge,**kwargs:edge.type=='U11/U12')},\
                   3:{}}):
    print m[1].id,m[2].id,m[1,2].id
\end{verbatim}

Let's examine these examples one by one:
\begin{itemize}

\item
adding a node to a graph is distinct from creating edges between it and other nodes.  The graph+=node notation simply adds node to the graph, initially with no edges to other nodes.

\item 
A similar syntax (graph[node1]+=node2) can be used to add an edge between two nodes, but with no edge information.  In this case the edge information stored for this relation is simply the Python None value.  Note that in Pygr the default type of graph has directed edges; that is a->b does not imply b->a.  In the default dictGraph graph class, these are two distinct edges that would have to be added separately if you truly want to have an edge going both from a to b and from b to a.

\item 
To add an edge between two nodes with edge information, use the graph[node1][node2]=edge_info syntax. 

\item 
You can bind an object attribute to a graph, using setschema(obj,attr,graph).  This acts like Python's built-in setattr(obj,attr,value), but instead of obj.attr simply storing the specified value, it is bound to the graph so that obj.attr is equivalent to graph[obj].  Both syntaxes are interchangeable and can be mixed in different pieces of code accessing the same object.

\item 
Since Pygr adopts the Mapping protocol as its model for storing graphs, you can create graphs simply by creating Python dict objects e.g. {foo:bar}.  In this example we construct a query graph whose "nodes" are just the integers 1, 2, and 3.  Since any kind of object is a valid key in Python mappings, they can therefore also be used as "nodes" in a Pygr graph.  This query graph illustrates a few simple principles:

\item 
a Pygr graph is just a two-level Python mapping.  For example, {1:{2,None}} is a graph with a single edge from 1 to 2, with no edge information.  Pygr graphs can have multiple edges from or to a given node. 

\item    
edge information in a query graph can be used to specify extra query arguments, again in the form of a Python dictionary.  This dictionary is interpreted as a set of "named arguments" to be used by the GraphQuery search method.  For example, a filter argument is interpreted as a callable function that is passed a set of named arguments describing the current edge / node matching being tested, and whose return value (True or False) will determine whether this edge "matches" our query graph.  In this example, we used it to check whether the edge.type attribute is "U11/U12" (an unusual type of splicing in gene structure graphs).

\item         
Graph query in Pygr simply means finding a subgraph of the datagraph that has node-to-node match to the edge structure given in the query graph.  In this example it is a simple exon-skip structure (3 exons, one of which can either be included or skipped).  The GraphQuery class provides a general mechanism for performing graph queries on any Python data (see below for full details).  It can be used as an iterator that will return all matches to the query (if any). 

\item          
Matches are themselves returned as a mapping of nodes and edges of the query graph (in this example, its nodes are the integers 1, 2 and 3) onto nodes and edges of the data graph.  In this example the match is returned as m, so m[1] is the node in the data graph corresponding to node 1 in the query graph.  This example assumes that object has an id attribute, which is printed out.  To refer to an edge, just use a tuple corresponding to a pair of nodes in the query graph.  In this example, 1,2 refers to the edge from node 1 to node 2 in the query graph, so m[1,2] is the edge in data graph between nodes m[1] and m[2].  This example also attempts to print an id attribute from that edge object.

\item         
Note on current behavior: currently, GraphQuery will throw a KeyError exception if it tries to search for a query node in the query graph and does not find it.  That's why we have to add the "node with no edges" entry 3:{} for node 3.  This will probably be addressed in the future, since this seems like a potential source of many annoying little bugs.

\end{itemize}

\subsubsection{dictGraph}

dictGraph is Pygr's main graph class.  It provides all the standard behaviors described above.  The current reference implementation uses standard Python dict objects to store the graph.  All the usual Mapping protocol methods can be used on dictGraph objects (top-level interface, in the examples above graph) and dictEdge objects (second-level interface; in the examples above graph[node]). e.g.

\begin{itemize}

\item
for node in graph: iterator method returns all nodes in the graph; you could also use graph.items() to get node,dictEdge pairs, etc.

\item
for node in graph[node]:  iterator method returns all nodes that are targets of edges originating at node.  Again, you could use graph[node].items() to get node,edgeInfo pairs.  Note: if node is not in graph, this will throw a KeyError exception just like any regular Python dict.

\item
if node in graph:  contains method checks whether node is present in the graph, using dict indexing.

\item
if node2 in graph[node1]:  test whether node1 has an edge to node2.  Again, if node1 isn't in graph, this will throw a KeyError exception.

\end{itemize}

\subsubsection{Directionality and Reverse Traversal}

Note that dictGraph stores directed edges, that is, a->b does not imply b->a; those are two distinct edges that would have to be added separately if you want an edge going both directions.  Moreover, the current implementation of dictGraph does not provide a mechanism for traveling an edge backwards.  To do so with algorithmic efficiency requires storing each edge twice: once in a forward index and once in a reverse index.  Since that doubles the memory requirements for storing a graph, the default dictGraph class does not do this.  If you want such a "forward-backwards" graph, use the dictGraphFB subclass that stores both forwad and reverse indexes, and supports the inverse operator ($\sim$).  $\sim$ graph gets the reverse mapping, e.g. ($\sim$ graph)[node2] corresponds to the set of nodes that have edges to node2.  This area of the code hasn't been tested much yet.

\subsubsection{Schema: binding object attributes to graphs}

The goal of Pygr is to provide a single consistent model for working with data explicitly modeled as graphs (i.e. dictGraph-like objects) and standard Python objects that were not originally designed to be queried (or thought of) as a "graph".  Since Python uses the Mapping concept throughout the language and object model, and provides introspection, there is no reason why Pygr can't work with both kinds of data transparently.  One mechanism for making this idea explicit is the idea of binding an object attribute to a graph, via the new method we've called setschema(obj,attr,graph).  The idea here is that once you bind an object attribute to a graph, the two different data models obj.attr (object model) or graph[obj] (graph model) are made equivalent and interchangeable.  Operating on one affects the other and vice versa; they are two ways of referring to the same relation.  This concept can be applied at several different levels

\begin{itemize}
\item
individual objects: just like getattr() and setattr(), you can apply schema methods to individual objects: getschema(obj,attr) (returns the bound graph) or setschema(obj,attr,graph) (binds the object attribute to the graph). 

\item
all instances of a class: you can bind specific attributes of a given class to a graph using the following class attribute syntax:

\end{itemize}
\begin{verbatim}
class ExonForm(object): # ADD ATTRIBUTES STORING SCHEMA INFO
    __class_schema__=SchemaDict(((spliceGraph,'next'),(alt5Graph,'alt5'),(alt3Graph,'alt3')))
\end{verbatim}

In this class we bound the next attribute to spliceGraph, alt5 attribute to alt5Graph, and alt3 attribute to alt3Graph.  That means, every instance obj of this class will have an attribute obj.next that is equivalent to spliceGraph[obj], etc.  Note that this is schema, not the actual operation of adding the object as a node to the graph.  Indeed, when obj is first created, it is not automatically added to spliceGraph; that is up to the user.  Unless your code has added the node to the graph (e.g. spliceGraph+=obj), obj.next should throw a KeyError exception.

The general method getschema(obj,attr) works regardless of whether the schema was stored on an individual object or at the class level.

\subsubsection{GraphQuery}

The GraphQuery class implements simple node-to-node matching, in which each new node-set is generated by an iterator associated with a specific node in the query graph.  This iterator model is general: since indexes (mappings) support the iterator protocol, a given iterator may actually be an index lookup (or other clever search algorithm).  The GraphQuery constructor takes two arguments: the default data graph being queried, and the query graph.  The query graph is just a graph; its nodes can be any object that can be a graph node (i.e. any object that is indexible, e.g. by adding a __hash__() method).  Its node objects will not be modified in any way by the GraphQuery.  Its edges are expected to be dictionaries that can be checked for specific keyword arguments:

\begin{itemize}

\item
filter: must be a callable function that accepts keyword arguments and returns True (accept this edge as a match to the queryGraph) or False (do not accept this edge as a match).  This function will be called with the following keyword arguments:
       \begin{itemize}
          \item
          toNode: the target node of this edge, in the data graph
          \item
          fromNode: the origin node of this edge, in the data graph
          \item
           edge: the edge information for this edge in the data graph
          \item 
           queryMatch: a mapping of the query graph to the data graph, based on the partial matchings made so far
           \item
           gqi: the GraphQueryIterator instance associated with this matching operation.  Much more data is available from specific attributes of this object.
	\end{itemize}

\item 
dataGraph: graph in which the current edge should be search for.  This allows a query to traverse multiple graphs.  In other words, when searching for edges from the current node, look up dataGraph[node] instead of defaultGraph[node].

\item
attr: object attribute name to use as the iterator, instead of the defaultGraph.In other words, generate edges from the current node via getattr(node,attr) instead of defaultGraph[node].  The object obtained from this attribute must act like a mapping; specifically, it must provide an items() method that returns zero or more pairs of targetNode,edgeInfo, just like a standard Pygr dictEdge object. 

\item
attrN: object attribute name to use as the iterator, instead of the defaultGraph. In other words, generate edges from the current node via getattr(node,attr) instead of defaultGraph[node].  The object obtained from this attribute must act like a sequence; specifically, it must provide an iterator that returns zero or more targetNode.  The edgeInfo for any edges generated this way will be None.

\item
f: a callable function that must return an iterator producing zero or more pairs of targetNode,edgeInfo.  Typically f is a Python generator function containing a statement like yield targetNode,edgeInfo.

\item
fN: a callable function that must return an iterator producing zero or more targetNode.  Typically fN is a Python generator function containing a statement like yield targetNode.  The edgeInfo for any edges generated this way will be None.

\item 
subqueries: a tuple of query graphs to be performed.  Since GraphQuery traversalcorresponds to logical AND (i.e. all the query graph nodes must be successfully matched to return a match), the subqueries are currently treated as a union (logical OR), by simply returning every match from each subquery as a match (at least for this node).  Each subquery is itself just another query graph.  Moreover, since query graphs can share nodes (i.e. the same object can appear as a node in multiple query graphs), subqueries can make reference to nodes that are already matched by the higher query.  This is an area that has not been explored much yet, but provides a pretty general model for powerful queries.

\end{itemize}
The attr - subqueries options are all implemented as extremely simple subclasses of GraphQuery.  If you want to see just how easy it is to write new subclasses of GraphQuery functionality, look at the graphquery.py module (the entire graph query module is only 237 lines long).

Note: an easy way to pass keyword dictionaries (e.g. as edge information) is simply using the dict() constructor, e.g. dict(dataGraph=myGraph,filter=my_filter).  I think this is a little more readable than {'dataGraph':myGraph, 'filter':my_filter}.

Note on current behavior: currently, the GraphQuery iterator returns the same mapping object for each iteration (simply changing its contents).  So to save these multiple values safely in a list comprehension we have to copy each one into a new dict object via dict(m).

\subsubsection{What is GraphQuery actually doing?}

A GraphQuery is basically an iterator that returns all possible mappings of the query graph onto the datagraph that match all of the nodes and edges of the query graph onto nodes and edges of the data graph.  As an iterator, it does not instantiate a list of the matches, but simply returns the matches one by one.  The current design is very simple.  The GraphQuery constructor builds an "iterator stack" of GraphQueryIterators, each representing one node in the query graph; they are enumerated in order by a breadth-first-search of the query graph.  The GraphQuery iterator processes the stack of GraphQueryIterators: any match simply pushes the stack to the next level; any match at the deepest level of the stack is a complete match (yield the queryMatch mapping); the end of any GraphQueryIterator simply pops the stack.  One obvious idea for improving all this is to replace this "interpreter" with a "compiler" that compiles Python for loops that are equivalent to this stack, and run that... likely to be many fold faster.





\subsection{sequence Module}
\label{sequence}

{\em Base classes for representing sequences and sequence intervals.}


\subsubsection{Overview}
Pygr provides one base class representing both sequences and sequence intervals (SeqPath),
from which all sequence classes are derived (Sequence, SQLSequence, BlastSequence etc.).
In this section we document both the features of the base class, and ways to extend or
customize it by creating your own subclasses derived from SeqPath.  The IntervalTransform
class represents a coordinate system mapping from one interval of a sequence, onto 
another interval of the same or a different sequence.

\subsubsection{SeqPath}
This class provides the basic capabilities of a sliceable sequence or sequence interval,
widely used in Pygr.  It tries to provide core operations on sequences in a highly
Pythonic way:

\begin{itemize}

\item    
{\em Python Sequence}: of course, SeqPath behaves like a Python sequence. i.e.
the length of a \class{SeqPath} \var{s} is just \code{len(s)}, 
and you iterate over the ``letters'' in it using \code{for l in s:}  
(Note, the individual letters produced by this iterator
will themselves be \class{SeqPath} objects (by default, of length 1)).  
And all the slicing
operations defined for Python Sequences also apply to 
\class{SeqPath} (see below).

\item    
{\em Slicing}: \class{SeqPath} is designed to represent a slice 
(subinterval) of a sequence.
Like the Python builtin \class{slice} class, it has \member{start}, 
\member{stop}, and \member{step} attributes that indicate 
the interval beginning, end, and ``stride''.
Moreover, it is itself sliceable in the usual pythonic way, 
i.e. \code{s[start:stop]},
where \var{start} and \var{stop} are in the local coordinate system of \var{s} 
(i.e. \code{s[0]} is the first letter of the interval represented by 
\var{s}). Note that \class{SeqPath}
follows the Python slicing coordinate conventions of positive integers as
forward coordinates (i.e. counting from the interval start) and negative integers
as reverse coordinates (i.e. counting from the interval end).

\item    
{\em String value}: to obtain the actual sequence string representation
of a \class{SeqPath}, just use the Python builtin \code{str(s)}.  
Note that in most cases
a SeqPath object does not itself store the sequence string associated with it
but obtains it from somewhere else when the user requests it.

\item
{\em comparison and containment}: \class{SeqPath}
implements the interval-ordering
and interval-containment relations using the standard Python order operators
and containment operators. i.e. s<t iff s.start<t.start, and s in t iff
t.start<=s.start and s.stop<=t.stop.

\item
{\em orientation}: SeqPath carefully represents relationships between intervals
on opposite strands of a double-stranded nucleotide sequence.  A SeqPath object
knows whether it is an interval on the forward or reverse strand.  Pygr provides
a number of operations for manipulating and comparing intervals of different
orientations.  For example, \code{-s} yields the interval of the opposite strand that
is base-paired to interval s (i.e. this is not just the reverse-complement of \var{s}
in the string 'atgc' $\rightarrow$ 'gcat' sense, but is specifically the SeqPath
object representing the coordinate
interval on the opposite strand that is base-paired to \var{s}).

\item
{\em schema}: a SeqPath object knows ``what sequence'' it is an interval of;
it is not just a (start,stop) coordinate pair, but is actually bound to a specific
parent sequence object.  Specifically, s.path is the parent sequence object of
which s is a subinterval; s.path will itself be an instance of SeqPath, and its path
attribute will simply be itself.  All SeqPath objects are descended from such ``top-level''
SeqPath objects.  Note that when you have sequence intervals from both forward
and reverse strands of a sequence, all of the forward strand intervals will share
the same path attribute (your original top-level sequence object representing
the whole sequence in forward orientation), while all the reverse strand intervals
will reference another top-level SeqPath created automatically to represent the
reverse strand.

\item
{\em graph structure}: a SeqPath object itself acts as a graph, whose nodes are
the individual letters of the sequence, and whose edges represent the link 
from each letter to the next (if any).  Thus standard graph query works on
SeqPath objects, through the usual interfaces:

\begin{verbatim}
for l in s: # GET EACH LETTER OF THE SEQUENCE
    c=str(l)

edge=s[l1][l2] # GET EDGE INFORMATION FOR l1 --> l2

for l1,l2,edge in s.edges(): # GET ALL l1 --> l2 EDGES
    do_something(l1,l2,e)

# DUMB GRAPH QUERY TO FIND 'AG' SUBSTRINGS IN SEQUENCE s
for d in GraphQuery(s,{0:{1:dict(filter=lambda fromNode,toNode:
                                 str(fromNode)=='A' and str(toNode)=='G')},
                       1:{}})
    l1,l2,edge=d[0],d[1],d[0,1]
\end{verbatim}  

For more information about edges, see the LetterEdge class.

\item
{\em Mutable Sequences}: Just as the Python builtin list class implements
``mutable sequence'' objects that can be resized, SeqPath objects can be
resized and changed, without breaking existing subinterval objects that
are ``part of'' the resized SeqPath object.  In particular, just as a list
can be resized by extending its ``stop'' coordinate to a higher value, a SeqPath can
be resized by extending its stop coordinate to a higher value.  Indeed,
you can even create a SeqPath for a particular sequence without knowing that
sequence's length (computing the length of a genome sequence might take a long
time, if all you want to do is create a sequence object to represent that
sequence).  You can do this by passing {\em None} as the stop (or start)
coordinate.  In that case, SeqPath will automatically determine its own
length at a later time iff a specific user operation makes it absolutely 
necessary to know this length.

\item
{\em intersection, union, difference}: SeqPath uses the Python *, + and - 
operators to implement interval intersection, union, and difference
operations respectively.

\end{itemize}


\subsubsection{Sequence class}

\begin{funcdesc}{sequence.Sequence}{s, id}
  The Sequence class provides a SeqPath flavor that stores a sequence string
  {\em s} and identifier {\em id} for this sequence.

\begin{verbatim}
from pygr import sequence
seq=sequence.Sequence('GPTPCDLMETQ','FOOG_HUMAN')
\end{verbatim}
\end{funcdesc}


\begin{funcdesc}{update}{s}
  You can change the actual string sequence to a new string {\em s}
  using the {\em update} method:

\begin{verbatim}
seq.update('TKRRPLEDKMNEPS')
\end{verbatim}
\end{funcdesc}

\begin{funcdesc}{seqtype}{}
  returns DNA_SEQTYPE for DNA sequences, 
  RNA_SEQTYPE for RNA, and PROTEIN_SEQTYPE for protein.
\end{funcdesc}


\begin{funcdesc}{reverse_complement}{s}
  returns the reverse complement of the sequence string s.
\end{funcdesc}


\begin{itemize}
\item
\member{id}: the \member{id} attribute stores the sequence's identifier.

\end{itemize}

\subsubsection{Coordinate System}
SeqPath follows Python slicing conventions (i.e. 0-based indexing, positive indexes
count forward from start, negative indexes count backwards from the sequence
end, and always {\em s.start<s.stop}).

Each SeqPath object has a number of attributes giving information about its
``location'':

\begin{itemize}

\item    
\member{orientation}: +1 if on the forward strand, or -1 if on the reverse strand.

\item
\member{path}: the top-level sequence object that this interval is part of, or self
if this object is its top-level (i.e. not a slice of a larger sequence).  Note that
all forward intervals share the same path attribute, but reverse strand intervals
all have a path attribute that represents the entire reverse strand.

\item
\member{pathForward}: same as {\em path}, but always the forward strand sequence.

\item
\member{start}: start coordinate of the interval.  NB: SeqPath stores coordinates
relative to the start of the {\em forward} strand.  This is necessary for allowing
resizing of the top-level SeqPath; if coordinates were relative to the end of the
sequence, they would have to be recomputed every time the length of the sequence 
changed.  The main consequence of this is that coordinates for forward intervals
are always positive, whereas coordinates for reverse intervals are always 
negative (i.e. following the Python convention
that negative coordinates count backwards
from the end, and the fact that the end of the reverse strand corresponds to 
the start of the forward strand). NB2: if the SeqPath was originally created with
{\em start=None}, requesting its start attribute will force it to compute its start
coordinate, typically requiring a computation of the sequence length.  In this
case, the start attribute will computed automatically by SeqPath.__getattr__().

\item
\member{stop}: end coordinate of the interval.  The above comments for {\em start}
apply to {\em stop}.  Note that for reverse intervals, a {\em stop} value of 0
means the end of the reverse strand (i.e. -1 is the last nucleotide of the 
reverse strand, and 0 is one beyond the last nucleotide of the reverse strand).

\item
\member{_abs_interval}: a tuple giving the ({\em start,stop}) coordinates of the 
interval on the forward strand corresponding to this interval (i.e. for a 
forward interval, itself, or for a reverse interval, the interval that base-pairs
to it).

\end{itemize}


\subsubsection{Extending and Customizing}
There are several methods and attributes you can override to extend or customize
the behavior of your own SeqPath-derived classes.  Typically you will derive
either from the Sequence class, or in some cases from the SeqPath class.

\begin{funcdesc}{strslice}{start, stop} 
  called to get the string
  sequence of the interval ({\em start, stop}).  You can provide your own strslice()
  method to customize how sequence is stored and accessed.  For example,
  \method{SQLSequence.strslice()} gets the sequence via a SQL query, and 
  \method{BlastSequence.strslice()} obtains it using the 
  \code{fastacmd -L start,stop} 
  UNIX shell command from the NCBI toolkit.
\end{funcdesc}

\begin{funcdesc}{__len__}{}
  called to compute the length of the sequence.  You can
  customize this to provide an efficient length method for your particular
  sequence storage.  e.g. \class{SQLSequence} obtains it via a SQL query; 
  \class{BlastSequence} obtains it from a precomputed length index.
  The default \method{Sequence.__len__()} method computes it from 
  \code{len(self.seq)}, assuming that the sequence can be accessed
  from the \member{seq} attribute.
\end{funcdesc}

\begin{funcdesc}{__getitem__}{slice_obj}
  if you want to monitor or intercept slicing
  requests on your sequence, you can do so by providing your own getitem method.
  See \class{seqdb.BlastSequenceCache} class for an example.
\end{funcdesc}

\begin{funcdesc}{__getattr__}{attr}
  if you subclass a \class{SeqPath}-derived class and supply a \method{__getattr__}
  method for your subclass, it {\em must} call the parent class's 
  \method{__getattr__}.  This is essential for ``delayed evaluation'' of
  \member{start} and \member{stop} attributes, which are generated automatically
  by \class{SeqPath}'s \method{__getattr__}.  If your subclass inherits from
  more than one parent class, check whether {\em both} parents supply a 
  \method{__getattr__}, in which case your subclass must supply a
  \method{__getattr__} that explicitly calls both of them.  Failing to do so
  will lead to strange bugs.
\end{funcdesc}

\begin{itemize}
\item
\member{seq}: the \method{Sequence.strslice()} method assumes that 
the actual sequence is stored
on the \member{seq} attribute.  You could customize this behavior by 
making the \member{seq} attribute a property that is computed on the fly
by some method of your own.

\end{itemize}


\subsubsection{IntervalTransform}
This class provides a mapping transform between the coordinate
systems of a pair of intervals.

\begin{verbatim}
xform=IntervalTransform(srcPath,destPath)
d2=xform(s2) # MAPS s2 FROM srcPath coords to destPath coord system
d3=xform[s2] # CLIPS s2 TO NOT EXTEND OUTSIDE srcPath, THEN XFORMS
s3=xform.reverse(d3) # MAP BACK TO srcPath COORD SYSTEM
\end{verbatim}

\subsubsection{Seq2SeqEdge}
This class represents a segment of alignment between two sequences.
It is a temporary object created in association with a MSASlice
object (see Alignment Module below).

\begin{funcdesc}{__init__}{msaSlice, targetPath, sourcePath=None}
  Create a Seq2SeqEdge for the targetPath, on the specified alignment
  slice.  If sourcePath is None, it will be calculated automatically
  by calling the slice's methods.
\end{funcdesc}

\begin{funcdesc}{__iter__}{sourceOnly=True, **kwargs}
  iterate over source intervals within this segment of alignment.
  \var{kwargs} will be passed on to the \var{msaSlice}'s 
  \method{groupByIntervals} and \method{groupBySequences} methods.
\end{funcdesc}

\begin{funcdesc}{items}{**kwargs}
  same as \method{__iter__}, but gets tuples of (source_interval,target_interval).
\end{funcdesc}

\begin{funcdesc}{percent_id}{}
  Compute the percent identity between the source and target sequence
  intervals in this segment of the alignment.
\end{funcdesc}

\subsubsection{SeqFilterDict}
This dict-like class provides a simple way for masking a set of sequences
to specific intervals.  It stores a specific interval for each
sequence.  Subsequent look-up using a sequence interval as a key will
return the intersection between that interval and the stored interval
for that sequence in the dictionary.  If there is no overlap, it
raises \code{KeyError}.

\begin{verbatim}
d=SeqFilterDict(seqIntervalList)
overlap=d[ival] # RETURNS INTERSECTION OF ival AND STORED IVAL, OR KeyError
\end{verbatim}

You can pass a list of intervals to store to the class constructor (as 
shown above).  You can also add a single interval using the syntax
\code{d[saveInterval]=saveInterval}.  (This syntax reflects the actual
mapping that the dictionary will perform if later called with the
same interval).

\subsubsection{LetterEdge}
This class represents an edge from origin -> target node.

\begin{funcdesc}{__iter__}{}
  iterate over seqpos for sequences that traverse this edge.
\end{funcdesc}

\begin{funcdesc}{iteritems}{}
  generate origin, target seqpos for sequences that traverse this edge.
\end{funcdesc}

\begin{funcdesc}{__getitem__}{seq}
  return origin,target seqpos for sequence \var{seq};
  raise KeyError if not in this edge
\end{funcdesc}

\begin{itemize}
\item
\member{seqs}: returns its sequences that traverse this edge
\end{itemize}


\subsubsection{Functions}
The sequence module also provides convenience functions:

\begin{funcdesc}{guess_seqtype}{s}
  based on the letter composition of
  the string {\em s}, returns DNA_SEQTYPE for DNA sequences, 
  RNA_SEQTYPE for RNA, and PROTEIN_SEQTYPE for protein.
\end{funcdesc}

\begin{funcdesc}{absoluteSlice}{seq, start, stop}
  returns the sequence interval of top-level sequence object associated
  with \var{seq}, interpreting \var{start} and \var{stop} according to
  the Pygr convention: a pair of positive values represents an interval
  on the forward strand; a pair of negative values represents an
  interval on the reverse strand (see Coordinate System, above).
\end{funcdesc}




\subsection{Alignment Module}
\label{seqdb}

{\em Pygr interface to sequence alignment, and scalable storage for multigenome alignments}


\subsubsection{Overview}

Pygr provides a general model for interfacing with any kind of sequence alignment,
and also a uniquely scalable storage system for working with huge multiple sequence
alignments such as multigenome alignments.  Specifically, it lets you work with
an alignment both in the traditional Row-Column model (each row is a sequence, each
column is a set of individual letters from different sequences, that are aligned;
we will refer to this as the RC-MSA model), and also
as a graph structure (known as a Partial Order Alignment, which we will refer to as
the PO-MSA model).  This supports ``traditional'' alignment analysis, as well
as graph-algorithms, and even graph query of alignments.

This model has a few basic classes:
\begin{itemize}
\item    
\class{MSA}: this class represents an entire alignment.  It acts as a graph whose
nodes are sequences (or sequence intervals) that are aligned, and whose edges 
represent specific alignment relationships between specific pairs of sequences 
(or intervals).  Specifically, it acts as a dictionary whose keys are SeqPath
objects, and whose values are MSASlice objects (representing an alignment segment
associated with a specific SeqPath, see below for details).  For example, to find
out what's aligned to some sequence interval s1:
\begin{verbatim}
for s2 in msa[s1]: # GET ALL INTERVALS s2 ALIGNED TO s1 IN msa
    do_something(s1,s2)
\end{verbatim}

In addition, its {\em letters} attribute acts as a graph interface
to the Partial Order alignment (PO-MSA) representation of the alignment.  I.e.
it is a graph whose nodes each represent a set of individual letters from 
different sequences, that are aligned to each other, and whose edges connect
pairs of nodes that are ``adjacent'' to each other in at least one sequence.
Specifically, it acts as a dictionary whose keys are MSANode objects (see below),
and whose edges are LetterEdge objects (see previous section).
\begin{verbatim}
for node in msa.letters: # GET ALL ALIGNMENT ``COLUMNS'' IN msa
    for l in node: # GET ALL INDIVIDUAL SEQ LETTERS ALIGNED HERE
        say_something(node,l)
\end{verbatim}


\item    
\class{MSASlice}: this class represents a segment of alignment associated with
a specific sequence interval (s1).  It acts as dictionary whose keys are sequence
intervals s2 aligned to s1, and whose values are MSASeqEdge objects
that represent the alignment relationship between s1 $\rightarrow$ s2.  It also 
has a {\em letters} attribute, that represents the subgraph of nodes
associated specifically with s1, and the edges that interconnect them.

\begin{verbatim}
myslice=msa[s1]: # GET SLICE ALIGNED TO s1 IN msa
    for node in myslice.letters:  # GET ALL ALIGNMENT ``COLUMNS'' FOR s1
        for l1,l2,e in node.edges(): # GET INDIVIDUAL LETTERS ALIGNED TO l1 OF s1
	    whatever(l1,l2,e)
\end{verbatim}

This class also has a {\em regions} method that generates all the alignment
interval relationships in this slice according to ``grouping'' criteria such
as maximum permissible gap length, etc.  (i.e. any region of alignment containing
no gaps larger than a specified size would be returned as a single region, 
whereas any gap larger than the specified size would split it into two separate
regions).  This provides a general interface for group-by operations in alignment
query.

\item    
\class{MSASeqEdge}: this class represents a relationship between a pair of 
sequence intervals s1 and s2 (SeqPath objects).  It provides a mapping between
subintervals of s1 $\rightarrow$ s2.  I.e. it acts as a dictionary 
that accepts subintervals of s1 as keys, and maps them to aligned
subintervals of s2.  It also 
has a {\em letters} attribute, that represents the subgraph of nodes
associated specifically with them, and the edges that interconnect the nodes.

\item    
\class{MSANode}: this class represents a specific ``column'' in the alignment
that aligns a set of individual letters from different sequences.  This
corresponds to a node in the PO-MSA representation of the alignment.
It acts as a dictionary whose keys are sequence intervals (typically only
one letter long) aligned in this column, and whose values are MSASeqEdges
representing the alignment of that letter to the column (see above).

\end{itemize}

\subsubsection{NestedList Storage}
Pygr provides a highly scalable storage mechanism for working with
multi-genome alignments.  One fundamental challenge in working with
very large alignments is the {\em interval overlap query} problem: 
to obtain a portion of an alignment (defined by some interval of
interest) requires finding all interval elements in the ``alignment
database'' that overlap the query interval.  Since the intervals
can be indexed by start (or end) position, one can typically find the
first overlapping element in $O(\log N)$ time, where $N$ is the total
number of intervals in the database.  The problem is that since
standard index structures cannot index both {\em start} and {\em end}, 
to obtain {\em all} intervals that overlap the query interval, one must scan
forwards (or backwards) from that point.  Furthermore, one cannot stop
at the first non-overlapping interval; there might be an extremely long 
interval at the very beginning of the index, that extends to overlap 
the query interval.  In this case, one would have to scan the entire
database ($O(N)$ time) to guarantee that all overlapping intervals are
found.

The {\em nested list} data structure solves this problem, by moving
any interval in the database that is {\em contained} in another interval
out of the top-level interval list, into the {\em sublist} of the
parent interval.  Based on this, one can prove that one can stop
the scanning operation at the first non-overlapping interval (i.e.
the overlapping intervals in any list form a single contiguous block).
Overall, this reduces the query time to $O(\log N + n)$, where $n$ is
the number of intervals in the database that actually overlap the 
query (i.e. results to return).  Moreover, the nested list data structure
can be implemented very well both in computer memory (RAM) or as indexed
disk files.  Pygr's disk-based cnestedlist database can complete
a typical interval query of the 26GB UCSC 8 genome alignment in
about 60 microseconds, compared with 10-30 seconds per query using
MySQL.

\subsubsection{NLMSA}
Top-level object representing an entire multiple sequence alignment, 
stored using a set of disk-based nested list interval databases.
The alignment is stored as an interval representation of a 
{\em linearized partial order} (LPO), using {\em nested list}
databases.  This has several elements:

\begin{itemize}

\item
{\em PO-MSA}: Conceptually, the alignment is represented as a partial order alignment
(PO-MSA), in which aligned sequence intervals are fused together as a single
``node'' in the alignment graph; two nodes are connected by an edge if and only
if they are adjacent in at least one of the sequences aligned to them
(i.e. if residue {\em i} of that sequence is in the first node, and 
residue {\em i+1} is in the second node, then there is a directed edge
from the first PO-MSA node to the second node).

\item
{\em LPO}: This alignment graph is {\em partially ordered}.  Let's define an
ordering relation {\em ``i<j''} to mean ``there exists a path
of directed edges from {\em i} to {\em j}''.  For two 
letters {\em i} and {\em j} in a sequence, {\em i<j XOR j<i} (i.e. all
nodes have an ordering relationship).  By contrast, if two nodes in the LPO
represent insertions in different sequences, then NOT {\em i<j} AND NOT {\em j<i}.
Thus there can be some nodes in the LPO that have no ordering relationship
with respect to each other.  It is still possible to map the PO-MSA onto 
a linear coordinate system (i.e. to ``linearize'' the partial order): as long
as the graph contains no cycles, we can map the nodes {\em i,j,k,...} of the graph
onto a linear coordinate system {\em x,y,z,...} such that for any pair of
nodes {\em i,j} mapped to coordinates {\em x<y}, we assert NOT {\em j<i}.  This is
called the {\em linearized partial order} (LPO). This maps the PO-MSA onto
a standard Row-Column MSA format, where the LPO coordinate (just an integer
sequence 0,1,2...) can be considered the index value of each alignment column.

\item
{\em nested list}: The actual alignment data are stored in the form of
({\em start,stop}) pairs representing aligned intervals.  Since this representation
uses intervals, not individual letters, it takes no more memory to store
an alignment of two 100 kb regions than it does to align two individual letters.
This is important for scalable storage (and query) of large multi-genome
alignments.  (Each alignment interval takes 24 bytes: five \class{int} for
the {\em (start,stop)} pairs and target sequence ID, plus one \class{int}
for the sublist ID).
These interval databases are stored using nested lists.  Specifically, 
the alignment is stored as 1) a mapping of each aligned sequence interval
onto an LPO coordinate interval; 2) a reverse mapping of each LPO interval onto
all the sequence intervals that are aligned there.  To find the alignment of
a sequence interval onto the other sequences in the alignment, that interval
is first mapped onto the LPO, and from there mapped back to intervals in the
other sequences.  A nested list database is stored for {\em each} of these 
mappings (i.e. for an alignment of {\em N} sequences, there will be {\em N+1}
nested list databases to store the MSA).  Furthermore, if the size of the LPO
coordinate system (i.e. number of columns in its RC-MSA format)
grows larger than the range representable by \class{int} (typically $2^{31}$  = 2 GB),
the LPO will have to be split into separate nested list databases of a size
smaller than the maximum range representable by \class{int}.  This is necessary
for handling alignments of large genomes (e.g. the human genome is approximately 3 GB).
Pygr takes care of all this for you automatically.  Note, as an entirely separate 
issue, that Pygr's cnestedlist
module uses the \class{long long} data type for file offsets and
the \function{fseeko()} POSIX interface for large file support (i.e. 64-bit
file sizes), which is supported by current versions of Linux, Mac OS X, etc;
otherwise, check if your filesystem supports this.

\end{itemize}

This functionality is encapsulated in the NLMSA class, which has a number of methods
and attributes.

Construction Methods:

\begin{funcdesc}{NLMSA}{pathstem='', mode='r', seqDict=None, mafFiles=None, maxOpenFiles=1024, maxlen=None}
  Constructor for the class.  \var{pathstem} specifies a path and filename prefix for
  the NLMSA files (since multiple files are used to store one NLMSA, it will automatically add a
  number of suffixes automatically to open the necessary set of files for the NLMSA).
  \var{mode} is either ``r'' to open an existing NLMSA (from the \var{pathstem} disk files), 
  or ``w'' to create a new one (which will be saved to the \var{pathstem} disk files).
  \var{seqDict} specifies a dictionary which maps sequence names to actual sequence
  objects representing those sequences.  \var{mafFiles} can be used to specify a list of
  filenames storing a multiple sequence alignment in the UCSC MAF format.
  \var{maxOpenFiles} limits the open file descriptors the NLMSA will use.  Since
  each sequence has a separate nested list database file, a large multi-genome alignment
  (with each genome containing 20 different chromosomes, say) can rapidly open a large
  number of file descriptors.  Note: NLMSA only opens a given sequence's nested list database
  when one of your queries actually requires access to that sequence; it then
  keeps that file descriptor open to make subsequent queries to it fast.  If the number
  of open file descriptors would exceed \var{maxOpenFiles}, it will close other open
  database files, which may slow down query performance (due to having to open and close
  databases repeatedly to process queries). \var{maxlen} specifies the maximum coordinate
  value for an LPO database.  Its default value is 2GB, to prevent \class{int} overflow.
  Using a smaller value can be useful, to 1) limit the size of the LPO in memory
  during initial construction, and 2) to limit the size of LPO database files on disk
  (if for example, your file system does not support files above some maximum size).
  During initial construction of the NLMSA (from MAF files or user-specified interval
  alignments), the algorithm performs a one-pass sort of the LPO intervals.  Thus,
  this set of intervals is briefly held in RAM for this sort.  If you have insufficient
  RAM, the construction step may raise a MemoryError.  If so, you can avoid this problem
  by using a smaller \var{maxlen} value.
\end{funcdesc}


\begin{funcdesc}{__setitem__}{i1, s1}
  store an alignment interval; specifically, align the sequence
  interval \var{s1} to the LPO coordinate interval \var{i1}.  This provides a trivial
  user interface for you to build any desired alignment.  If the sequence containing
  interval \var{s1} is not already in the NLMSA, it will be added for you automatically
  (i.e. creating the necessary indexing, nested list database files, etc.).  In this
  case, the sequence must supply a unique string identifier, which will be used
  on subsequent attempts to open the NLMSA database, to match the individual sequence
  nested-list databases against corresponding sequence objects (using {\em seqDict},
  see above).
\end{funcdesc}


\begin{funcdesc}{build}{}
  to construct the final nested list databases,
  after all the desired alignment intervals have been saved (using either the
  \var{mafFiles} constructor argument, or using \method{setitem} above).  This method
  simply calls the build() method on all the constituent NLMSASequence objects
  in this alignment.
\end{funcdesc}


Alignment Usage Methods:

\begin{funcdesc}{__getitem__}{s1}
  get the alignment slice for the sequence interval \var{s1},
  i.e. get an NLMSASlice object representing the set of intervals aligned to \var{s1}.
  You can also use a regular Python {\em slice} object using integer indices
  ie. \code{nlmsa[1:45]}, in which case, it gets the NLMSA slice corresponding to that 
  region of the LPO coordinate system.
\end{funcdesc}

\begin{itemize}
\item
\member{seqs}: This attribute provides a dictionary of the sequences in
the NLMSA, whose keys are top-level sequence objects, and whose values are
the associated NLMSASequence object for each sequence.  Ordinarily you will have
no need to access the NLMSASequence object directly; only do so if you know what
you're doing (details below).  This dictionary is of type NLMSASeqDict (see below).

\end{itemize}

\subsubsection{NLMSASlice}
A temporary object created on-the-fly to represent (an interface to provide 
information about) the portion of the alignment associated with a specific
sequence interval.  Specifically, it acts like a dictionary whose keys are
sequence intervals that are aligned to this region, and whose values are
\class{Seq2SeqEdge} objects providing detailed information about the alignment of
the target interval (key) to the source interval (the sequence interval
used to create the NLMSASlice in the first place).  You can use this
dictionary interface in several ways:


\begin{funcdesc}{__iter__}{}
  iterates over all sequence intervals that have
  a 1:1 mapping (i.e. a block of alignment containing no indels) to
  all or part of the source interval.
\end{funcdesc}


\begin{funcdesc}{keys}{maxgap=0, maxinsert=0, mininsert= 0, filterSeqs=None, mergeAll=True, ivalMethod=None, sourceOnly=False, indelCut=False, seqGroups=None, minAligned=1, pMinAligned=0., seqMethod=None, **kwargs}
  Provides a more general interface than {\em iter()}, with two types of 
  group-by capabilities, ``group-by'' operations on the alignment intervals
  contained within this slice (``horizontal'' grouping), 
  and on the sets of sequences aligned
  to this slice (``vertical'' grouping).

  1. ``group-by'' operations on the alignment intervals
  contained within this slice.  It allows the user to supply
  various parameters for controlling when alignment intervals will be
  merged or split in the results that it returns.  \var{maxgap} sets the
  maximum gap size for merging two adjacent intervals.  If the target sequence
  for the two alignment intervals has a gap longer than \var{maxgap} 
  letters between the two alignment intervals, they will be returned as
  separate intervals; otherwise they will be merged as a single alignment
  region.  \var{maxinsert} sets the maximum length of insert in the target
  sequence that allows to adjacent intervals to be merged as a single alignment
  region in the results.  \var{mininsert} is specifically for handling
  alignments that may have small ``cycles'' (due to slight inconsistencies
  in the reported alignment intervals, for example, if a portion of sequence
  can align at both the end of one interval or at the beginning of another, and
  the intervals are actually added to the NLMSA that way, then the {\em start}
  of the second interval will actually be {\em before} the {\em stop} of 
  the first interval; this corresponds to a negative insert value).  A
  \var{mininsert} value of zero (the default), prevents any such interval
  pairs from being merged.  Giving a negative \var{mininsert} value will allow
  interval pairs whose insert value is greater than or equal to this value, 
  to be merged.  \var{filterSeqs}, if not None, should be a dict of sequences
  used to filter the group-by analysis; i.e. only alignment intervals 
  containing these sequences are considered in the analysis.  More
  specifically, \var{filterSeqs} can be used to mask the group-by analysis
  to a specific interval of a sequence, by having \var{filterSeqs}
  return only the intersection between the interval it is passed as a key,
  and the masking interval that it stores.  If there is no overlap, it
  must raise \code{KeyError}.  The \class{sequence.SeqFilterDict} class
  provides exactly this masking capability, i.e.
\begin{verbatim}
d=sequence.SeqFilterDict(someIntervals)
overlap=d[ival] # RETURNS INTERSECTION BETWEEN ival AND someIntervals, OR KeyError
\end{verbatim}
  \var{ivalMethod},
  if not None, allows the user to provide a Python function that performs
  interval grouping.  Specifically it is called as
  \function{ivalMethod(l, ns,msaSlice=self, **kwargs)}, where \var{l} is the
  list of intervals for NLMSASequence \var{ns} within the current slice 
  \var{msaSlice}; all other args are passed as a dict in \var{kwargs}.

  2. merge groups of sequences using "vertical" group-by rules.
  \var{seqGroups}: a list of one or more lists of sequences to group.
  If None, the whole set of sequences will be treated as a single group.
  Each group will be analyzed separately, as follows:
  \var{sourceOnly}: output intervals will be reported giving only
  the corresponding interval on the source sequence; redundant
  output intervals (mapping to the same source interval) are
  culled.  Has the effect of giving a single interval traversal
  of each group.
  \var{indelCut}: for \var{sourceOnly} mode, do not merge separate 
  intervals that the groupByIntervals analysis separated due to an indel).
  \var{minAligned}: the minimum number of sequences that must be aligned to
  the source sequence for masking the output.  Regions below
  this threshold are masked out; no intervals will be reported
  in these regions.
  \var{pMinAligned}: the minimum fraction of sequences (out of the
  total in the group) that must be aligned to the source
  sequence for masking the output.
  \var{seqMethod}: you may supply your own function for grouping.
  Called as \function{seqMethod(bounds,seqs,**kwargs)}, where
  \var{bounds} is a sorted list of
  \var{(ipos,isStart,i,ns,isIndel,(start,end,targetStart,targetEnd))}
  and \var{seqs} is a list of sequences in the group.
  Must return a list of \var{(sourceIval,targetIval)}.  See the docs.

\end{funcdesc}


\begin{funcdesc}{iteritems}{**kwargs}
  same keys as {\em iter}, but for each provides the source interval
  to target interval mapping (\class{Seq2SeqEdge}).
  Uses same group-by arguments as \method{keys()}.
\end{funcdesc}


\begin{funcdesc}{edges}{**kwargs}
  same interval mappings as {\em iteritems}, but for
  each provides a tuple of three objects:
  the source interval, the corresponding target interval,
  and the \class{Seq2SeqEdge} providing detailed
  information about the alignment between the source and target intervals
  (such as percent identity, etc.).
  Uses same group-by arguments as \method{keys()}.
\end{funcdesc}


\begin{funcdesc}{__getitem__}{s1}
  treats \var{s1} as a key (target sequence
  interval), and returns an \class{Seq2SeqEdge} object providing detailed
  information about the alignment between this target interval
  and the source interval.
\end{funcdesc}


\begin{funcdesc}{__len__}{}
  returns the number of distinct sequences that
  are aligned to the source interval.  {\em Note}: this is NOT necessarily 
  equal to the number of items that will be returned by the above iterators,
  since a single target sequence might have multiple 1:1 intervals of
  alignment to the source interval, due to indels.
\end{funcdesc}



In addition to these standard dictionary methods, NLMSASlice provides
several additional methods and attributes:


\begin{itemize}
\item
\member{letters}: this attribute provides an interface to 
the individual alignment columns (NLMSANode objects) containing the
source interval, in order from \var{start} to \var{stop}.  This provides
an easy way to obtain detailed information about the letter-to-letter
alignment of different sequences within this region of the alignment.
For details on the kinds of information you can obtain for each
alignment column, see NLMSANode, below.

It also provides a graph interface to subset of the partial order alignment 
graph corresponding to this slice.  For details, see NLMSASliceLetters, below.
\end{itemize}

\begin{funcdesc}{split}{**kwargs}
  this method provides a way to perform group-by operations on the slice;
  the output of split() is one or more NLMSASlice objects; if the
  group-by analysis results in no splitting of the current slice, then
  it is returned unchanged (i.e. the method just returns {\em self}).
  Uses same group-by arguments as \method{keys()}.
  For further details on group-by operations, see \method{keys()} above.
\end{funcdesc}

\begin{funcdesc}{regions}{**kwags}
  performs the same group-by analysis as {\em split()}, but replaces
  the source interval by the corresponding interval in the LPO.  The main
  practical consequence of this is that target sequence {\em inserts}
  are included in the resulting slice (because they are present in the LPO
  interval corresponding to the original source interval), whereas they
  were NOT included in the original slice (because they are not aligned
  to the source interval).  The main place where this matters is in graph
  traversal of the slice's {\em letters} attribute: whereas the nodes
  and edges corresponding to these inserts are not considered to be part
  of the {\em letters} graph for the original slice, they {\em are} part of the
  LPO slice.  Also, the ``source interval'' in any subsequent operations
  with the LPO slice will be LPO coordinates instead of subintervals of the
  original source sequence interval.
  Uses same group-by arguments as \method{keys()}.
\end{funcdesc}

\begin{funcdesc}{groupByIntervals}{maxgap=0, maxinsert=0, mininsert= 0, filterSeqs=None, mergeAll=True, ivalMethod=None, **kwargs}
  This method performs the interval grouping analysis for all the iterators
  described above.  Users will not need to call it directly.  Its arguments
  are described above (see \method{keys()}).  It returns a dictionary
  whose keys are sequences aligned to this slice, and whose values are
  the list of intervals produced by the group-by analysis for the corresponding
  sequence.  The values are tuples of the form
  \var{(source_start, source_stop, target_start, target_stop)}, showing the
  mapping of a source sequence interval onto a target sequence interval.
  This dictionary is the primary input to the \method{groupBySequences()}
  method below.
\end{funcdesc}

\begin{funcdesc}{groupBySequences}{seqIntervals, sourceOnly=False, indelCut=False, seqGroups=None, minAligned=1, pMinAligned=0., seqMethod=None, **kwargs}
  This method performs the sequence grouping analysis for all the iterators
  described above.  \var{seqIntervals} must be a dictionary of sequences
  and their associated list of intervals (produced by \method{groupByIntervals()}
  above).  It returns a list of output sequence intevals, which is either
  a list of source sequence intervals (\var{sourceOnly} mode), or a list
  of tuples of the form \var{(source_interval, target_interval)}.
\end{funcdesc}


\begin{funcdesc}{matchIntervals}{seq=None}
  this method returns the set of
  1:1 match intervals for the target sequence {\em seq} (or all
  aligned sequences, if {\em seq} is None), as a dictionary
  whose keys are target sequence intervals, and whose values are
  the corresponding source sequence intervals to which they are
  aligned.
\end{funcdesc}

\begin{funcdesc}{findSeqEnds}{seq}
  returns the largest possible interval of
  {\em seq} that is aligned to this slice, i.e. it merges all 
  alignment intervals in this slice containing {\em seq}, and
  returns the merged sequence interval based on the minimum {\em start}
  value and maximum {\em stop} value found.
\end{funcdesc}

\subsubsection{NLMSASliceLetters}
represents the {\em letters} graph of a specific NLMSASlice.  It is
a graph whose nodes are the NLMSANode objects in this slice, and whose
edges are sequence.LetterEdge objects. {\em Note}: currently the edge objects
are just returned as None -- please implement!

This graph has the following methods:

\begin{funcdesc}{__iter__}{}
  generates all the nodes in the slice, in order from left to right.
\end{funcdesc}

\begin{funcdesc}{items}{}
  also \method{iteritems()}. Generate the same set of nodes as above,
  as keys, but for each also returns a value representing its outgoing
  directed edges (see getitem, below).
\end{funcdesc}

\begin{funcdesc}{__getitem__}{node}
  gets a dictionary indicating all the outgoing
  directed edges from {\em node} to subsequence nodes, whose keys are
  the target nodes, and whose edges are the 
  \class{sequence.LetterEdge} objects representing each edge.
\end{funcdesc}

\subsubsection{NLMSANode}
A temporary object (created on-the-fly) 
representing a single letter ``column'' in the alignment.  It acts like
a container of the sequence letters aligned to the source sequence in
this column.  It has the following methods:

\begin{funcdesc}{__iter__}{}
  generates all the individual sequence letters 
  (as SeqPath intervals, presumably of length 1) that are aligned to 
  the source sequence, in this column of the alignment.
\end{funcdesc}

    
\begin{funcdesc}{edges}{}
  generates the same list of of target sequence letters as
  the iterator, but as a tuple of (target letter, source letter, edge).
  Currently, edge is just None.
\end{funcdesc}

\begin{funcdesc}{__len__}{}
  returns the number of distinct sequences aligned to
  the source interval, in this column.
\end{funcdesc}

Other, internal methods that regular users are unlikely to need:

\begin{funcdesc}{getSeqPos}{seq}
  returns the sequence interval of \var{seq}
  that is aligned to this column, or raises KeyError if it is not
  aligned here.
\end{funcdesc}


\begin{funcdesc}{getEdgeSeqs}{node2}
  returns a dictionary of sequences
  that traverse the edge directly from this node to {\em node2},
  i.e. if letter {\em i} of seq is aligned to this node, then
  letter letter {\em i+1} is aligned to {\em node2}.  The
  dictionary's keys are top-level sequence objects, and its
  value for each is the letter position index {\em i} as defined above.
\end{funcdesc}

\begin{funcdesc}{nodeEdges}{}
  returns a dictionary of the outgoing edges
  from this node, whose keys are target nodes, and whose values
  are the corresponding edge objects (of type sequence.LetterEdge).
\end{funcdesc}


\subsubsection{NLMSASequence}
You are unlikely to need to manipulate NLMSASequence objects directly;
they perform the back-end work for accessing the nested list disk storage
of the alignment of the associated sequence.

However, one thing you should know is that for a sequence to be stored
in a NLMSA, it needs to have a unique string identifier.
NLMSASequence obtains a string identifier for the sequence in one of the following
ways (in decreasing order of precedence): 1) the sequence ``object'' can itself just
be a Python string, in which case that string is used as the identifier. 2) otherwise,
the object should be a SeqPath instance.  If it has a {\em name} attribute, that will
be used as the identifier. 3) Otherwise, if it has a {\em id} attribute (which is present
by default on sequence.Sequence objects), that will be used.


\subsection{Seqdb Module}
\label{seqdb}

{\em Pygr interface to sequence databases stored in FASTA, BLAST or relational databases.}


\subsubsection{Overview}

The seqdb module provides a simple, consistent interface to sequence databases from a variety of different storage sources such as FASTA, BLAST and relational databases.  Sequence databases are modeled (like other Pygr container classes) as dictionaries, whose keys are sequence IDs and whose values are sequence objects.  Pygr sequence objects use the Python sequence protocol in all the ways you'd expect: a subinterval of a sequence object is just a Python slice (s[0:10]), which just returns a sequence object representing that interval; the reverse complement is just -s; the length of a sequence is just len(s); to obtain the actual string sequence of a sequence object is just str(s).  Pygr sequence objects work intelligently with different types of back-end storage (e.g. relational databases or BLAST databases) to efficiently access just the parts of sequence that are requested, only when an actual sequence string is needed.

\subsubsection{External Requirements}
This module makes use of several external programs:
\begin{itemize}
\item
{\em NCBI toolkit}: The BLAST database functionality in this module 
requires that the NCBI toolkit
be installed on your system.  Specifically, some functions will call the command line
programs \code{formatdb}, \code{fastacmd}, \code{blastall}, and \code{megablast}.

\item
{\em RepeatMasker}: the \method{BlastDB.megablast()} method calls the command line
program \code{RepeatMasker} to mask out repetitive sequences from seeding alignments,
but to allow extension of alignments into masked regions.

\item
{\em Python DB-API 2.0}: the \class{SQLTable} class, and dependent classes such as 
\class{SQLSequence} and \class{StoredPathMapping}, conform to the Python DB-API 2.0.
Typically you must supply a DB-API 2.0-compliant database cursor to the 
\class{SQLTable} constructor.  To do so, you must have some DB-API 2.0-compliant
module (such as \module{MySQLdb}) installed for connecting to a database server.
\end{itemize} 

If you are lacking one or more of these requirements, you can still install Pygr
and use all Pygr functionality that does not depend on the missing requirements.
If you try to use a function for which a requirement is missing, Pygr will raise
an appropriate exception (e.g. unable to run \code{blastall}).

\subsubsection{BlastDB}
Interface to an existing BLAST database or FASTA sequence file; in the latter case, it will automatically construct BLAST database files for you using the NCBI tool formatdb. Here's a simple example of opening a BLAST database and searching it for matches to a specific piece of sequence:

\begin{verbatim}
from pygr.seqdb import *
db=BlastDB('sp') # OPEN SWISSPROT BLAST DB
s=NamedSequence(str(db['CYGB_HUMAN'][40:-40]),'boo')
m=db.blast(s) # DO BLAST SEARCH
myg=db['MYG_CHICK']
for i in m[s][myg]:
    print repr(i.srcPath),repr(i.destPath),i.blast_score,i.percent_id
\end{verbatim}

Let's go through this example line by line:

\begin{itemize}

\item    
construction of a BlastDB object: This looks for either a FASTA file with the path 'sp' or BLAST database formatted files based on this path (e.g. 'sp.psd' for protein sequences, or 'sp.nsd' for nucleotide sequences).

\item
db['CYGB_HUMAN'] obtains a sequence object representing the SwissProt sequence whose ID is CYGB_HUMAN.  The slice operation [40:-40] behaves just like normal Python slicing: it obtains a sequence object representing the subinterval omitting the first 40 letters and last 40 letters of the sequence.  The str() operation obtains the actual string representation of this subinterval.

\item
NamedSequence(letter_string, name) creates a new sequence object whose sequence is letter_string, and whose ID is name.

\item
Running the db.blast(s) method searches the BLAST database for homologies to s, using NCBI BLAST.  It chooses reasonable parameters based upon the sequence types of the database and supplied query.  However, you can specify extra parameter options if you wish.  It returns a Pygr sequence mapping (multiple alignment) that represents a standard Pygr graph of alignment relationships between s and the homologies that were found.

\item
The expression m[s][myg] obtains the "edge information" for the graph relationship between the two sequence nodes s and myg.  (if there was no edge in the m graph representing a relationship between these two sequences, this would produce a KeyError).  This edge information consists of a set of interval alignment relationships (described in detail below), which are printed out in this example.

\end{itemize}

Additional options for constructing a BlastDB:

\begin{itemize}

\item
skipSeqLenDict: To facilitate the rapid creation of sequence objects (which requires the length of the sequence), it also creates a sequence length index (based on the filepath, in this case 'sp.seqlen' as a Python shelve).  This enables it to avoid actually loading the sequence string into memory each time a sequence object is created; instead it just looks up the sequence length.  While this speeds up access to genomic sequence databases (where each sequence tends to be extremely long), it is unnecessary and slow for databases of short sequences.  Setting skipSeqLenDict option to True, will prevent construction of this sequence length index.

\item
ifile: if you have a file object, you can pass it directly to BlastDB instead of a filepath.  NB: the BlastDB() constructor will close ifile when it is done reading from the file object.

\end{itemize}
Useful methods:

\begin{itemize}

\item
iter(): iterate over all IDs in the BLAST database.

\item
len(): number of sequences in the BLAST database.

\item 
blast(seq,al=None,blastpath='blastall',blastprog=None,expmax=0.001,maxseq=None): run a BLAST search on sequence object seq.  Maxseq will limit the number of returned hits to the best maxseq hits. 
 
\item
megablast(seq,al=None,blastpath='megablast',expmax=1e-20,maxseq=None,minIdentity
=None,maskOpts='-U T -F m',rmOpts=''): first performs repeat masking on the sequ
ence by converting repeats to lowercase, then runs megablast with command line o
ptions to prevent seeding new alignments within repeats, but allowing extension 
of alignments into repeats.  minIdentity should be a number (maximum value, 100)indicating the minimum percent identity for hits to be returned.

Useful attributes:

\item
seqClass: the object class to use for instantiating new sequence objects from this BLAST database.  You can set this to create customized sequence behaviors.

\subsubsection{BlastSequence}

The default class for sequence objects returned from BlastDB.  It has several optimizations for working with BLAST databases:

\item
it uses the NCBI tool fastacmd to retrieve sequence efficiently from a BLAST database, when your program requests an actual string of sequence text.  Moreover, for subintervals (slices) of the sequence, it uses fastacmd's -L option to request just the desired subinterval of the sequence, rather than the whole sequence.  This makes it efficient for requesting specific intervals of large genomic contigs.  Basically, just use Python slicing and str() methods on sequence objects, and subsequences will be obtained in an efficient manner.

\item 
the len() method is implemented using the seqLenDict, a precalculated index of the sequence lengths.  So again no sequence has to be read by Python.

\end{itemize}
\subsubsection{BlastSequenceCache}

Implements a variant of BlastSequence designed to merge and cache requests for local intervals of sequence so that repeated accesses to these regions are bundled and cached for efficiency.  You work with sequence objects of this type normally, using Python slicing to obtain subintervals, and str() to get the sequence string for a subinterval.  But behind the scenes, it does two things:

\begin{itemize}
\item
all slicing operations are recorded, in the form of a cache of superintervals.  Overlapping or adjacent intervals are merged into a superinterval up to a maximum superinterval size (default 20000).  It will automatically create as many superintervals as needed to cover the requested subinterval slices.  Each superinterval is represented by an object of the FastacmdIntervalCache class.

\item 
when the sequence string of a subinterval is requested, the cache actually retrieves (and caches) the entire superinterval containing that subinterval.  Fastacmd only needs to be called once for this superinterval.  Subsequent subinterval string requests that fall within this cached superinterval are simply returned directly from the cache, without calling fastacmd.

\end{itemize}

\subsubsection{SQLSequence}

Implements a subclass inheriting from SQLRow and NamedSequenceBase, to use a relational database table to obtain the actual sequence.  There are three minor variants DNASQLSequence, RNASQLSequence, ProteinSQLSequence (so that the sequence does not have to analyze itself to determine what kind of sequence it is).  Its constructor takes the same arguments as SQLRow(table, id), where table is the SQLTable object representing the table in which the sequence is stored, and id is the primary key of the row representing this sequence.  However, normally this class is simply passed to the Table object itself so that it will use it to instantiate new row objects whenever they are requested via its dictionary interface.  Here's a simple example:

\begin{verbatim}
class YiProteinSequence(ProteinSQLSequence): # CREATE A NEW SQL SEQUENCE CLASS
    def __len__(self): return self.protein_length  # USE LENGTH STORED IN DATABASE
protein=jun03[protein_seq_t] # protein IS OUR SQLTable OBJECT REPRESENTING PROTEIN SEQUENCE TABLE
protein.objclass(YiProteinSequence) # FORCE PROTEIN SEQ TABLE TO USE THIS TO INSTANTIATE ROW OBJECTS
pseq=protein['Hs.1162'] # GET PROTEIN SEQUENCE OBJECT FOR A SPECIFIC CLUSTER
\end{verbatim}

Let's go through this line by line:

\begin{itemize}

\item
we create a subclass of ProteinSQLSequence to show how Python makes it easy to create customized behaviors that can make database access more efficient.  Here we've simply added a __len__ method that uses the protein_length attribute obtained directly from the database, courtesy of SQLRow.__getattr__, which knows what columns exist in the database, and provides them transparently as object attributes.  (The ordinary NamedSequenceBase __len__ method calculates it by obtaining the whole sequence string and calculating its length.  Clearly it's more efficient for the database to retrieve this number (stored as a column called protein_length) and return it, rather than making it send us the whole sequence).

\item
next we call the protein.objclass() method to inform the table object that it should use our new class for instantiating any row objects for this table.  It will call this class with the usual SQLRow contructor arguments (table, id).
\end{itemize}

\subsubsection{Sequence Alignment}

A second major area in Pygr is representation and query of multiple sequence alignment databases in a way that is scalable to whole genomes.  We have previously showed (in our work on Partial Order Alignment) that graphs provide both a compact and algorithmically powerful way to store alignments.  Combining this with "interval alignment" makes it scalable and gives a simple interface.  In Pygr, alignments are just another kind of graph, whose nodes are sequence intervals, and edges are alignment relations.  This provides a general-purpose facility for working with sets of sequence intervals, sequence annotation databases, and multiple sequence alignments, all queryable via Pygr graph queries.  We have implemented different container subclasses to work with these data in memory or to work transparently with data stored in relational databases.  The consistency and simplicity of the Pygr framework makes it a good interface both to run external tools like BLAST, and to store or query the results in persistent storage like a MySQL database.

\begin{verbatim}
hg17=BlastDB('/data/ucsc/hg17') # GET CONTAINER FOR HUMAN GENOME DATABASE
bcl2m=hg17['chr22'][16544303:16588541] # GET INTERVAL WITH BCL2L13 GENE
al=hg17.megablast(mouse_bcl2,maxseq=1) # GET REPEAT-MASKED MEGABLAST ALIGNMENT, ONLY TOP HIT
al[bcl2m[1000:1100] ]+=mrna[210:310] #ADD ALIGNMENT OF A 100nt SEGMENT TO mrna SEGMENT
al.storeSQL('test.table',db_cursor) # STORE COMPLETE ALIGNMENT IN RELATIONAL DATABASE
for e in MAFStoredPathMapping(bcl2m,'ucsc_maf8',u).edges(): #GET ITS MULTIGENOMEALIGNMENTS
   print str(e.srcPath),str(e.destPath) # PRINT THE ACTUAL ALIGNED SEQUENCE INTERVALS
\end{verbatim}


\subsubsection{VirtualSeq}
This class provides an empty sequence object that
acts purely as a reference system.
Automatically elongates if slice extends beyond current stop.
This class avoids setting the {\em stop} attribute, taking advantage
of SeqPath's mechanism for allowing a sequence to grow in length.
\begin{verbatim}
s=VirtualSeq('FOOG_HUMAN')
len(s) # ONLY 1 LETTER LONG BY DEFAULT
s1=s[100:215] # GET A SLICE OF THIS SEQUENCE
len(s) # NOW IT'S 215
\end{verbatim}

The associated VirtualSeqDB class provides a ``sequence database''
that returns a VirtualSeq object for every identifier requested of
it.  It acts like a Python dictionary:
\begin{verbatim}
db=VirtualSeqDB()
s=db['FOOG_HUMAN'] # ASK FOR A SEQUENCE BY ITS IDENTIFIER
s1=s[100:215] # GET A SLICE OF THIS SEQUENCE
\end{verbatim}
For a given identifier it always returns the same VirtualSeq
object (i.e. the object returned from the first request for that identifier).
In other words, if the identifier was previously requested,
it returns the VirtualSeq for that identifier; if not, it 
creates a new one.
This can be convenient when performing operations that just
need a coordinate reference system, not actual sequence.


\subsubsection{PrefixUnionDict}
This class acts as a wrapper for a set of dictionaries, each
of which is assigned a specific string ``prefix''.  It provides
a dictionary interface that accepts string keys of the form
``prefix.suffix'', and returns d['suffix'] where {\em d} is
the dictionary associated with the corresponding prefix.  This
is useful for providing a unified ``database interface'' to a
set of multiple databases.
\begin{verbatim}
hg17=BlastDB('/usr/tmp/ucsc_msa/hg17')
mm5=BlastDB('/usr/tmp/ucsc_msa/mm5')
... # LOAD A BUNCH OF OTHER GENOMES TOO...
genomes={'hg17':hg17,'mm5':mm5, 'rn3':rn3, 'canFam1':cf1, 'danRer1':dr1,
'fr1':fr1, 'galGal2':gg2, 'panTro1':pt1} # PREFIX DICTIONARY FOR THE UNION 
					 # OF ALL OUR GENOMES
genomeUnion=PrefixUnionDict(genomes)
ptChr7=genomeUnion['panTro1.chr7'] # GET CHIMP CHROMOSOME 7
\end{verbatim}


\subsubsection{Functions}
The seqdb module also provides several convenience functions:

\begin{funcdesc}{read_fasta}{ifile, onlyReadOneLine=False}
  a generator function
  that yields tuples of \var{id,title,seq} from \var{ifile}.  
  The \var{onlyReadOneLine} option is useful if you only want to 
  determine the sequence type (e.g. DNA vs protein) and the
  whole sequence might be extremely long (e.g. a genome!).
\end{funcdesc}

\begin{funcdesc}{write_fasta}{ofile, s, chunk=60, id=None}
  writes the sequence \var{s}
  to the output file \var{ofile}, using \var{chunk} as the line width.
  \var{id} can provide an identifier to use instead of the default 
  \code{s.id}.
\end{funcdesc}






\subsection{Coordinator Module}
\label{coord-module}

{\em Framework for running subtasks distributed over many computers, in a pythonic way, using SSH for secure process invocation and XMLRPC for message passing. Also provides simple interface for queuing and managing any number of such "batch jobs".}

\subsubsection{Overview}

The coordinator module provides a simple system for running a large collection of tasks on a set of cluster nodes.  It assumes:

\begin{itemize}

\item
authentication is handled using ssh-agent.  The coordinator module does no authentication itself; it simply tries to spawn jobs to remote nodes using ssh, assuming that you have previously authenticated yourself to ssh-agent. 

\item
the client nodes can access your scripts using the same path as on the initiating system.  In other words, if you launch a coordinator job /home/bob/mydir/myscript.py, your client nodes must also be able to access /home/bob/mydir/myscript.py (e.g. via NFS).

\item
your job consists of a large set of task IDs, and a computation to be performed on each ID.  To run this job, you provide an iterator that generates the list of task IDs for the Coordinator to distribute to your client nodes.  You start your script to run a Coordinator that serves your list of task IDs to the client nodes.  You also provide  a function that performs your desired computation on each task ID it receives from the Coordinator.  Typically, you provide both the server function (i.e. the iterator that generates the list of task IDs) and the client function (that runs your desired computation for each ID) within a single Python script file.  Running this script without extra flags starts the Coordinator, which in turn launches your script as a Processor on one or more client nodes.  The Processors andCoordinator work together to complete all the task IDs.

\item
a ResourceController performs load balancing and resource allocation functions, including: dividing up loads from one or more Coordinators over a set of hosts (each with one or more CPUs); serving a Resource database to Processors requesting specific resources; resource-locking on a per node basis for preventing Processors from using a Resource that is under construction by another Processor.  For very large files that are used repeatedly by your computation, it is preferable to first copy them to local disk on each cluster node (fast), rather than reading them over and over again from NFS (slow).  Resources provide a simple mechanism for doing this.

\end{itemize}
To see how to use this, let's look at an example script, mapclusters5.py:

\begin{verbatim}

from pygr.apps.leelabdb import *
from pygr import coordinator

def map_clusters(server,genome_rsrc='hg17',dbname='HUMAN_SPLICE_03',
                 result_table='GENOME_ALIGNMENT.hg17_cluster_JUN03_all',
                 rmOpts='',**kwargs):
    "map clusters one by one"
    # CONSTRUCT RESOURCE FOR US IF NEEDED
    genome=BlastDB(ifile=server.open_resource(genome_rsrc,'r'))
    # LOAD DB SCHEMA
    (clusters,exons,splices,genomic_seq,spliceGraph,alt5Graph,alt3Graph,mrna, \
     protein,clusterExons,clusterSplices)=getSpliceGraphFromDB(spliceCalcs[dbname])

    for cluster_id in server:
        g=genomic_seq[cluster_id]
        m=genome.megablast(g,maxseq=1,minIdentity=98,rmOpts=rmOpts) # MASK, BLAST, READ INTO m
        # SAVE ALIGNMENT m TO DATABASE TABLE test.mytable USING cursor
        createTableFromRepr(m.repr_dict(),result_table,clusters.cursor,
                            {'src_id':'varchar(12)','dest_id':'varchar(12)'})
        yield cluster_id # WE MUST FUNCTION AS GENERATOR

def serve_clusters(dbname='HUMAN_SPLICE_03',
                   source_table='HUMAN_SPLICE_03.genomic_cluster_JUN03',**kwargs):
    "serve up cluster_id one by one"
    cursor=getUserCursor(dbname)
    t=SQLTable(source_table,cursor)
    for id in t:
        yield id

if __name__=='__main__':
    coordinator.start_client_or_server(map_clusters,serve_clusters,['hg17'],__file__)
\end{verbatim}

Let's analyze the script line by line:

\begin{itemize}

\item
mapclusters() is a client generator function to be run in a Processor on a client node.  It takes one argument representing its connection to the server (a Processor object), and optional keyword arguments read from the command line.  It first does some initial setup (opens a BLAST database and loads a schema from a MySQL database), then iterates over task IDs returned to it from the server.  A few key points:

\item
server.open_resource(genome_rsrc,'r') requests a resource given by the genome_rsrc argument from the ResourceController, does whatever is necessary to copy this resource to local disk, and then opens it for reading, returning a file-like object.  This can then be used however you like, but you MUST call its close() method (just as you should always do for any file object) to indicate that you're done using it.  Failure to close() the file object will leave the Resource "hg17" permanently locked on this specific node.  (You would then have to unlock it by hand using the ResourceController.release_rule() method).

\item
yield cluster_id: the client function must be a Python generator function (i.e. it must use the yield statement), and it must yield the list of IDs that it has processed.  Python's generator construct is extremely convenient for many purposes: here it lets us perform both our initializations and iteration over IDs within a single function, while at the same time wrapping each iteration within the Processor's error trapping code (to prevent a single error in your code from causing the entire Processor to shut down).  The Processor will trap any errors in your code and and send tracebacks to your Coordinator, which will report them in its logfile.  The Processor will tolerate occasional errors and continue processing more IDs.  However, if more than a certain number of IDs in a row fail with errors (controlled by the Processor.max_errors_in_a_row attribute), the Processor will exit, on the assumption that either your code or this specific client node don't work correctly.

\item
serve_clusters() is the server generating function to be run in the Coordinator.  It returns an iterator that generates all the task IDs that we want to run.  Again, the Python generator construct provides a very clean way of doing this: we simply yield each ID that we want to process in our client Processors.

\item
if __name__=="__main__": this final clause automatically launches our script as either a Coordinator or Processor depending on the command line options (which are automatically parsed by start_client_or_server()).  All we have to do is pass the client generator function, the server generator function, a list of the resources this job will use, and the name of the script file to be run on client nodes.  Since that is just this script itself, we use the Python builtin symbol __file__ (which just evaluates to the name of the current script).

\item     
Command-line arguments are parsed (GNU-style, ie. --foo=bar) by start_client_or_server() and passed to your client and server functions as Python named parameters.  Because the same list of arguments is passed to your client and server functions, and each of these functions won't necessarily want to get all the named arguments, you should include the **kwargs at the end of the argument list.  Any unmatched arguments will be stored in kwargs as a Python mapping (dictionary).  If you fail to do this, your client or server function will crash if called with any named parameters other than the ones it expects.
\end{itemize}

\subsubsection{Log and Error Information}

Process logging and error information go to three different types of logs:

\begin{itemize}

\item
Processor logfile(s): every individual Processor (and all subprocesses run by it) send stdout and stderr to a logfile on local disk of the host on which it is running.  Currently the filename is /usr/tmp/NAME_N.log, where NAME is the name you assigned to the job when you started the Coordinator, and N is the numeric ID of the Processor assigned by the coordinator (just an auto-increment integer beginning at 0, and increasing by one for each Processor the Coordinator starts).  This logfile is the place to look if your job is failing mysteriously--look in the logfile and see its last words before its demise.  You can get a complete list of the logfiles for all the Coordinator's Processors by inspecting the logfile attribute of the CoordinatorMonitor (see below).

\item
Coordinator logfile: all XMLRPC requests from client Processors, as well as error messages from them, are logged here.  All Python errors (tracebacks) in your client (Processor) code are reported here.  Also, the actual SSH commands used to invoke your Processors on cluster nodes, are logged here.  This is usually the place to start, to see whether things are going well (you should see a long stream of next requests as Processors finish a task and request the next one), or failing with errors.

\item
ResourceController logfile: all XMLRPC requests from Processors and Coordinatorsare logged here, including register() and unregister(), resource requests, and load reporting from cluster nodes.  If things are working well, you should see a stream of regular report_load() messages showing steady, full utilization of all the host processors.  Excessive register/unregister churning (jobs that start and immediately exit) is a common sign of trouble with your jobs.

\end{itemize}
\subsubsection{Coordinator}

To start a job coordinator (which in turn will the run the whole job by starting Processors on cluster nodes using SSH):

\begin{verbatim}
python mapclusters5.py mm5_jan02 --errlog=/usr/tmp/leec/mm5_jan02.log \ 
  --dbname=MOUSE_SPLICE --source_table=genomic_cluster_jan02 \
  --genome_rsrc=mm5 --result_table=GENOME_ALIGNMENT.mm5_cluster_jan02_all \ 
  --rmOpts=-rodent \
\end{verbatim}

Here we have told the Coordinator to name itself "mm5_jan02" in all its communications with the ResourceController.  Since we gave no command-line flags, the Coordinator will assume that a ResourceController is already running on port 5000 of the current host.    You must have an ssh-agent running BEFORE you start the Coordinator, since the Coordinator will attempt to spawn jobs using SSH.  The Coordinator will exit with an error message if it is unable to connect to ssh-agent.  A few notes:

\begin{itemize}

\item
The Coordinator will run as a demon process (i.e. in the background, and detached from your terminal session), and redirect its  output into a file (here, given by the --errlog option). If you don't specify an --errlog filename, it will create a filename determined by the name we told it to run as, in this case "mm_jan02.log".  

\item
You must ensure that SSH can launch processes on your client nodes "unattended" i.e. without a connection to a controlling terminal.  If SSH has to ask for userconfirmations when connecting to a given host (e.g. if it asks whether you want to accept the host key), the Coordinator will not be able to use that host.

\item
Python errors (tracebacks) in your will be GNU-style command-line options (e.g. --port=8889) are automatically parsed by start_client_or_server() and passed to the Coordinator.__init__() as keyword arguments.  This constructor takes the following optional arguments: 
    \begin{itemize}
    \item
    port: the port number on which this Coordinator should run
    
    \item
    priority: a floating point number specifying the priority level at which this Coordinator should be run by the ResourceController.  The default value is 1.0.  A value of 2.0 will give it twice as many Processors as a competing Coordinator of priority 1.0.

    \item
    rc_url: the URL for the ResourceController.  Defaults to http://THISHOST:5000
    \item
    errlog: logfile path for saving all output to.  Defaults to NAME.log, where NAME is the name you assigned to this Coordinator. Can be an absolute path.

    \item
    immediate: if True, make the job run immediately, without waiting for previous jobs to finish.  Default: False.
    
    \item
    demand_ncpu: if set to a non-zero value, specifies the exact number of Processors you want to run your job.

    \item
    NB: command line arguments are also passed to your server function, and to your client function, as Python named parameters.  See the mapclusters5.py example above.
    \end{itemize}
\end{itemize}

\subsubsection{ResourceController}

Whereas you start a separate Coordinator for each set of jobs you want to run, you only need a single ResourceController running. To start the ResourceController, run:

\begin{verbatim}
python coordinator.py --rc=bigcheese
\end{verbatim}

This starts the ResourceController (running as a demon process in the background) and names it "bigcheese"; a name argument (given by the --rc flag) is REQUIRED.  Since you didn't specify command-line flags, it will run on the default port 5000.  It will use several files based on the name you gave it:
\begin{itemize}
 
\item
bigcheese.hosts: a list of cluster nodes and associated maximum load (separated by whitespace, one pair per line).  It will attempt to fill these nodes with jobs, up to the maximum load level specified for each, sharing the load between whatever set of Coordinators contact it.

\item
bigcheese.log: all output from the ResourceController (showing requests made to it by Coordinators and Processors) is logged to this file.

\item
bigcheese.rules: this file is a Python shelve created by the ResourceController as its rules database.

\item
bigcheese.rsrc: this file is a Python shelve created by the ResourceController as its resource database.GNU-style command-line options (e.g. --port=5001) are automatically parsed by start_client_or_server() and passed to the ResourceController.__init__() as keyword arguments.  This constructor takes the following optional arguments:

\item
port: the port number on which this ResourceController should run

\item
overload_margin: how much "extra" load above the standard level is allowable.  This prevents temporary load spikes from causing Processors to exit.  Set by default to 0.6.  I.e. if the maxload for a host was set to 2.0, any load above 2.6 would cause the ResourceController to start shutting down Processor(s) on that host.

\item
rebalance_frequency: the time interval, in seconds, for rerunning the ResourceController.load_balance() method.  Defaults to 1200 sec.

\item
errlog: logfile path for saving all output to.  Defaults to NAME.log, where NAME is the name you assigned to this ResourceController. Can be an absolute path.

\end{itemize}

\subsubsection{RCMonitor}

The coordinator module also provides a convenience interface for interrogating and controlling jobs.  In an interactive Python shell, import the coordinator module, and create an RCMonitor object::

\begin{verbatim}
from pygr import coordinator
m=coordinator.RCMonitor()
\end{verbatim}

Since you did not specify any arguments, it will default to searching for the ResourceController on the current host, port 5000.  You can specify a host and or port as additional arguments.  It also loads an index of coordinators currently registered with this ResourceController, accessible on its coordinators attribute:

\begin{verbatim}
for name,c in m.coordinators.items():
  print name,len(c.client_report)
\end{verbatim}

will print a list of the coordinators and how many Processors each is currently running.  Each coordinator is represented by a CoordinatorMonitor object in this coordinators index.

Both RCMonitor and CoordinatorMonitor objects give you access to the XMLRPC methods of the ResourceController and Coordinators they represent.  That is, running a method on the RCMonitor actually runs the identically-named method on the ResourceController.  Some of the most useful ResourceController methods are:

\begin{itemize}

\item
report_load(host,pid,load): inform RC that the current load on host is load.

\item
load_balance(): make the RC rebalance load, using all available nodes and coordinators

\item
setrule(rsrc,rule): set a production rule for the resource named rsrc.  rule must be a tuple consisting of the local filepath to be used for the resource, and a shell command that will construct it, with a %s where you want the filename to be filled in.

\item
delrule(rsrc): deletes the rule for rsrc from the rules database.

\item
set_hostinfo(host,attr,val) set an attribute for host.  For example, to set the maximum load for this host: rcm.set_hostinfo(host,'maxload',2.0).  This should usually be the number of CPUs on this host.  NB: these settings will apply only to the current ResourceController, and are not saved back to its NAME.hosts file.  If you want to make these settings permanent (i.e. to apply to ResourceControllers you start anew in the future), then edit the NAME.hosts file.

\item
retry_unused_hosts(): make the RC search its hosts database for hosts that are not currently in use (e.g. jobs may have died) and try to reallocate them to the existing coordinators.

\end{itemize}
Both RCMonitor and CoordinatorMonitor objects have a get_status() method that updates them with the latest information from their associated ResourceController or Coordinator.

Here are some typical monitor usages:

\begin{verbatim}
c=m.coordinators['mapclusters3'] # GET MY COORDINATOR
c.client_report.sort() # MAKE IT SORT CLIENTS BY HOSTNAME
c.client_report # PRINT THE SORTED LIST, SHOWING HOST, PID, #TASKS DONE
c.pending_report # PRINT LIST OF TASK IDS CURRENTLY RUNNING
c.nsuccess # PRINT TOTAL #TASKS DONE
c.nerrors  # PRINT TOTAL #TASKS FAILED
c.logfile # PRINT LIST OF ALL PROCESSOR LOGFILES

m.rules # PRINT THE CURRENT RULES DATABASE
m.resources # PRINT THE CURRENT RESOURCES DATABASE
m.setrule('hg17',
('/usr/tmp/ucsc_msa/hg17',
'gunzip -c /data/yxing/databases/ucsc_msa/human_assembly_HG17/*.fa.gz
>%s'))
m.get_status() # UPDATE OUR RC INFO
m.set_hostinfo('llc22','maxload',2.0) # ADD A NEW HOST TO OUR DATABASE
m.setload('llc1','maxload',0.0) # STOP USING llc1 FOR THE MOMENT
m.load_balance() # MAKE IT ALLOCATE ANY FREE CPUS NOW...
m.locks # SHOW LIST OF RESOURCES CURRENTLY LOCKED, UNDER CONSTRUCTION
\end{verbatim}

\subsubsection{Security}

Internal communication between Processors, Coordinators and ResourceController is performed using XMLRPC and thus is not secure. However, since no authentication information or actual commands are transmitted by XMLRPC, and the coordinator module does not enable the processes that use it to do anything that they are not ALREADY capable of doing on their own (i.e. spawn ssh processes), the main security vulnerabilty is Denial Of Service (i.e. an attacker listening to the XMLRPC traffic could send messages causing Processors to shutdown, or Coordinators to be blocked from running any Processors).  In other words the security philosophy of this module is to avoid compromising your security, by leaving the security of process invocation entirely to your existing security mechanisms (i.e. ssh and ssh-agent).  Commands are only sent using SSH, not XMLRPC, and the XMLRPC components are designed to prevent known ways that an XMLRPC caller might be able to run a command on an XMLRPC server or client. (I blocked known security vulnerabilities in Python's SimpleXMLRPCServer module).

In the same spirit, the current implementation does not seek to block users from issuing commands that could let them "hog" resources, for the simple reason that in an SSH-enabled environment, they would be able to do so regardless of this module's policy.  I.e. the user can simply not use this module, and spawn lots of processes directly using SSH.  In the current implementation, every user can send directives to the ResourceController that affect resource allocation to other users' jobs.  This means everybody has to "play nice", only giving their Coordinator(s) higher priority if it is really appropriate and agreed by other users.  Unless a different process invocation mechanism (other than SSH by each user) were adopted, it doesn't really make sense to me to try to enforce a policy that is stricter than the policy of the underlying process invocation mechanism (i.e. SSH).  Since every user can use SSH to spawn as many jobs as they want, without regard for sharing with others, making this module's policy "strict" doesn't really secure anything.

\end{document}
